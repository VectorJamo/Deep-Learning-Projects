{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "646b38c2-386a-4f83-a983-ea764ad8267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to recover training data using model's parameters and a DCGAN that was trained on similar data\n",
    "# Author: Suraj Neupane\n",
    "# Written from scratch as a part of a Research Project 2025, Concordia University of Edmonton.\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b80ddb2-ed75-4870-85b4-9300fce0df7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6950585-eb31-4f61-bd60-5bf8716fcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36c47e07-6b7d-4123-a59d-5d465f9ef280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b852ca55-f33e-469f-b7b9-e00d21124927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Implementation Class\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input shape: img_channels x 64 x 64\n",
    "            nn.Conv2d(\n",
    "              in_channels=img_channels, out_channels=features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # Output shape: features_d x 32 x 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1), # Output shape: features_d*2 x 16 x 16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), # Output shape: features_d*4 x 8 x 8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), # Output shape: features_d*8 x 4 x 4\n",
    "           \n",
    "            nn.Conv2d(in_channels=features_d*8, out_channels=1, kernel_size=4, stride=2, padding=0), # Output shape: 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.disc(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5afe02e5-61d3-4f25-8933-8484e519cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels, features_g):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0), # z_dim: (batch_size, 100, 1, 1) -> (batch_size, 1024, 4, 4)\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), # z_dim: (batch_size, 1024, 4, 4) -> (batch_size, 512, 8, 8)\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), # z_dim: (batch_size, 512, 8, 8) -> (batch_size, 256, 16, 16)\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), # z_dim: (batch_size, 128, 16, 16) -> (batch_size, 64, 32, 32)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2, out_channels=img_channels, kernel_size=4, stride=2, padding=1 # z_dim: (batch_size, 64, 32, 32) -> (batch_size, 1, 64, 64)\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.gen(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48c1be35-d58a-4d0c-990d-9250fbdbf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "z_dim = 100\n",
    "img_channels = 1\n",
    "features_disc = 64\n",
    "features_gen = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2dc1f40-5c2e-4903-b2e7-7abe5cae9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instances\n",
    "gen = Generator(z_dim, img_channels, features_gen).to(device)\n",
    "disc = Discriminator(img_channels, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc7a5deb-f8d8-41d4-aed3-d012d39a84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained GAN\n",
    "gen.load_state_dict(torch.load('saved models/Generator.pth', weights_only=True))\n",
    "disc.load_state_dict(torch.load('saved models/Discriminator.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a504e932-e168-4bb8-88cc-eea0810aafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random fake image\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "fake_img = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98b5d73f-b130-4bc1-a868-c225bd2abe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d19338c-18e4-4b6e-842f-a225718fa8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27213297a60>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_img = fake_img.squeeze()\n",
    "print(fake_img.shape)\n",
    "plt.axis(False)\n",
    "plt.title('Fake Generated Image:')\n",
    "plt.imshow(fake_img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf2aefac-a759-4335-a3de-2065d6a6a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Model\n",
    "class TFCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Reshape(),\n",
    "        )\n",
    "\n",
    "        self.h_size = 64 * 4 * 4\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, xs):\n",
    "        code = self.encoder(xs)\n",
    "        logits = self.classifier(code)\n",
    "        return code, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d28876e2-91ee-4ebf-bc52-b71dc82edfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return xs.reshape((xs.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd57210c-2c18-4651-8f29-be94a7a7a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "    def __init__(self, net, init_way, n_classes, input_size=None):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.init_way = init_way\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        model = TFCNN(n_classes)\n",
    "\n",
    "        self.h_size = model.h_size\n",
    "\n",
    "        # Convo and pool layers\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(\n",
    "            self.h_size, self.n_classes, bias=False\n",
    "        )\n",
    "\n",
    "        if self.init_way == \"orth\":\n",
    "            ws = get_orth_weights(self.h_size, self.n_classes)\n",
    "            self.classifier.load_state_dict({\"weight\": ws})\n",
    "\n",
    "    def forward(self, xs):\n",
    "        hs = self.encoder(xs)\n",
    "        logits = self.classifier(hs)\n",
    "        return hs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83ea3c08-0470-4c9a-ba15-8328a316b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target_model(base_net, n_classes, path):\n",
    "    # Create the base model\n",
    "    model = ClassifyNet(net=base_net, init_way='none', n_classes=n_classes)\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0880f1aa-9d08-445a-85ac-f1ff3cebde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Number of parameters in the loaded model: 96704\n"
     ]
    }
   ],
   "source": [
    "# Target Model Loading\n",
    "BASE_NET = 'TFCNN'\n",
    "DATASET = 'face'\n",
    "N_CLASSES = 40\n",
    "\n",
    "target_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedavg_global_model.pth').to(device)\n",
    "print('Model loaded successfully!')\n",
    "target_model.eval()\n",
    "\n",
    "num_params = sum(p.numel() for p in target_model.parameters())\n",
    "print('Number of parameters in the loaded model:', num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fb699c1-2a03-4ea0-b64e-0e2a56dcf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecoveredImage:\n",
    "    def __init__(self, label, latent_vectors):\n",
    "        self.label = label\n",
    "        self.latent_vectors = latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99890363-5891-4750-a50e-76d54ea333ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering Image 0/5. Step 0, Loss: 13.313090324401855\n",
      "Recovering Image 1/5. Step 0, Loss: 13.184553146362305\n",
      "Recovering Image 2/5. Step 0, Loss: 13.087235450744629\n",
      "Recovering Image 3/5. Step 0, Loss: 13.00445556640625\n",
      "Recovering Image 4/5. Step 0, Loss: 12.925421714782715\n",
      "Recovering Image 5/5. Step 0, Loss: 12.849586486816406\n",
      "Recovering Image 6/5. Step 0, Loss: 12.788383483886719\n",
      "Recovering Image 7/5. Step 0, Loss: 12.724252700805664\n",
      "Recovering Image 8/5. Step 0, Loss: 12.656343460083008\n",
      "Recovering Image 9/5. Step 0, Loss: 12.581178665161133\n",
      "Recovering Image 10/5. Step 0, Loss: 12.490716934204102\n",
      "Recovering Image 11/5. Step 0, Loss: 12.39327335357666\n",
      "Recovering Image 12/5. Step 0, Loss: 12.288424491882324\n",
      "Recovering Image 13/5. Step 0, Loss: 12.17317008972168\n",
      "Recovering Image 14/5. Step 0, Loss: 12.036796569824219\n",
      "Recovering Image 15/5. Step 0, Loss: 11.888025283813477\n",
      "Recovering Image 16/5. Step 0, Loss: 11.741721153259277\n",
      "Recovering Image 17/5. Step 0, Loss: 11.586307525634766\n",
      "Recovering Image 18/5. Step 0, Loss: 11.415750503540039\n",
      "Recovering Image 19/5. Step 0, Loss: 11.229231834411621\n",
      "Recovering Image 20/5. Step 0, Loss: 11.031110763549805\n",
      "Recovering Image 21/5. Step 0, Loss: 10.807798385620117\n",
      "Recovering Image 22/5. Step 0, Loss: 10.552765846252441\n",
      "Recovering Image 23/5. Step 0, Loss: 10.254666328430176\n",
      "Recovering Image 24/5. Step 0, Loss: 9.862451553344727\n",
      "Recovering Image 25/5. Step 0, Loss: 9.408988952636719\n",
      "Recovering Image 26/5. Step 0, Loss: 8.905324935913086\n",
      "Recovering Image 27/5. Step 0, Loss: 8.320915222167969\n",
      "Recovering Image 28/5. Step 0, Loss: 7.700202941894531\n",
      "Recovering Image 29/5. Step 0, Loss: 7.150921821594238\n",
      "Recovering Image 30/5. Step 0, Loss: 6.714034080505371\n",
      "Recovering Image 31/5. Step 0, Loss: 6.355600833892822\n",
      "Recovering Image 32/5. Step 0, Loss: 6.069638252258301\n",
      "Recovering Image 33/5. Step 0, Loss: 5.8670501708984375\n",
      "Recovering Image 34/5. Step 0, Loss: 5.707690238952637\n",
      "Recovering Image 35/5. Step 0, Loss: 5.6102294921875\n",
      "Recovering Image 36/5. Step 0, Loss: 5.488426208496094\n",
      "Recovering Image 37/5. Step 0, Loss: 5.385440826416016\n",
      "Recovering Image 38/5. Step 0, Loss: 5.307944297790527\n",
      "Recovering Image 39/5. Step 0, Loss: 5.236213684082031\n",
      "Recovering Image 40/5. Step 0, Loss: 5.189421653747559\n",
      "Recovering Image 41/5. Step 0, Loss: 5.148425102233887\n",
      "Recovering Image 42/5. Step 0, Loss: 5.092665672302246\n",
      "Recovering Image 43/5. Step 0, Loss: 5.033428192138672\n",
      "Recovering Image 44/5. Step 0, Loss: 4.975222110748291\n",
      "Recovering Image 45/5. Step 0, Loss: 4.911900520324707\n",
      "Recovering Image 46/5. Step 0, Loss: 4.855489253997803\n",
      "Recovering Image 47/5. Step 0, Loss: 4.804780006408691\n",
      "Recovering Image 48/5. Step 0, Loss: 4.763332366943359\n",
      "Recovering Image 49/5. Step 0, Loss: 4.719436168670654\n",
      "Recovering Image 50/5. Step 0, Loss: 4.672220230102539\n",
      "Recovering Image 51/5. Step 0, Loss: 4.616975784301758\n",
      "Recovering Image 52/5. Step 0, Loss: 4.575952053070068\n",
      "Recovering Image 53/5. Step 0, Loss: 4.539264678955078\n",
      "Recovering Image 54/5. Step 0, Loss: 4.501586437225342\n",
      "Recovering Image 55/5. Step 0, Loss: 4.473845481872559\n",
      "Recovering Image 56/5. Step 0, Loss: 4.449007987976074\n",
      "Recovering Image 57/5. Step 0, Loss: 4.429795265197754\n",
      "Recovering Image 58/5. Step 0, Loss: 4.407861709594727\n",
      "Recovering Image 59/5. Step 0, Loss: 4.38768196105957\n",
      "Recovering Image 60/5. Step 0, Loss: 4.367196559906006\n",
      "Recovering Image 61/5. Step 0, Loss: 4.345840930938721\n",
      "Recovering Image 62/5. Step 0, Loss: 4.322859287261963\n",
      "Recovering Image 63/5. Step 0, Loss: 4.298101902008057\n",
      "Recovering Image 64/5. Step 0, Loss: 4.274509906768799\n",
      "Recovering Image 65/5. Step 0, Loss: 4.253505706787109\n",
      "Recovering Image 66/5. Step 0, Loss: 4.234302043914795\n",
      "Recovering Image 67/5. Step 0, Loss: 4.216353416442871\n",
      "Recovering Image 68/5. Step 0, Loss: 4.197945594787598\n",
      "Recovering Image 69/5. Step 0, Loss: 4.178153038024902\n",
      "Recovering Image 70/5. Step 0, Loss: 4.1602559089660645\n",
      "Recovering Image 71/5. Step 0, Loss: 4.143170356750488\n",
      "Recovering Image 72/5. Step 0, Loss: 4.125316619873047\n",
      "Recovering Image 73/5. Step 0, Loss: 4.108005523681641\n",
      "Recovering Image 74/5. Step 0, Loss: 4.091058731079102\n",
      "Recovering Image 75/5. Step 0, Loss: 4.074233531951904\n",
      "Recovering Image 76/5. Step 0, Loss: 4.059667587280273\n",
      "Recovering Image 77/5. Step 0, Loss: 4.046980857849121\n",
      "Recovering Image 78/5. Step 0, Loss: 4.0316267013549805\n",
      "Recovering Image 79/5. Step 0, Loss: 4.017792701721191\n",
      "Recovering Image 80/5. Step 0, Loss: 4.00537109375\n",
      "Recovering Image 81/5. Step 0, Loss: 3.9935741424560547\n",
      "Recovering Image 82/5. Step 0, Loss: 3.983790636062622\n",
      "Recovering Image 83/5. Step 0, Loss: 3.9746479988098145\n",
      "Recovering Image 84/5. Step 0, Loss: 3.9669525623321533\n",
      "Recovering Image 85/5. Step 0, Loss: 3.9600024223327637\n",
      "Recovering Image 86/5. Step 0, Loss: 3.9525866508483887\n",
      "Recovering Image 87/5. Step 0, Loss: 3.94558048248291\n",
      "Recovering Image 88/5. Step 0, Loss: 3.9377288818359375\n",
      "Recovering Image 89/5. Step 0, Loss: 3.9297902584075928\n",
      "Recovering Image 90/5. Step 0, Loss: 3.9211513996124268\n",
      "Recovering Image 91/5. Step 0, Loss: 3.9131157398223877\n",
      "Recovering Image 92/5. Step 0, Loss: 3.9055557250976562\n",
      "Recovering Image 93/5. Step 0, Loss: 3.8977208137512207\n",
      "Recovering Image 94/5. Step 0, Loss: 3.8907713890075684\n",
      "Recovering Image 95/5. Step 0, Loss: 3.8838987350463867\n",
      "Recovering Image 96/5. Step 0, Loss: 3.8755221366882324\n",
      "Recovering Image 97/5. Step 0, Loss: 3.8668766021728516\n",
      "Recovering Image 98/5. Step 0, Loss: 3.8587214946746826\n",
      "Recovering Image 99/5. Step 0, Loss: 3.8508007526397705\n",
      "Recovering Image 100/5. Step 0, Loss: 3.8436551094055176\n",
      "Recovering Image 101/5. Step 0, Loss: 3.835444927215576\n",
      "Recovering Image 102/5. Step 0, Loss: 3.827937126159668\n",
      "Recovering Image 103/5. Step 0, Loss: 3.8205950260162354\n",
      "Recovering Image 104/5. Step 0, Loss: 3.812664270401001\n",
      "Recovering Image 105/5. Step 0, Loss: 3.8051910400390625\n",
      "Recovering Image 106/5. Step 0, Loss: 3.7972121238708496\n",
      "Recovering Image 107/5. Step 0, Loss: 3.7898001670837402\n",
      "Recovering Image 108/5. Step 0, Loss: 3.7816686630249023\n",
      "Recovering Image 109/5. Step 0, Loss: 3.7743024826049805\n",
      "Recovering Image 110/5. Step 0, Loss: 3.7662620544433594\n",
      "Recovering Image 111/5. Step 0, Loss: 3.7577617168426514\n",
      "Recovering Image 112/5. Step 0, Loss: 3.7492291927337646\n",
      "Recovering Image 113/5. Step 0, Loss: 3.7411282062530518\n",
      "Recovering Image 114/5. Step 0, Loss: 3.732728958129883\n",
      "Recovering Image 115/5. Step 0, Loss: 3.7242014408111572\n",
      "Recovering Image 116/5. Step 0, Loss: 3.7169299125671387\n",
      "Recovering Image 117/5. Step 0, Loss: 3.709186553955078\n",
      "Recovering Image 118/5. Step 0, Loss: 3.7013473510742188\n",
      "Recovering Image 119/5. Step 0, Loss: 3.693262815475464\n",
      "Recovering Image 120/5. Step 0, Loss: 3.685396909713745\n",
      "Recovering Image 121/5. Step 0, Loss: 3.6775739192962646\n",
      "Recovering Image 122/5. Step 0, Loss: 3.670799732208252\n",
      "Recovering Image 123/5. Step 0, Loss: 3.663576602935791\n",
      "Recovering Image 124/5. Step 0, Loss: 3.656367301940918\n",
      "Recovering Image 125/5. Step 0, Loss: 3.649259328842163\n",
      "Recovering Image 126/5. Step 0, Loss: 3.6425273418426514\n",
      "Recovering Image 127/5. Step 0, Loss: 3.6354498863220215\n",
      "Recovering Image 128/5. Step 0, Loss: 3.628793954849243\n",
      "Recovering Image 129/5. Step 0, Loss: 3.6228063106536865\n",
      "Recovering Image 130/5. Step 0, Loss: 3.6168699264526367\n",
      "Recovering Image 131/5. Step 0, Loss: 3.6108198165893555\n",
      "Recovering Image 132/5. Step 0, Loss: 3.6043663024902344\n",
      "Recovering Image 133/5. Step 0, Loss: 3.598360300064087\n",
      "Recovering Image 134/5. Step 0, Loss: 3.5935287475585938\n",
      "Recovering Image 135/5. Step 0, Loss: 3.5888116359710693\n",
      "Recovering Image 136/5. Step 0, Loss: 3.584716796875\n",
      "Recovering Image 137/5. Step 0, Loss: 3.580869197845459\n",
      "Recovering Image 138/5. Step 0, Loss: 3.5756008625030518\n",
      "Recovering Image 139/5. Step 0, Loss: 3.5712270736694336\n",
      "Recovering Image 140/5. Step 0, Loss: 3.566497802734375\n",
      "Recovering Image 141/5. Step 0, Loss: 3.5621519088745117\n",
      "Recovering Image 142/5. Step 0, Loss: 3.5580687522888184\n",
      "Recovering Image 143/5. Step 0, Loss: 3.553750514984131\n",
      "Recovering Image 144/5. Step 0, Loss: 3.5499329566955566\n",
      "Recovering Image 145/5. Step 0, Loss: 3.5452070236206055\n",
      "Recovering Image 146/5. Step 0, Loss: 3.5409064292907715\n",
      "Recovering Image 147/5. Step 0, Loss: 3.5364229679107666\n",
      "Recovering Image 148/5. Step 0, Loss: 3.5323665142059326\n",
      "Recovering Image 149/5. Step 0, Loss: 3.527794599533081\n",
      "Recovering Image 150/5. Step 0, Loss: 3.523829221725464\n",
      "Recovering Image 151/5. Step 0, Loss: 3.519486665725708\n",
      "Recovering Image 152/5. Step 0, Loss: 3.5156142711639404\n",
      "Recovering Image 153/5. Step 0, Loss: 3.5107603073120117\n",
      "Recovering Image 154/5. Step 0, Loss: 3.5067527294158936\n",
      "Recovering Image 155/5. Step 0, Loss: 3.5019352436065674\n",
      "Recovering Image 156/5. Step 0, Loss: 3.497692584991455\n",
      "Recovering Image 157/5. Step 0, Loss: 3.4936318397521973\n",
      "Recovering Image 158/5. Step 0, Loss: 3.489858627319336\n",
      "Recovering Image 159/5. Step 0, Loss: 3.4858410358428955\n",
      "Recovering Image 160/5. Step 0, Loss: 3.4810240268707275\n",
      "Recovering Image 161/5. Step 0, Loss: 3.4767942428588867\n",
      "Recovering Image 162/5. Step 0, Loss: 3.4723310470581055\n",
      "Recovering Image 163/5. Step 0, Loss: 3.468379020690918\n",
      "Recovering Image 164/5. Step 0, Loss: 3.464017152786255\n",
      "Recovering Image 165/5. Step 0, Loss: 3.4601941108703613\n",
      "Recovering Image 166/5. Step 0, Loss: 3.4560022354125977\n",
      "Recovering Image 167/5. Step 0, Loss: 3.453190803527832\n",
      "Recovering Image 168/5. Step 0, Loss: 3.449685573577881\n",
      "Recovering Image 169/5. Step 0, Loss: 3.446451187133789\n",
      "Recovering Image 170/5. Step 0, Loss: 3.4441773891448975\n",
      "Recovering Image 171/5. Step 0, Loss: 3.440873622894287\n",
      "Recovering Image 172/5. Step 0, Loss: 3.438086986541748\n",
      "Recovering Image 173/5. Step 0, Loss: 3.4347658157348633\n",
      "Recovering Image 174/5. Step 0, Loss: 3.4315924644470215\n",
      "Recovering Image 175/5. Step 0, Loss: 3.428344964981079\n",
      "Recovering Image 176/5. Step 0, Loss: 3.424898624420166\n",
      "Recovering Image 177/5. Step 0, Loss: 3.42238712310791\n",
      "Recovering Image 178/5. Step 0, Loss: 3.418921947479248\n",
      "Recovering Image 179/5. Step 0, Loss: 3.414937734603882\n",
      "Recovering Image 180/5. Step 0, Loss: 3.411771059036255\n",
      "Recovering Image 181/5. Step 0, Loss: 3.4076650142669678\n",
      "Recovering Image 182/5. Step 0, Loss: 3.4043827056884766\n",
      "Recovering Image 183/5. Step 0, Loss: 3.400454044342041\n",
      "Recovering Image 184/5. Step 0, Loss: 3.3968851566314697\n",
      "Recovering Image 185/5. Step 0, Loss: 3.393303394317627\n",
      "Recovering Image 186/5. Step 0, Loss: 3.390348434448242\n",
      "Recovering Image 187/5. Step 0, Loss: 3.3869082927703857\n",
      "Recovering Image 188/5. Step 0, Loss: 3.3839170932769775\n",
      "Recovering Image 189/5. Step 0, Loss: 3.381479024887085\n",
      "Recovering Image 190/5. Step 0, Loss: 3.37900447845459\n",
      "Recovering Image 191/5. Step 0, Loss: 3.376278877258301\n",
      "Recovering Image 192/5. Step 0, Loss: 3.3741674423217773\n",
      "Recovering Image 193/5. Step 0, Loss: 3.3718984127044678\n",
      "Recovering Image 194/5. Step 0, Loss: 3.3693037033081055\n",
      "Recovering Image 195/5. Step 0, Loss: 3.3675050735473633\n",
      "Recovering Image 196/5. Step 0, Loss: 3.3645124435424805\n",
      "Recovering Image 197/5. Step 0, Loss: 3.3619308471679688\n",
      "Recovering Image 198/5. Step 0, Loss: 3.35982608795166\n",
      "Recovering Image 199/5. Step 0, Loss: 3.3576838970184326\n",
      "Recovering Image 200/5. Step 0, Loss: 3.3552138805389404\n",
      "Recovering Image 201/5. Step 0, Loss: 3.352461338043213\n",
      "Recovering Image 202/5. Step 0, Loss: 3.350044012069702\n",
      "Recovering Image 203/5. Step 0, Loss: 3.347256660461426\n",
      "Recovering Image 204/5. Step 0, Loss: 3.345121383666992\n",
      "Recovering Image 205/5. Step 0, Loss: 3.3429269790649414\n",
      "Recovering Image 206/5. Step 0, Loss: 3.3405535221099854\n",
      "Recovering Image 207/5. Step 0, Loss: 3.33834171295166\n",
      "Recovering Image 208/5. Step 0, Loss: 3.336156129837036\n",
      "Recovering Image 209/5. Step 0, Loss: 3.3339014053344727\n",
      "Recovering Image 210/5. Step 0, Loss: 3.3322317600250244\n",
      "Recovering Image 211/5. Step 0, Loss: 3.329580307006836\n",
      "Recovering Image 212/5. Step 0, Loss: 3.327507972717285\n",
      "Recovering Image 213/5. Step 0, Loss: 3.3258275985717773\n",
      "Recovering Image 214/5. Step 0, Loss: 3.3242111206054688\n",
      "Recovering Image 215/5. Step 0, Loss: 3.321469306945801\n",
      "Recovering Image 216/5. Step 0, Loss: 3.319770336151123\n",
      "Recovering Image 217/5. Step 0, Loss: 3.317915916442871\n",
      "Recovering Image 218/5. Step 0, Loss: 3.3156871795654297\n",
      "Recovering Image 219/5. Step 0, Loss: 3.3137922286987305\n",
      "Recovering Image 220/5. Step 0, Loss: 3.3116979598999023\n",
      "Recovering Image 221/5. Step 0, Loss: 3.310326337814331\n",
      "Recovering Image 222/5. Step 0, Loss: 3.308563470840454\n",
      "Recovering Image 223/5. Step 0, Loss: 3.3063998222351074\n",
      "Recovering Image 224/5. Step 0, Loss: 3.3044745922088623\n",
      "Recovering Image 225/5. Step 0, Loss: 3.3025119304656982\n",
      "Recovering Image 226/5. Step 0, Loss: 3.30069899559021\n",
      "Recovering Image 227/5. Step 0, Loss: 3.298255443572998\n",
      "Recovering Image 228/5. Step 0, Loss: 3.2957844734191895\n",
      "Recovering Image 229/5. Step 0, Loss: 3.2945146560668945\n",
      "Recovering Image 230/5. Step 0, Loss: 3.2930068969726562\n",
      "Recovering Image 231/5. Step 0, Loss: 3.290536880493164\n",
      "Recovering Image 232/5. Step 0, Loss: 3.288301706314087\n",
      "Recovering Image 233/5. Step 0, Loss: 3.286280632019043\n",
      "Recovering Image 234/5. Step 0, Loss: 3.2834525108337402\n",
      "Recovering Image 235/5. Step 0, Loss: 3.281888961791992\n",
      "Recovering Image 236/5. Step 0, Loss: 3.279448986053467\n",
      "Recovering Image 237/5. Step 0, Loss: 3.277169704437256\n",
      "Recovering Image 238/5. Step 0, Loss: 3.2744650840759277\n",
      "Recovering Image 239/5. Step 0, Loss: 3.272477626800537\n",
      "Recovering Image 240/5. Step 0, Loss: 3.270918846130371\n",
      "Recovering Image 241/5. Step 0, Loss: 3.269265651702881\n",
      "Recovering Image 242/5. Step 0, Loss: 3.267075538635254\n",
      "Recovering Image 243/5. Step 0, Loss: 3.2654428482055664\n",
      "Recovering Image 244/5. Step 0, Loss: 3.2633581161499023\n",
      "Recovering Image 245/5. Step 0, Loss: 3.260366439819336\n",
      "Recovering Image 246/5. Step 0, Loss: 3.2585296630859375\n",
      "Recovering Image 247/5. Step 0, Loss: 3.255855083465576\n",
      "Recovering Image 248/5. Step 0, Loss: 3.2533445358276367\n",
      "Recovering Image 249/5. Step 0, Loss: 3.2511305809020996\n",
      "Recovering Image 250/5. Step 0, Loss: 3.2483553886413574\n",
      "Recovering Image 251/5. Step 0, Loss: 3.247130870819092\n",
      "Recovering Image 252/5. Step 0, Loss: 3.244152784347534\n",
      "Recovering Image 253/5. Step 0, Loss: 3.2417142391204834\n",
      "Recovering Image 254/5. Step 0, Loss: 3.2398133277893066\n",
      "Recovering Image 255/5. Step 0, Loss: 3.2367103099823\n",
      "Recovering Image 256/5. Step 0, Loss: 3.23476243019104\n",
      "Recovering Image 257/5. Step 0, Loss: 3.232591152191162\n",
      "Recovering Image 258/5. Step 0, Loss: 3.2303853034973145\n",
      "Recovering Image 259/5. Step 0, Loss: 3.2284677028656006\n",
      "Recovering Image 260/5. Step 0, Loss: 3.2264328002929688\n",
      "Recovering Image 261/5. Step 0, Loss: 3.224545478820801\n",
      "Recovering Image 262/5. Step 0, Loss: 3.2216949462890625\n",
      "Recovering Image 263/5. Step 0, Loss: 3.2196946144104004\n",
      "Recovering Image 264/5. Step 0, Loss: 3.218177318572998\n",
      "Recovering Image 265/5. Step 0, Loss: 3.2160799503326416\n",
      "Recovering Image 266/5. Step 0, Loss: 3.213923454284668\n",
      "Recovering Image 267/5. Step 0, Loss: 3.212399959564209\n",
      "Recovering Image 268/5. Step 0, Loss: 3.2103209495544434\n",
      "Recovering Image 269/5. Step 0, Loss: 3.208104372024536\n",
      "Recovering Image 270/5. Step 0, Loss: 3.2063064575195312\n",
      "Recovering Image 271/5. Step 0, Loss: 3.2042953968048096\n",
      "Recovering Image 272/5. Step 0, Loss: 3.201432704925537\n",
      "Recovering Image 273/5. Step 0, Loss: 3.200362205505371\n",
      "Recovering Image 274/5. Step 0, Loss: 3.1976027488708496\n",
      "Recovering Image 275/5. Step 0, Loss: 3.195748805999756\n",
      "Recovering Image 276/5. Step 0, Loss: 3.1944162845611572\n",
      "Recovering Image 277/5. Step 0, Loss: 3.192085027694702\n",
      "Recovering Image 278/5. Step 0, Loss: 3.189631462097168\n",
      "Recovering Image 279/5. Step 0, Loss: 3.188013792037964\n",
      "Recovering Image 280/5. Step 0, Loss: 3.18612003326416\n",
      "Recovering Image 281/5. Step 0, Loss: 3.1844534873962402\n",
      "Recovering Image 282/5. Step 0, Loss: 3.182218551635742\n",
      "Recovering Image 283/5. Step 0, Loss: 3.180044174194336\n",
      "Recovering Image 284/5. Step 0, Loss: 3.1781439781188965\n",
      "Recovering Image 285/5. Step 0, Loss: 3.176483631134033\n",
      "Recovering Image 286/5. Step 0, Loss: 3.1743736267089844\n",
      "Recovering Image 287/5. Step 0, Loss: 3.173074960708618\n",
      "Recovering Image 288/5. Step 0, Loss: 3.1709742546081543\n",
      "Recovering Image 289/5. Step 0, Loss: 3.1693944931030273\n",
      "Recovering Image 290/5. Step 0, Loss: 3.167590618133545\n",
      "Recovering Image 291/5. Step 0, Loss: 3.1657607555389404\n",
      "Recovering Image 292/5. Step 0, Loss: 3.1635665893554688\n",
      "Recovering Image 293/5. Step 0, Loss: 3.1616034507751465\n",
      "Recovering Image 294/5. Step 0, Loss: 3.1604204177856445\n",
      "Recovering Image 295/5. Step 0, Loss: 3.158560276031494\n",
      "Recovering Image 296/5. Step 0, Loss: 3.1567440032958984\n",
      "Recovering Image 297/5. Step 0, Loss: 3.154982805252075\n",
      "Recovering Image 298/5. Step 0, Loss: 3.153257369995117\n",
      "Recovering Image 299/5. Step 0, Loss: 3.151160478591919\n",
      "Recovering Image 300/5. Step 0, Loss: 3.1494781970977783\n",
      "Recovering Image 301/5. Step 0, Loss: 3.147064685821533\n",
      "Recovering Image 302/5. Step 0, Loss: 3.145139694213867\n",
      "Recovering Image 303/5. Step 0, Loss: 3.1435165405273438\n",
      "Recovering Image 304/5. Step 0, Loss: 3.1404976844787598\n",
      "Recovering Image 305/5. Step 0, Loss: 3.1383087635040283\n",
      "Recovering Image 306/5. Step 0, Loss: 3.1360201835632324\n",
      "Recovering Image 307/5. Step 0, Loss: 3.1342008113861084\n",
      "Recovering Image 308/5. Step 0, Loss: 3.1316614151000977\n",
      "Recovering Image 309/5. Step 0, Loss: 3.129751205444336\n",
      "Recovering Image 310/5. Step 0, Loss: 3.127650260925293\n",
      "Recovering Image 311/5. Step 0, Loss: 3.125603675842285\n",
      "Recovering Image 312/5. Step 0, Loss: 3.1228508949279785\n",
      "Recovering Image 313/5. Step 0, Loss: 3.1206610202789307\n",
      "Recovering Image 314/5. Step 0, Loss: 3.1180672645568848\n",
      "Recovering Image 315/5. Step 0, Loss: 3.116868495941162\n",
      "Recovering Image 316/5. Step 0, Loss: 3.1141233444213867\n",
      "Recovering Image 317/5. Step 0, Loss: 3.1117968559265137\n",
      "Recovering Image 318/5. Step 0, Loss: 3.11044979095459\n",
      "Recovering Image 319/5. Step 0, Loss: 3.109008312225342\n",
      "Recovering Image 320/5. Step 0, Loss: 3.1067850589752197\n",
      "Recovering Image 321/5. Step 0, Loss: 3.1056084632873535\n",
      "Recovering Image 322/5. Step 0, Loss: 3.1031670570373535\n",
      "Recovering Image 323/5. Step 0, Loss: 3.100935459136963\n",
      "Recovering Image 324/5. Step 0, Loss: 3.0995426177978516\n",
      "Recovering Image 325/5. Step 0, Loss: 3.0968360900878906\n",
      "Recovering Image 326/5. Step 0, Loss: 3.0956311225891113\n",
      "Recovering Image 327/5. Step 0, Loss: 3.0944409370422363\n",
      "Recovering Image 328/5. Step 0, Loss: 3.0924134254455566\n",
      "Recovering Image 329/5. Step 0, Loss: 3.090304136276245\n",
      "Recovering Image 330/5. Step 0, Loss: 3.0883984565734863\n",
      "Recovering Image 331/5. Step 0, Loss: 3.0868258476257324\n",
      "Recovering Image 332/5. Step 0, Loss: 3.0851259231567383\n",
      "Recovering Image 333/5. Step 0, Loss: 3.0831668376922607\n",
      "Recovering Image 334/5. Step 0, Loss: 3.080859661102295\n",
      "Recovering Image 335/5. Step 0, Loss: 3.078568935394287\n",
      "Recovering Image 336/5. Step 0, Loss: 3.0768661499023438\n",
      "Recovering Image 337/5. Step 0, Loss: 3.07462739944458\n",
      "Recovering Image 338/5. Step 0, Loss: 3.07245135307312\n",
      "Recovering Image 339/5. Step 0, Loss: 3.0701420307159424\n",
      "Recovering Image 340/5. Step 0, Loss: 3.068399429321289\n",
      "Recovering Image 341/5. Step 0, Loss: 3.0666465759277344\n",
      "Recovering Image 342/5. Step 0, Loss: 3.0643012523651123\n",
      "Recovering Image 343/5. Step 0, Loss: 3.062906265258789\n",
      "Recovering Image 344/5. Step 0, Loss: 3.0612411499023438\n",
      "Recovering Image 345/5. Step 0, Loss: 3.059666633605957\n",
      "Recovering Image 346/5. Step 0, Loss: 3.057506561279297\n",
      "Recovering Image 347/5. Step 0, Loss: 3.0558383464813232\n",
      "Recovering Image 348/5. Step 0, Loss: 3.05471134185791\n",
      "Recovering Image 349/5. Step 0, Loss: 3.052712917327881\n",
      "Recovering Image 350/5. Step 0, Loss: 3.051058292388916\n",
      "Recovering Image 351/5. Step 0, Loss: 3.04923939704895\n",
      "Recovering Image 352/5. Step 0, Loss: 3.047649621963501\n",
      "Recovering Image 353/5. Step 0, Loss: 3.0456669330596924\n",
      "Recovering Image 354/5. Step 0, Loss: 3.0439863204956055\n",
      "Recovering Image 355/5. Step 0, Loss: 3.0418570041656494\n",
      "Recovering Image 356/5. Step 0, Loss: 3.0392093658447266\n",
      "Recovering Image 357/5. Step 0, Loss: 3.0377752780914307\n",
      "Recovering Image 358/5. Step 0, Loss: 3.0351316928863525\n",
      "Recovering Image 359/5. Step 0, Loss: 3.033034324645996\n",
      "Recovering Image 360/5. Step 0, Loss: 3.030956506729126\n",
      "Recovering Image 361/5. Step 0, Loss: 3.0287654399871826\n",
      "Recovering Image 362/5. Step 0, Loss: 3.0266337394714355\n",
      "Recovering Image 363/5. Step 0, Loss: 3.024533987045288\n",
      "Recovering Image 364/5. Step 0, Loss: 3.0223124027252197\n",
      "Recovering Image 365/5. Step 0, Loss: 3.0202648639678955\n",
      "Recovering Image 366/5. Step 0, Loss: 3.017338275909424\n",
      "Recovering Image 367/5. Step 0, Loss: 3.01528263092041\n",
      "Recovering Image 368/5. Step 0, Loss: 3.0125553607940674\n",
      "Recovering Image 369/5. Step 0, Loss: 3.0098206996917725\n",
      "Recovering Image 370/5. Step 0, Loss: 3.0066983699798584\n",
      "Recovering Image 371/5. Step 0, Loss: 3.004209518432617\n",
      "Recovering Image 372/5. Step 0, Loss: 3.0005617141723633\n",
      "Recovering Image 373/5. Step 0, Loss: 2.996591567993164\n",
      "Recovering Image 374/5. Step 0, Loss: 2.993439197540283\n",
      "Recovering Image 375/5. Step 0, Loss: 2.9889960289001465\n",
      "Recovering Image 376/5. Step 0, Loss: 2.9845099449157715\n",
      "Recovering Image 377/5. Step 0, Loss: 2.9801180362701416\n",
      "Recovering Image 378/5. Step 0, Loss: 2.97676944732666\n",
      "Recovering Image 379/5. Step 0, Loss: 2.9730923175811768\n",
      "Recovering Image 380/5. Step 0, Loss: 2.969944715499878\n",
      "Recovering Image 381/5. Step 0, Loss: 2.966813564300537\n",
      "Recovering Image 382/5. Step 0, Loss: 2.963149309158325\n",
      "Recovering Image 383/5. Step 0, Loss: 2.9599130153656006\n",
      "Recovering Image 384/5. Step 0, Loss: 2.9565236568450928\n",
      "Recovering Image 385/5. Step 0, Loss: 2.9525582790374756\n",
      "Recovering Image 386/5. Step 0, Loss: 2.949873685836792\n",
      "Recovering Image 387/5. Step 0, Loss: 2.9472241401672363\n",
      "Recovering Image 388/5. Step 0, Loss: 2.944380044937134\n",
      "Recovering Image 389/5. Step 0, Loss: 2.94144868850708\n",
      "Recovering Image 390/5. Step 0, Loss: 2.9370741844177246\n",
      "Recovering Image 391/5. Step 0, Loss: 2.932007312774658\n",
      "Recovering Image 392/5. Step 0, Loss: 2.9264636039733887\n",
      "Recovering Image 393/5. Step 0, Loss: 2.920320987701416\n",
      "Recovering Image 394/5. Step 0, Loss: 2.9152276515960693\n",
      "Recovering Image 395/5. Step 0, Loss: 2.910616874694824\n",
      "Recovering Image 396/5. Step 0, Loss: 2.9044437408447266\n",
      "Recovering Image 397/5. Step 0, Loss: 2.899075984954834\n",
      "Recovering Image 398/5. Step 0, Loss: 2.893854856491089\n",
      "Recovering Image 399/5. Step 0, Loss: 2.891791582107544\n",
      "Recovering Image 400/5. Step 0, Loss: 2.889216184616089\n",
      "Recovering Image 401/5. Step 0, Loss: 2.8851799964904785\n",
      "Recovering Image 402/5. Step 0, Loss: 2.8802855014801025\n",
      "Recovering Image 403/5. Step 0, Loss: 2.8757452964782715\n",
      "Recovering Image 404/5. Step 0, Loss: 2.8726372718811035\n",
      "Recovering Image 405/5. Step 0, Loss: 2.869234085083008\n",
      "Recovering Image 406/5. Step 0, Loss: 2.8652162551879883\n",
      "Recovering Image 407/5. Step 0, Loss: 2.8600332736968994\n",
      "Recovering Image 408/5. Step 0, Loss: 2.8547658920288086\n",
      "Recovering Image 409/5. Step 0, Loss: 2.850511074066162\n",
      "Recovering Image 410/5. Step 0, Loss: 2.846281051635742\n",
      "Recovering Image 411/5. Step 0, Loss: 2.8410873413085938\n",
      "Recovering Image 412/5. Step 0, Loss: 2.8344216346740723\n",
      "Recovering Image 413/5. Step 0, Loss: 2.8283352851867676\n",
      "Recovering Image 414/5. Step 0, Loss: 2.823211669921875\n",
      "Recovering Image 415/5. Step 0, Loss: 2.8183488845825195\n",
      "Recovering Image 416/5. Step 0, Loss: 2.8131422996520996\n",
      "Recovering Image 417/5. Step 0, Loss: 2.808030605316162\n",
      "Recovering Image 418/5. Step 0, Loss: 2.8023500442504883\n",
      "Recovering Image 419/5. Step 0, Loss: 2.797607660293579\n",
      "Recovering Image 420/5. Step 0, Loss: 2.7950832843780518\n",
      "Recovering Image 421/5. Step 0, Loss: 2.791978120803833\n",
      "Recovering Image 422/5. Step 0, Loss: 2.788642406463623\n",
      "Recovering Image 423/5. Step 0, Loss: 2.7837371826171875\n",
      "Recovering Image 424/5. Step 0, Loss: 2.7797460556030273\n",
      "Recovering Image 425/5. Step 0, Loss: 2.7775204181671143\n",
      "Recovering Image 426/5. Step 0, Loss: 2.774313449859619\n",
      "Recovering Image 427/5. Step 0, Loss: 2.770514965057373\n",
      "Recovering Image 428/5. Step 0, Loss: 2.7676334381103516\n",
      "Recovering Image 429/5. Step 0, Loss: 2.764427661895752\n",
      "Recovering Image 430/5. Step 0, Loss: 2.761848211288452\n",
      "Recovering Image 431/5. Step 0, Loss: 2.759039878845215\n",
      "Recovering Image 432/5. Step 0, Loss: 2.755610942840576\n",
      "Recovering Image 433/5. Step 0, Loss: 2.7541139125823975\n",
      "Recovering Image 434/5. Step 0, Loss: 2.7512521743774414\n",
      "Recovering Image 435/5. Step 0, Loss: 2.7474756240844727\n",
      "Recovering Image 436/5. Step 0, Loss: 2.7448811531066895\n",
      "Recovering Image 437/5. Step 0, Loss: 2.7421112060546875\n",
      "Recovering Image 438/5. Step 0, Loss: 2.740199327468872\n",
      "Recovering Image 439/5. Step 0, Loss: 2.737696409225464\n",
      "Recovering Image 440/5. Step 0, Loss: 2.7353286743164062\n",
      "Recovering Image 441/5. Step 0, Loss: 2.731990337371826\n",
      "Recovering Image 442/5. Step 0, Loss: 2.729764223098755\n",
      "Recovering Image 443/5. Step 0, Loss: 2.727344512939453\n",
      "Recovering Image 444/5. Step 0, Loss: 2.7246367931365967\n",
      "Recovering Image 445/5. Step 0, Loss: 2.7223503589630127\n",
      "Recovering Image 446/5. Step 0, Loss: 2.7202937602996826\n",
      "Recovering Image 447/5. Step 0, Loss: 2.7173659801483154\n",
      "Recovering Image 448/5. Step 0, Loss: 2.7156295776367188\n",
      "Recovering Image 449/5. Step 0, Loss: 2.7134861946105957\n",
      "Recovering Image 450/5. Step 0, Loss: 2.711064338684082\n",
      "Recovering Image 451/5. Step 0, Loss: 2.7077324390411377\n",
      "Recovering Image 452/5. Step 0, Loss: 2.70546555519104\n",
      "Recovering Image 453/5. Step 0, Loss: 2.7036681175231934\n",
      "Recovering Image 454/5. Step 0, Loss: 2.7003941535949707\n",
      "Recovering Image 455/5. Step 0, Loss: 2.6985390186309814\n",
      "Recovering Image 456/5. Step 0, Loss: 2.697505474090576\n",
      "Recovering Image 457/5. Step 0, Loss: 2.695068359375\n",
      "Recovering Image 458/5. Step 0, Loss: 2.6930484771728516\n",
      "Recovering Image 459/5. Step 0, Loss: 2.6917898654937744\n",
      "Recovering Image 460/5. Step 0, Loss: 2.6898741722106934\n",
      "Recovering Image 461/5. Step 0, Loss: 2.688011884689331\n",
      "Recovering Image 462/5. Step 0, Loss: 2.6863958835601807\n",
      "Recovering Image 463/5. Step 0, Loss: 2.6848902702331543\n",
      "Recovering Image 464/5. Step 0, Loss: 2.683231830596924\n",
      "Recovering Image 465/5. Step 0, Loss: 2.681168794631958\n",
      "Recovering Image 466/5. Step 0, Loss: 2.68003511428833\n",
      "Recovering Image 467/5. Step 0, Loss: 2.677974224090576\n",
      "Recovering Image 468/5. Step 0, Loss: 2.6764659881591797\n",
      "Recovering Image 469/5. Step 0, Loss: 2.6752755641937256\n",
      "Recovering Image 470/5. Step 0, Loss: 2.673780679702759\n",
      "Recovering Image 471/5. Step 0, Loss: 2.672243118286133\n",
      "Recovering Image 472/5. Step 0, Loss: 2.670360565185547\n",
      "Recovering Image 473/5. Step 0, Loss: 2.6688575744628906\n",
      "Recovering Image 474/5. Step 0, Loss: 2.667001724243164\n",
      "Recovering Image 475/5. Step 0, Loss: 2.6658596992492676\n",
      "Recovering Image 476/5. Step 0, Loss: 2.6643404960632324\n",
      "Recovering Image 477/5. Step 0, Loss: 2.6625514030456543\n",
      "Recovering Image 478/5. Step 0, Loss: 2.6614432334899902\n",
      "Recovering Image 479/5. Step 0, Loss: 2.6601028442382812\n",
      "Recovering Image 480/5. Step 0, Loss: 2.6583287715911865\n",
      "Recovering Image 481/5. Step 0, Loss: 2.6572213172912598\n",
      "Recovering Image 482/5. Step 0, Loss: 2.6556451320648193\n",
      "Recovering Image 483/5. Step 0, Loss: 2.654815673828125\n",
      "Recovering Image 484/5. Step 0, Loss: 2.652972459793091\n",
      "Recovering Image 485/5. Step 0, Loss: 2.652348041534424\n",
      "Recovering Image 486/5. Step 0, Loss: 2.650531530380249\n",
      "Recovering Image 487/5. Step 0, Loss: 2.6493301391601562\n",
      "Recovering Image 488/5. Step 0, Loss: 2.6476423740386963\n",
      "Recovering Image 489/5. Step 0, Loss: 2.6460883617401123\n",
      "Recovering Image 490/5. Step 0, Loss: 2.6449713706970215\n",
      "Recovering Image 491/5. Step 0, Loss: 2.6435275077819824\n",
      "Recovering Image 492/5. Step 0, Loss: 2.6419501304626465\n",
      "Recovering Image 493/5. Step 0, Loss: 2.6408851146698\n",
      "Recovering Image 494/5. Step 0, Loss: 2.639086961746216\n",
      "Recovering Image 495/5. Step 0, Loss: 2.6375410556793213\n",
      "Recovering Image 496/5. Step 0, Loss: 2.6364240646362305\n",
      "Recovering Image 497/5. Step 0, Loss: 2.6349339485168457\n",
      "Recovering Image 498/5. Step 0, Loss: 2.6334595680236816\n",
      "Recovering Image 499/5. Step 0, Loss: 2.632070541381836\n",
      "Recovering Image 500/5. Step 0, Loss: 2.630830764770508\n",
      "Recovering Image 501/5. Step 0, Loss: 2.6296772956848145\n",
      "Recovering Image 502/5. Step 0, Loss: 2.6274847984313965\n",
      "Recovering Image 503/5. Step 0, Loss: 2.6263766288757324\n",
      "Recovering Image 504/5. Step 0, Loss: 2.6247572898864746\n",
      "Recovering Image 505/5. Step 0, Loss: 2.6237733364105225\n",
      "Recovering Image 506/5. Step 0, Loss: 2.621898889541626\n",
      "Recovering Image 507/5. Step 0, Loss: 2.6205127239227295\n",
      "Recovering Image 508/5. Step 0, Loss: 2.6196086406707764\n",
      "Recovering Image 509/5. Step 0, Loss: 2.6180148124694824\n",
      "Recovering Image 510/5. Step 0, Loss: 2.6167171001434326\n",
      "Recovering Image 511/5. Step 0, Loss: 2.615542411804199\n",
      "Recovering Image 512/5. Step 0, Loss: 2.614492654800415\n",
      "Recovering Image 513/5. Step 0, Loss: 2.6129603385925293\n",
      "Recovering Image 514/5. Step 0, Loss: 2.6119871139526367\n",
      "Recovering Image 515/5. Step 0, Loss: 2.610316753387451\n",
      "Recovering Image 516/5. Step 0, Loss: 2.609285593032837\n",
      "Recovering Image 517/5. Step 0, Loss: 2.608273506164551\n",
      "Recovering Image 518/5. Step 0, Loss: 2.606590747833252\n",
      "Recovering Image 519/5. Step 0, Loss: 2.605189800262451\n",
      "Recovering Image 520/5. Step 0, Loss: 2.6041102409362793\n",
      "Recovering Image 521/5. Step 0, Loss: 2.6025331020355225\n",
      "Recovering Image 522/5. Step 0, Loss: 2.6014509201049805\n",
      "Recovering Image 523/5. Step 0, Loss: 2.599823474884033\n",
      "Recovering Image 524/5. Step 0, Loss: 2.5987372398376465\n",
      "Recovering Image 525/5. Step 0, Loss: 2.5972366333007812\n",
      "Recovering Image 526/5. Step 0, Loss: 2.595917224884033\n",
      "Recovering Image 527/5. Step 0, Loss: 2.5946130752563477\n",
      "Recovering Image 528/5. Step 0, Loss: 2.5933499336242676\n",
      "Recovering Image 529/5. Step 0, Loss: 2.592423915863037\n",
      "Recovering Image 530/5. Step 0, Loss: 2.5908584594726562\n",
      "Recovering Image 531/5. Step 0, Loss: 2.5891261100769043\n",
      "Recovering Image 532/5. Step 0, Loss: 2.587686061859131\n",
      "Recovering Image 533/5. Step 0, Loss: 2.5865426063537598\n",
      "Recovering Image 534/5. Step 0, Loss: 2.585252285003662\n",
      "Recovering Image 535/5. Step 0, Loss: 2.583944082260132\n",
      "Recovering Image 536/5. Step 0, Loss: 2.5820164680480957\n",
      "Recovering Image 537/5. Step 0, Loss: 2.580461025238037\n",
      "Recovering Image 538/5. Step 0, Loss: 2.579484462738037\n",
      "Recovering Image 539/5. Step 0, Loss: 2.578254222869873\n",
      "Recovering Image 540/5. Step 0, Loss: 2.576348304748535\n",
      "Recovering Image 541/5. Step 0, Loss: 2.5749294757843018\n",
      "Recovering Image 542/5. Step 0, Loss: 2.5730762481689453\n",
      "Recovering Image 543/5. Step 0, Loss: 2.5718894004821777\n",
      "Recovering Image 544/5. Step 0, Loss: 2.5700085163116455\n",
      "Recovering Image 545/5. Step 0, Loss: 2.567938804626465\n",
      "Recovering Image 546/5. Step 0, Loss: 2.5665206909179688\n",
      "Recovering Image 547/5. Step 0, Loss: 2.5649473667144775\n",
      "Recovering Image 548/5. Step 0, Loss: 2.563133716583252\n",
      "Recovering Image 549/5. Step 0, Loss: 2.5612778663635254\n",
      "Recovering Image 550/5. Step 0, Loss: 2.5599331855773926\n",
      "Recovering Image 551/5. Step 0, Loss: 2.558684825897217\n",
      "Recovering Image 552/5. Step 0, Loss: 2.5569570064544678\n",
      "Recovering Image 553/5. Step 0, Loss: 2.5553362369537354\n",
      "Recovering Image 554/5. Step 0, Loss: 2.5536022186279297\n",
      "Recovering Image 555/5. Step 0, Loss: 2.5520758628845215\n",
      "Recovering Image 556/5. Step 0, Loss: 2.5501954555511475\n",
      "Recovering Image 557/5. Step 0, Loss: 2.549011707305908\n",
      "Recovering Image 558/5. Step 0, Loss: 2.5472617149353027\n",
      "Recovering Image 559/5. Step 0, Loss: 2.544975757598877\n",
      "Recovering Image 560/5. Step 0, Loss: 2.543173313140869\n",
      "Recovering Image 561/5. Step 0, Loss: 2.5415608882904053\n",
      "Recovering Image 562/5. Step 0, Loss: 2.539928674697876\n",
      "Recovering Image 563/5. Step 0, Loss: 2.5379772186279297\n",
      "Recovering Image 564/5. Step 0, Loss: 2.5365967750549316\n",
      "Recovering Image 565/5. Step 0, Loss: 2.5352683067321777\n",
      "Recovering Image 566/5. Step 0, Loss: 2.533249616622925\n",
      "Recovering Image 567/5. Step 0, Loss: 2.5322656631469727\n",
      "Recovering Image 568/5. Step 0, Loss: 2.5307090282440186\n",
      "Recovering Image 569/5. Step 0, Loss: 2.5291943550109863\n",
      "Recovering Image 570/5. Step 0, Loss: 2.527956008911133\n",
      "Recovering Image 571/5. Step 0, Loss: 2.526369571685791\n",
      "Recovering Image 572/5. Step 0, Loss: 2.524949312210083\n",
      "Recovering Image 573/5. Step 0, Loss: 2.5237765312194824\n",
      "Recovering Image 574/5. Step 0, Loss: 2.522498607635498\n",
      "Recovering Image 575/5. Step 0, Loss: 2.5213119983673096\n",
      "Recovering Image 576/5. Step 0, Loss: 2.5200018882751465\n",
      "Recovering Image 577/5. Step 0, Loss: 2.51893949508667\n",
      "Recovering Image 578/5. Step 0, Loss: 2.5180749893188477\n",
      "Recovering Image 579/5. Step 0, Loss: 2.5166428089141846\n",
      "Recovering Image 580/5. Step 0, Loss: 2.515773057937622\n",
      "Recovering Image 581/5. Step 0, Loss: 2.51473331451416\n",
      "Recovering Image 582/5. Step 0, Loss: 2.5135996341705322\n",
      "Recovering Image 583/5. Step 0, Loss: 2.5122909545898438\n",
      "Recovering Image 584/5. Step 0, Loss: 2.511326313018799\n",
      "Recovering Image 585/5. Step 0, Loss: 2.5105013847351074\n",
      "Recovering Image 586/5. Step 0, Loss: 2.509047031402588\n",
      "Recovering Image 587/5. Step 0, Loss: 2.5081071853637695\n",
      "Recovering Image 588/5. Step 0, Loss: 2.5067591667175293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rounds):\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     generated_image \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Generate an image from the latent vector\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#generated_image = generated_image.repeat(1, 1, 1, 1)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     generated_image \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39minterpolate(generated_image, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[41], line 26\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recovered_images = []\n",
    "\n",
    "for j in range(0, 5):\n",
    "    # Define a learnable latent vector\n",
    "    z = torch.randn(1, z_dim, 1, 1, requires_grad=True)  # Start with random noise\n",
    "    z.shape\n",
    "\n",
    "    target_class = torch.tensor([i])  # Set the target class\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam([z], lr=learning_rate)\n",
    "    \n",
    "    # Loss function (e.g., Cross-Entropy with classifier outputs)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    rounds = 1000\n",
    "    for x in range(rounds):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        generated_image = gen(z.to(device))  # Generate an image from the latent vector\n",
    "        #generated_image = generated_image.repeat(1, 1, 1, 1)\n",
    "        generated_image = functional.interpolate(generated_image, size=(32, 32), mode='nearest')\n",
    "        #print(generated_image.shape)\n",
    "        hs, prediction_logits = target_model(generated_image)  # Classifier output\n",
    "        #print(prediction_logits)\n",
    "        #print(type(prediction_logits))\n",
    "        loss = loss_fn(prediction_logits, target_class.to(device))  # Minimize difference with true class\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Recovering Image {x}/5. Step {i}, Loss: {loss.item()}\")\n",
    "            \n",
    "    recovered_images.append(RecoveredImage(i, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb4c9f-b4f7-4820-ad77-647215783c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
