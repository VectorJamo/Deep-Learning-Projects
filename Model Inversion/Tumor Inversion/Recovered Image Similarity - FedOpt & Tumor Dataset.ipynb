{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646b38c2-386a-4f83-a983-ea764ad8267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to recover training data using model's parameters and a DCGAN that was trained on similar data\n",
    "# Author: Suraj Neupane\n",
    "# Written from scratch as a part of a Research Project 2025, Concordia University of Edmonton.\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b80ddb2-ed75-4870-85b4-9300fce0df7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6950585-eb31-4f61-bd60-5bf8716fcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c47e07-6b7d-4123-a59d-5d465f9ef280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b852ca55-f33e-469f-b7b9-e00d21124927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Implementation Class\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input shape: img_channels x 64 x 64\n",
    "            nn.Conv2d(\n",
    "              in_channels=img_channels, out_channels=features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # Output shape: features_d x 32 x 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1), # Output shape: features_d*2 x 16 x 16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), # Output shape: features_d*4 x 8 x 8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), # Output shape: features_d*8 x 4 x 4\n",
    "           \n",
    "            nn.Conv2d(in_channels=features_d*8, out_channels=1, kernel_size=4, stride=2, padding=0), # Output shape: 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.disc(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afe02e5-61d3-4f25-8933-8484e519cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels, features_g):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0), # z_dim: (batch_size, 100, 1, 1) -> (batch_size, 1024, 4, 4)\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), # z_dim: (batch_size, 1024, 4, 4) -> (batch_size, 512, 8, 8)\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), # z_dim: (batch_size, 512, 8, 8) -> (batch_size, 256, 16, 16)\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), # z_dim: (batch_size, 128, 16, 16) -> (batch_size, 64, 32, 32)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2, out_channels=img_channels, kernel_size=4, stride=2, padding=1 # z_dim: (batch_size, 64, 32, 32) -> (batch_size, 1, 64, 64)\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.gen(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c1be35-d58a-4d0c-990d-9250fbdbf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "z_dim = 100\n",
    "img_channels = 1\n",
    "features_disc = 64\n",
    "features_gen = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dc1f40-5c2e-4903-b2e7-7abe5cae9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instances\n",
    "gen = Generator(z_dim, img_channels, features_gen).to(device)\n",
    "disc = Discriminator(img_channels, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7a5deb-f8d8-41d4-aed3-d012d39a84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained GAN\n",
    "gen.load_state_dict(torch.load('saved models/Generator2.pth', weights_only=True))\n",
    "disc.load_state_dict(torch.load('saved models/Discriminator2.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a504e932-e168-4bb8-88cc-eea0810aafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random fake image\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "fake_img = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b5d73f-b130-4bc1-a868-c225bd2abe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d19338c-18e4-4b6e-842f-a225718fa8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x173d01caa30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_img = fake_img.squeeze()\n",
    "print(fake_img.shape)\n",
    "plt.axis(False)\n",
    "plt.title('Fake Generated Image:')\n",
    "plt.imshow(fake_img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf2aefac-a759-4335-a3de-2065d6a6a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Model\n",
    "class TFCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Reshape(),\n",
    "        )\n",
    "\n",
    "        self.h_size = 64 * 4 * 4\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, xs):\n",
    "        code = self.encoder(xs)\n",
    "        logits = self.classifier(code)\n",
    "        return code, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28876e2-91ee-4ebf-bc52-b71dc82edfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return xs.reshape((xs.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd57210c-2c18-4651-8f29-be94a7a7a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "    def __init__(self, net, init_way, n_classes, input_size=None):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.init_way = init_way\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        model = TFCNN(n_classes)\n",
    "\n",
    "        self.h_size = model.h_size\n",
    "\n",
    "        # Convo and pool layers\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(\n",
    "            self.h_size, self.n_classes, bias=False\n",
    "        )\n",
    "\n",
    "        if self.init_way == \"orth\":\n",
    "            ws = get_orth_weights(self.h_size, self.n_classes)\n",
    "            self.classifier.load_state_dict({\"weight\": ws})\n",
    "\n",
    "    def forward(self, xs):\n",
    "        hs = self.encoder(xs)\n",
    "        logits = self.classifier(hs)\n",
    "        return hs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ea3c08-0470-4c9a-ba15-8328a316b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target_model(base_net, n_classes, path):\n",
    "    # Create the base model\n",
    "    model = ClassifyNet(net=base_net, init_way='none', n_classes=n_classes)\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0880f1aa-9d08-445a-85ac-f1ff3cebde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Number of parameters in the loaded model: 60416\n"
     ]
    }
   ],
   "source": [
    "# Target Model Loading\n",
    "BASE_NET = 'TFCNN'\n",
    "DATASET = 'tumor4'\n",
    "N_CLASSES = 4\n",
    "\n",
    "target_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedopt_global_model.path').to(device)\n",
    "print('Model loaded successfully!')\n",
    "target_model.eval()\n",
    "\n",
    "num_params = sum(p.numel() for p in target_model.parameters())\n",
    "print('Number of parameters in the loaded model:', num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f843f19e-b2a5-4ffe-b22b-de4ff93c5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN BATCH MODEL INVERSION\n",
    "img_count = 20\n",
    "latent_vectors = torch.randn(img_count, z_dim, 1, 1, requires_grad=True) # Start with 20 random noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c21320ed-d5ac-4919-aaad-8cfa093998a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_images(count, target_image_class, latent_vectors):\n",
    "    optimized_latent_vectors = []\n",
    "    \n",
    "    for x in range(0, count):\n",
    "        z = latent_vectors[x].unsqueeze(dim=0).clone().detach().requires_grad_(True)\n",
    "        \n",
    "        learning_rate = 0.01\n",
    "        target_class = 1 # The second class\n",
    "        \n",
    "        # Create the optimizer\n",
    "        optimizer = torch.optim.Adam([z], lr=learning_rate)\n",
    "        \n",
    "        # Loss function (e.g., Cross-Entropy with classifier outputs)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        target_class = torch.tensor([target_image_class])  # Set the target class\n",
    "    \n",
    "        # Optimization loop to find the best z\n",
    "        rounds = 500\n",
    "        for i in range(rounds):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            generated_image = gen(z.to(device))  # Generate an image from the latent vector\n",
    "            generated_image = generated_image.repeat(1, 3, 1, 1)\n",
    "            generated_image = functional.interpolate(generated_image, size=(32, 32), mode='nearest')\n",
    "            #print(generated_image.shape)\n",
    "            hs, prediction_logits = target_model(generated_image)  # Classifier output\n",
    "            #print(prediction_logits)\n",
    "            #print(type(prediction_logits))\n",
    "            loss = loss_fn(prediction_logits, target_class.to(device))  # Minimize difference with true class\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Generating Image Number: {x+1} for target class {target_class}. Step {i}, Loss: {loss.item()}\")\n",
    "    \n",
    "        optimized_latent_vectors.append(z)\n",
    "        \n",
    "    return optimized_latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ebc9533-23ca-4d44-8aea-9d134281b3a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Image Number: 1 for target class tensor([0]). Step 0, Loss: 3.5601742267608643\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 50, Loss: 0.15747381746768951\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 100, Loss: 0.12076052278280258\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 150, Loss: 0.09245713800191879\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 200, Loss: 0.07745920866727829\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 250, Loss: 0.06740788370370865\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 300, Loss: 0.05940081551671028\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 350, Loss: 0.05279591679573059\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 400, Loss: 0.04689368978142738\n",
      "Generating Image Number: 1 for target class tensor([0]). Step 450, Loss: 0.041974399238824844\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 0, Loss: 5.09871244430542\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 50, Loss: 1.3340120315551758\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 100, Loss: 0.63624507188797\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 150, Loss: 0.38346895575523376\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 200, Loss: 0.2215595841407776\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 250, Loss: 0.14657917618751526\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 300, Loss: 0.09919407218694687\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 350, Loss: 0.07988196611404419\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 400, Loss: 0.06865231692790985\n",
      "Generating Image Number: 2 for target class tensor([0]). Step 450, Loss: 0.0619402751326561\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 0, Loss: 4.00660514831543\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 50, Loss: 0.6021897792816162\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 100, Loss: 0.23293904960155487\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 150, Loss: 0.123307004570961\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 200, Loss: 0.08404477685689926\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 250, Loss: 0.06841065734624863\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 300, Loss: 0.05933790281414986\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 350, Loss: 0.05244395509362221\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 400, Loss: 0.0457044281065464\n",
      "Generating Image Number: 3 for target class tensor([0]). Step 450, Loss: 0.0404268279671669\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 0, Loss: 13.867389678955078\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 50, Loss: 2.4797394275665283\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 100, Loss: 1.8069908618927002\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 150, Loss: 1.3720513582229614\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 200, Loss: 1.0542829036712646\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 250, Loss: 0.8609756827354431\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 300, Loss: 0.7330613136291504\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 350, Loss: 0.6358562707901001\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 400, Loss: 0.5475353002548218\n",
      "Generating Image Number: 4 for target class tensor([0]). Step 450, Loss: 0.4882853031158447\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 0, Loss: 14.283016204833984\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 50, Loss: 5.01863956451416\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 100, Loss: 3.9300248622894287\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 150, Loss: 3.1371750831604004\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 200, Loss: 2.5718820095062256\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 250, Loss: 2.1528780460357666\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 300, Loss: 1.8253827095031738\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 350, Loss: 1.5174474716186523\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 400, Loss: 1.2198493480682373\n",
      "Generating Image Number: 5 for target class tensor([0]). Step 450, Loss: 1.0304914712905884\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 0, Loss: 4.031933307647705\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 50, Loss: 1.0819571018218994\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 100, Loss: 0.4069543480873108\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 150, Loss: 0.2635229229927063\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 200, Loss: 0.20879220962524414\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 250, Loss: 0.17301426827907562\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 300, Loss: 0.14475642144680023\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 350, Loss: 0.11962182819843292\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 400, Loss: 0.10013098269701004\n",
      "Generating Image Number: 6 for target class tensor([0]). Step 450, Loss: 0.08589234948158264\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 0, Loss: 3.8112194538116455\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 50, Loss: 0.11174099147319794\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 100, Loss: 0.09040235728025436\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 150, Loss: 0.07726391404867172\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 200, Loss: 0.06641089916229248\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 250, Loss: 0.05665361508727074\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 300, Loss: 0.04861878231167793\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 350, Loss: 0.04144066199660301\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 400, Loss: 0.036650363355875015\n",
      "Generating Image Number: 7 for target class tensor([0]). Step 450, Loss: 0.03284700959920883\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 0, Loss: 11.820684432983398\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 50, Loss: 3.186697244644165\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 100, Loss: 1.575027585029602\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 150, Loss: 0.8881670236587524\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 200, Loss: 0.4414087235927582\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 250, Loss: 0.26798316836357117\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 300, Loss: 0.19224071502685547\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 350, Loss: 0.1445939689874649\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 400, Loss: 0.11581780761480331\n",
      "Generating Image Number: 8 for target class tensor([0]). Step 450, Loss: 0.09697850048542023\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 0, Loss: 8.47996711730957\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 50, Loss: 2.216524600982666\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 100, Loss: 1.7052769660949707\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 150, Loss: 1.2411664724349976\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 200, Loss: 0.9332567453384399\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 250, Loss: 0.7382001876831055\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 300, Loss: 0.5888978242874146\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 350, Loss: 0.5029682517051697\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 400, Loss: 0.4392465353012085\n",
      "Generating Image Number: 9 for target class tensor([0]). Step 450, Loss: 0.39091727137565613\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 0, Loss: 6.367949485778809\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 50, Loss: 2.0097477436065674\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 100, Loss: 1.847999095916748\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 150, Loss: 1.6811730861663818\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 200, Loss: 1.5370299816131592\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 250, Loss: 1.422224760055542\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 300, Loss: 1.3410491943359375\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 350, Loss: 1.2437127828598022\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 400, Loss: 1.1522477865219116\n",
      "Generating Image Number: 10 for target class tensor([0]). Step 450, Loss: 1.0905869007110596\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 0, Loss: 1.702599287033081\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 50, Loss: 0.24159036576747894\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 100, Loss: 0.11239384114742279\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 150, Loss: 0.07781924307346344\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 200, Loss: 0.06256844848394394\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 250, Loss: 0.05282633751630783\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 300, Loss: 0.04570522531867027\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 350, Loss: 0.040744248777627945\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 400, Loss: 0.03654095530509949\n",
      "Generating Image Number: 11 for target class tensor([0]). Step 450, Loss: 0.032835014164447784\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 0, Loss: 4.595584869384766\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 50, Loss: 2.2442586421966553\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 100, Loss: 1.4035950899124146\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 150, Loss: 1.008059024810791\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 200, Loss: 0.7776547074317932\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 250, Loss: 0.6484391093254089\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 300, Loss: 0.5506165027618408\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 350, Loss: 0.4755074977874756\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 400, Loss: 0.41755080223083496\n",
      "Generating Image Number: 12 for target class tensor([0]). Step 450, Loss: 0.3822464942932129\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 0, Loss: 3.6081645488739014\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 50, Loss: 1.8318617343902588\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 100, Loss: 1.7464332580566406\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 150, Loss: 1.667754054069519\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 200, Loss: 1.5917413234710693\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 250, Loss: 1.534742832183838\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 300, Loss: 1.4812109470367432\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 350, Loss: 1.4365811347961426\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 400, Loss: 1.3790183067321777\n",
      "Generating Image Number: 13 for target class tensor([0]). Step 450, Loss: 1.3390552997589111\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 0, Loss: 5.238861083984375\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 50, Loss: 3.0937693119049072\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 100, Loss: 2.462357521057129\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 150, Loss: 2.1191508769989014\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 200, Loss: 1.8379182815551758\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 250, Loss: 1.651618242263794\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 300, Loss: 1.4778848886489868\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 350, Loss: 1.2880126237869263\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 400, Loss: 1.185795545578003\n",
      "Generating Image Number: 14 for target class tensor([0]). Step 450, Loss: 1.0308711528778076\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 0, Loss: 4.542636394500732\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 50, Loss: 1.4585161209106445\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 100, Loss: 0.5418555736541748\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 150, Loss: 0.3185223937034607\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 200, Loss: 0.21792499721050262\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 250, Loss: 0.14381448924541473\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 300, Loss: 0.11604764312505722\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 350, Loss: 0.09916039556264877\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 400, Loss: 0.08482296019792557\n",
      "Generating Image Number: 15 for target class tensor([0]). Step 450, Loss: 0.0715738981962204\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 0, Loss: 3.891805410385132\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 50, Loss: 0.7906014919281006\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 100, Loss: 0.407467246055603\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 150, Loss: 0.24077662825584412\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 200, Loss: 0.15426631271839142\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 250, Loss: 0.11322001367807388\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 300, Loss: 0.08992371708154678\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 350, Loss: 0.07522717863321304\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 400, Loss: 0.06091270223259926\n",
      "Generating Image Number: 16 for target class tensor([0]). Step 450, Loss: 0.052732933312654495\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 0, Loss: 13.873634338378906\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 50, Loss: 5.634480953216553\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 100, Loss: 3.9930334091186523\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 150, Loss: 3.0443570613861084\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 200, Loss: 2.573920965194702\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 250, Loss: 2.2449607849121094\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 300, Loss: 1.951673984527588\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 350, Loss: 1.7910385131835938\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 400, Loss: 1.5403987169265747\n",
      "Generating Image Number: 17 for target class tensor([0]). Step 450, Loss: 1.3740640878677368\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 0, Loss: 11.062164306640625\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 50, Loss: 2.228869915008545\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 100, Loss: 1.573331594467163\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 150, Loss: 1.1055997610092163\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 200, Loss: 0.7820631861686707\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 250, Loss: 0.5816081166267395\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 300, Loss: 0.4605729281902313\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 350, Loss: 0.37617799639701843\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 400, Loss: 0.31555286049842834\n",
      "Generating Image Number: 18 for target class tensor([0]). Step 450, Loss: 0.28097498416900635\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 0, Loss: 7.071814060211182\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 50, Loss: 0.49899211525917053\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 100, Loss: 0.3049446940422058\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 150, Loss: 0.22047924995422363\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 200, Loss: 0.16281116008758545\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 250, Loss: 0.1275661289691925\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 300, Loss: 0.10273316502571106\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 350, Loss: 0.08700671046972275\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 400, Loss: 0.07682221382856369\n",
      "Generating Image Number: 19 for target class tensor([0]). Step 450, Loss: 0.06770771741867065\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 0, Loss: 4.024765491485596\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 50, Loss: 0.2621804475784302\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 100, Loss: 0.16843006014823914\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 150, Loss: 0.12277048081159592\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 200, Loss: 0.09565866738557816\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 250, Loss: 0.07456254214048386\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 300, Loss: 0.06193489581346512\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 350, Loss: 0.0526561439037323\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 400, Loss: 0.04696136713027954\n",
      "Generating Image Number: 20 for target class tensor([0]). Step 450, Loss: 0.04226687178015709\n"
     ]
    }
   ],
   "source": [
    "class0_optimized_latent_vectors = recover_images(img_count, 0, latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6718334c-2bb1-42a3-bd8c-6aea38e6b59c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Image Number: 1 for target class tensor([1]). Step 0, Loss: 11.434466361999512\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 50, Loss: 6.4480485916137695\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 100, Loss: 5.3284196853637695\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 150, Loss: 4.765542507171631\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 200, Loss: 4.503907203674316\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 250, Loss: 4.312953472137451\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 300, Loss: 4.118089199066162\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 350, Loss: 3.879011869430542\n",
      "Generating Image Number: 1 for target class tensor([1]). Step 400, Loss: 3.8106889724731445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      2\u001b[0m latent_vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(img_count, z_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Start with 20 random noises\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m class1_optimized_latent_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mrecover_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m, in \u001b[0;36mrecover_images\u001b[1;34m(count, target_image_class, latent_vectors)\u001b[0m\n\u001b[0;32m     22\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m gen(z\u001b[38;5;241m.\u001b[39mto(device))  \u001b[38;5;66;03m# Generate an image from the latent vector\u001b[39;00m\n\u001b[0;32m     23\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m generated_image\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#print(generated_image.shape)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m hs, prediction_logits \u001b[38;5;241m=\u001b[39m target_model(generated_image)  \u001b[38;5;66;03m# Classifier output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:4536\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[0;32m   4535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "latent_vectors = torch.randn(img_count, z_dim, 1, 1, requires_grad=True) # Start with 20 random noises\n",
    "class1_optimized_latent_vectors = recover_images(img_count, 1, latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8af686-b1ff-419d-946d-2447773e47c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(200)\n",
    "latent_vectors = torch.randn(img_count, z_dim, 1, 1, requires_grad=True) # Start with 20 random noises\n",
    "class2_optimized_latent_vectors = recover_images(img_count, 2, latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381b36d-6c92-43bc-b776-1eefd3eab451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(300)\n",
    "latent_vectors = torch.randn(img_count, z_dim, 1, 1, requires_grad=True) # Start with 20 random noises\n",
    "class3_optimized_latent_vectors = recover_images(img_count, 3, latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6425a5-dd86-4e87-8112-883336345ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0_1 = class0_optimized_latent_vectors[0]\n",
    "z0_2 = class0_optimized_latent_vectors[1]\n",
    "z0_3 = class0_optimized_latent_vectors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ed4fb-b693-4963-afcf-17a512e50b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated image\n",
    "img = gen(z0_1.to(device)).squeeze(dim=0).squeeze(dim=0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc9416-c6ca-486b-bd72-63885de78d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis(False)\n",
    "plt.title('Recovered Image:')\n",
    "plt.imshow(img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fc370-78c5-495f-b1e7-1239873b4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = class1_optimized_latent_vectors[0]\n",
    "z2 = class1_optimized_latent_vectors[1]\n",
    "z3 = class1_optimized_latent_vectors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f38b7d-14f6-4d63-9efa-cb96ec2e1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated image\n",
    "img = gen(z1.to(device)).squeeze(dim=0).squeeze(dim=0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad57ff-7961-4078-9dd4-1e0b4879ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis(False)\n",
    "plt.title('Recovered Image:')\n",
    "plt.imshow(img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecd67d-cb2d-44a3-b375-759d63db75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            # Dataset Loading\n",
    "# Load the .pkl files in as numpy arrays of pixels\n",
    "def load_tumor_data(file_path):\n",
    "    train_xs = []\n",
    "    train_ys = []\n",
    "    \"\"\" Load Digits Data from pickle data\n",
    "    return:\n",
    "    @xs: numpy.array, (n, c, w, h) \n",
    "    @ys: numpy.array, (n, ), 0-9\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        train_xs.append(data[\"data\"])\n",
    "        train_ys.append(data[\"labels\"])\n",
    "    train_xs = np.concatenate(train_xs, axis=0)\n",
    "    train_ys = np.concatenate(train_ys, axis=0)\n",
    "    \n",
    "    return train_xs, train_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72f62f-1eec-4b17-a8ce-fd66781f64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tumor Dataset\n",
    "class TumorDataset(data.Dataset):\n",
    "    def __init__(self, xs, ys, is_train=True):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "        if is_train is True:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((64, 64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5],\n",
    "                    [0.5]\n",
    "                )\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((64, 64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465),\n",
    "                    (0.2023, 0.1994, 0.2010)\n",
    "                )\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.xs[index]\n",
    "        #print(img.shape)\n",
    "        label = self.ys[index]\n",
    "\n",
    "        img = img.transpose((1, 2, 0)).astype(np.uint8)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        img = torch.FloatTensor(img)\n",
    "        label = torch.LongTensor([label])[0]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb91588-ef69-4373-b33f-74aed6c246bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_xs, train_ys):\n",
    "    tumor_dataset = TumorDataset(train_xs, train_ys, is_train=True)\n",
    "\n",
    "    return tumor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415fba0-830c-42c6-bbde-c6958e779fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d4995-5133-4037-ab0e-f2705470cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the pickle dataset file\n",
    "train_xs, train_ys = load_tumor_data('datasets/Tumor/tumor4train.pkl')\n",
    "#train_xs = train_xs[:, 1, :, :]\n",
    "#train_xs = np.expand_dims(train_xs, axis=1)\n",
    "train_xs = train_xs.mean(axis=1, keepdims=True)\n",
    "print(train_xs.shape)\n",
    "print(type(train_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684b01a-eb4a-4210-9f59-d613085a404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "tumor_dataset = create_dataset(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "193f07cc-d34f-425e-aa08-62b6d6adbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(image_x):\n",
    "    # We have to visualize by reverting the normalization (just for visualization).\n",
    "    mean = torch.tensor([0.5])\n",
    "    std = torch.tensor([0.5])\n",
    "\n",
    "    image = image_x * std + mean  # Denormalize the image\n",
    "\n",
    "    image = torch.clamp(image, 0, 1) # Clip values to [0, 1] to ensure valid range for display\n",
    "\n",
    "    # Permute the image to (H, W, C) for matplotlib\n",
    "    \n",
    "    image = image.permute(1, 2, 0)\n",
    "\n",
    "    plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e93c17-a89b-46a2-8198-71e66e370f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first tumor sample. \n",
    "image_x, image_y = tumor_dataset[7000]\n",
    "print(image_x.shape)\n",
    "visualize_image(image_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2da47-7cb6-493e-aeda-aa80d7965de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7937e5-b808-4b3e-8288-75dcd9862340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature embeddings\n",
    "tfcnn_model = target_model = load_target_model(base_net='TFCNN', n_classes=4, path='saved models/fedavg_global_model.pth').to(device)\n",
    "model_embeddings = tfcnn_model.encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e3d92-45b6-45f6-9bf2-d51f379d6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb1015-bdfb-439b-bea1-81f9f50dd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(img):\n",
    "    with torch.no_grad():\n",
    "        features = model_embeddings(img).squeeze()\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f38654-6670-440d-a322-b2cae18a71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an original image through the model\n",
    "image_x, image_y = tumor_dataset[10]\n",
    "image_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2659e-dcaa-4d4a-b5a5-06b363ce6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x = image_x.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14348bfc-5988-48f0-88cf-4761d7a99d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66160253-9543-4aba-bb81-031d51ef9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x = image_x.repeat(1, 3, 1, 1)\n",
    "image_x = image_x.to(device)\n",
    "image_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615c7d3-2dab-425d-81e1-2595ec59e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_feature(image_x)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9ddc9-3fc8-4a47-bfc7-74a6df27d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_original_images = []\n",
    "class1_original_images = []\n",
    "class2_original_images = []\n",
    "class3_original_images = []\n",
    "target_num = 20 # 20 images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804c948-2dc8-41df-a633-113300baf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tumor_dataset)):\n",
    "    img_x, img_y = tumor_dataset[i]\n",
    "\n",
    "    if img_y == 0:\n",
    "        if len(class0_original_images) < target_num:\n",
    "            class0_original_images.append(img_x)\n",
    "    elif img_y == 1:\n",
    "        if len(class1_original_images) < target_num:\n",
    "            class1_original_images.append(img_x)\n",
    "    elif img_y == 2:\n",
    "        if len(class2_original_images) < target_num:\n",
    "            class2_original_images.append(img_x)\n",
    "    elif img_y == 3:\n",
    "        if len(class3_original_images) < target_num:\n",
    "            class3_original_images.append(img_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07c896-c4cd-48da-9e59-c9d7bd90a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class3_original_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102404a-68a3-40c5-99a7-45813eb87f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the cosine similarity\n",
    "class1_original_image = class1_original_images[8]\n",
    "class1_original_image = class1_original_image.unsqueeze(dim=0).to(device)\n",
    "class1_original_image = class1_original_image.repeat(1, 3, 1, 1)\n",
    "class1_original_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7fe9c85-2c83-4580-be7d-32833b706948",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake1 = class0_optimized_latent_vectors[8]\n",
    "fake1_image = gen(fake1.to(device))\n",
    "fake1_image = fake1_image.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74647152-cdcb-4593-a88f-d3ac43fd97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake1_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ff405e5-5e35-4c3f-b34e-1bca88641726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake = class0_optimized_latent_vectors[13]\n",
    "fake_img = gen(fake.to(device)).squeeze(dim=0)\n",
    "print(fake_img.shape)\n",
    "visualize_image(fake_img.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f6396-0ef2-45f5-96e0-6048be01af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features of the images\n",
    "features_original = get_feature(class1_original_image)\n",
    "features_fake = get_feature(fake1_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e2bca-5f2b-41ea-b810-15d1f9d30f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "similarity = 1 - cosine(features_original, features_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c7e35-27ee-453c-9356-cb9a84fdd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e04e8-52b7-4cf2-876c-d9b07a435ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class0_sim():\n",
    "    similarities = []\n",
    "    for i in range(0, 20):\n",
    "        class0_original = class0_original_images[i].unsqueeze(dim=0).to(device)\n",
    "        class0_original = class0_original.repeat(1, 3, 1, 1)\n",
    "\n",
    "        class0_fake = class0_optimized_latent_vectors[i]\n",
    "        class0_fake_img = gen(class0_fake.to(device))\n",
    "        class0_fake_img = class0_fake_img.repeat(1, 3, 1, 1)\n",
    "\n",
    "        features_original = get_feature(class0_original)\n",
    "        features_fake = get_feature(class0_fake_img)\n",
    "\n",
    "        cosine_similarity = 1 - cosine(features_original, features_fake)\n",
    "\n",
    "        similarities.append(cosine_similarity)\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8f579-0b80-4157-b59b-7b92be9e52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class1_sim():\n",
    "    similarities = []\n",
    "    for i in range(0, 20):\n",
    "        class1_original = class1_original_images[i].unsqueeze(dim=0).to(device)\n",
    "        class1_original = class1_original.repeat(1, 3, 1, 1)\n",
    "\n",
    "        class1_fake = class1_optimized_latent_vectors[i]\n",
    "        class1_fake_img = gen(class1_fake.to(device))\n",
    "        class1_fake_img = class1_fake_img.repeat(1, 3, 1, 1)\n",
    "\n",
    "        features_original = get_feature(class1_original)\n",
    "        features_fake = get_feature(class1_fake_img)\n",
    "\n",
    "        cosine_similarity = 1 - cosine(features_original, features_fake)\n",
    "        similarities.append(cosine_similarity)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdc999-fa6d-4005-b785-3c54a7bd89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class2_sim():\n",
    "    similarities = []\n",
    "    for i in range(0, 20):\n",
    "        class2_original = class2_original_images[i].unsqueeze(dim=0).to(device)\n",
    "        class2_original = class2_original.repeat(1, 3, 1, 1)\n",
    "\n",
    "        class2_fake = class2_optimized_latent_vectors[i]\n",
    "        class2_fake_img = gen(class2_fake.to(device))\n",
    "        class2_fake_img = class2_fake_img.repeat(1, 3, 1, 1)\n",
    "\n",
    "        features_original = get_feature(class2_original)\n",
    "        features_fake = get_feature(class2_fake_img)\n",
    "\n",
    "        cosine_similarity = 1 - cosine(features_original, features_fake)\n",
    "        similarities.append(cosine_similarity)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb8de1-5a87-4dd4-8fc2-d3f27d6ddc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class3_sim():\n",
    "    similarities = []\n",
    "    for i in range(0, 20):\n",
    "        class3_original = class3_original_images[i].unsqueeze(dim=0).to(device)\n",
    "        class3_original = class3_original.repeat(1, 3, 1, 1)\n",
    "\n",
    "        class3_fake = class3_optimized_latent_vectors[i]\n",
    "        class3_fake_img = gen(class3_fake.to(device))\n",
    "        class3_fake_img = class3_fake_img.repeat(1, 3, 1, 1)\n",
    "\n",
    "        features_original = get_feature(class3_original)\n",
    "        features_fake = get_feature(class3_fake_img)\n",
    "\n",
    "        cosine_similarity = 1 - cosine(features_original, features_fake)\n",
    "        similarities.append(cosine_similarity)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868f175-5845-4ee4-b4cf-1c89513375ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_sims = calculate_class0_sim()\n",
    "class1_sims = calculate_class0_sim()\n",
    "class2_sims = calculate_class0_sim()\n",
    "class3_sims = calculate_class0_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4597d-a583-4204-a3ce-9085936e0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9d480-a58f-4ce6-8973-eb9c8d6e7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class3_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc103d-1c30-473a-adc3-fcd5ca5100a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        # VISUALIZE\n",
    "# NOTE: Class indexes now start from 1 so, previous class 0 = now class 1 and so on\n",
    "# FIG 1: Raw similarity plot\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate sample similarity data for four classes (each with 20 values in the range 0 to 1)\n",
    "class1_sim = class0_sims\n",
    "class2_sim = class1_sims\n",
    "class3_sim = class2_sims\n",
    "class4_sim = class3_sims\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(6.5, 4.2))  # Set the size of the figure (width, height)\n",
    "\n",
    "# Sample DataFrame with four columns\n",
    "data=pd.DataFrame({'Class 1':class1_sim,'Class 2':class2_sim,'Class 3':class3_sim,'Class 4':class4_sim})\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the probability distributions\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and line styles for each column\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "linestyles = ['-.', '--', '-', '-']\n",
    "\n",
    "# Plot the probability distributions with unique colors and line styles\n",
    "for column, color, linestyle in zip(df.columns, colors, linestyles):\n",
    "    sns.kdeplot(df[column], label=column, color=color, linestyle=linestyle)\n",
    "\n",
    "#plt.title('Probability Distributions of Four Columns')\n",
    "plt.xlabel('Similarity',fontsize=12)\n",
    "plt.ylabel('Density',fontsize=12)\n",
    "plt.yticks(fontsize=12)  # Set y-tick labels font size\n",
    "plt.legend(loc='upper right',fontsize=12)  # Increase legend font size\n",
    "\n",
    "# Optimize layout\n",
    "plt.tight_layout()\n",
    "# Save the plot to a file\n",
    "plt.savefig('output/fedvg.png', dpi=300, bbox_inches='tight')  # Save as a PNG file with high resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71af78e-28e9-4564-9609-3039455bb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 2: DIFFERENCES IN COSINE SIMILARITY\n",
    "\n",
    "# For each class, we have 20 similarity values — one per image — showing how similar fake and real versions are.These values are very close across classes\n",
    "# So, to highlight subtle differences, compute and plot the difference between each pair of classes, value by value.\n",
    "# This gives you 20 values showing how much the similarity differs for each image between class 0 and class 1.\n",
    "# It's a magnified view of the gap between classes.\n",
    "# If the differences are tiny (e.g., 0.003), the plot helps make that visible and comparable.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Differences between class distributions (pairwise)\n",
    "diff_12 = np.array(class2_sim) - np.array(class1_sim)\n",
    "diff_13 = np.array(class3_sim) - np.array(class1_sim)\n",
    "diff_14 = np.array(class4_sim) - np.array(class1_sim)\n",
    "diff_23 = np.array(class3_sim) - np.array(class2_sim)\n",
    "diff_24 = np.array(class4_sim) - np.array(class2_sim)\n",
    "diff_34 = np.array(class4_sim) - np.array(class3_sim)\n",
    "\n",
    "# Create a DataFrame\n",
    "diff_df = pd.DataFrame({\n",
    "    'Class 2 - Class 1': diff_12,\n",
    "    'Class 3 - Class 1': diff_13,\n",
    "    'Class 4 - Class 1': diff_14,\n",
    "    'Class 3 - Class 2': diff_23,\n",
    "    'Class 4 - Class 2': diff_24,\n",
    "    'Class 4 - Class 3': diff_34,\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "\n",
    "colors = sns.color_palette(\"tab10\", n_colors=6)\n",
    "\n",
    "for column, color in zip(diff_df.columns, colors):\n",
    "    sns.kdeplot(diff_df[column], label=column, color=color, linestyle='-')\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=1)  # reference line at 0\n",
    "plt.xlabel('Difference in Similarity', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.title(\"Pairwise Differences in Cosine Similarity\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bf182-7927-4ce2-bf5a-97299d426bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interclass_similarity(first, second):\n",
    "    similarities = []\n",
    "\n",
    "    first_images = []\n",
    "    second_images = []\n",
    "    if first == 'class1':\n",
    "        first_images = class0_original_images.copy()\n",
    "    elif first == 'class2':\n",
    "        first_images = class1_original_images.copy()\n",
    "    elif first == 'class3':\n",
    "        first_images = class2_original_images.copy()\n",
    "    elif first == 'class4':\n",
    "        first_images = class3_original_images.copy()\n",
    "\n",
    "    if second == 'class1':\n",
    "        second_images = class0_original_images.copy()\n",
    "    elif second == 'class2':\n",
    "        second_images = class1_original_images.copy()\n",
    "    elif second == 'class3':\n",
    "        second_images = class2_original_images.copy()\n",
    "    elif second == 'class4':\n",
    "        second_images = class3_original_images.copy()\n",
    "    \n",
    "    for i in range(0, 20):\n",
    "        first_image = first_images[i].unsqueeze(dim=0).to(device)\n",
    "        first_image = first_image.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        second_image = second_images[i].unsqueeze(dim=0).to(device)\n",
    "        second_image = second_image.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        features_first = get_feature(first_image)\n",
    "        features_second = get_feature(second_image)\n",
    "\n",
    "        cosine_similarity = 1 - cosine(features_first, features_second)\n",
    "        similarities.append(cosine_similarity)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954dfe3-cb08-4551-a5f9-d2082ac3d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class1class2 = calculate_interclass_similarity('class1', 'class2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ab50d-9d9d-443b-bea5-2d6e302e2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class1class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29441f40-36a7-4854-98be-5d128bae49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                # FIG: 3 -> INTERCLASS SIMILARITIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(6.5, 4.2))  # Set the size of the figure (width, height)\n",
    "\n",
    "# Generate sample similarity data for four classes (each with 20 values in the range 0 to 1)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "class1class2 = calculate_interclass_similarity('class1', 'class2')\n",
    "class1class3 = calculate_interclass_similarity('class1', 'class3')\n",
    "class1class4 = calculate_interclass_similarity('class1', 'class4')\n",
    "class2class3 = calculate_interclass_similarity('class2', 'class3')\n",
    "class2class4 = calculate_interclass_similarity('class2', 'class4')\n",
    "class3class4 = calculate_interclass_similarity('class3', 'class4')\n",
    "\n",
    "# Sample DataFrame with four columns\n",
    "data=pd.DataFrame({'Class1 vs Class2':class1class2,'Class1 vs Class3':class1class3,'Class1 vs Class4':class1class4,'Class2 vs Class3':class2class3,'Class2 vs Class4':class2class4,'Class3 vs Class4':class3class4})\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the probability distributions\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors and line styles for each column\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "linestyles = ['-.', '--', '-', '-', '-.', '--']\n",
    "\n",
    "# Plot the probability distributions with unique colors and line styles\n",
    "for column, color, linestyle in zip(df.columns, colors, linestyles):\n",
    "    sns.kdeplot(df[column], label=column, color=color, linestyle=linestyle)\n",
    "\n",
    "#plt.title('Probability Distributions of Four Columns')\n",
    "plt.xlabel('Similarity',fontsize=12)\n",
    "plt.ylabel('Density',fontsize=12)\n",
    "plt.yticks(fontsize=12)  # Set y-tick labels font size\n",
    "plt.legend(loc='upper right',fontsize=12)  # Increase legend font size\n",
    "\n",
    "# Optimize layout\n",
    "plt.tight_layout()\n",
    "# Save the plot to a file\n",
    "plt.savefig('output/VoiceDivergenceD3.png', dpi=300, bbox_inches='tight')  # Save as a PNG file with high resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19754eee-5c1e-47e6-bbaa-b2cef05a1641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
