{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646b38c2-386a-4f83-a983-ea764ad8267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to recover training data using model's parameters and a DCGAN that was trained on similar data\n",
    "# Author: Suraj Neupane\n",
    "# Written from scratch as a part of a Research Project 2025, Concordia University of Edmonton.\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b80ddb2-ed75-4870-85b4-9300fce0df7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6950585-eb31-4f61-bd60-5bf8716fcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c47e07-6b7d-4123-a59d-5d465f9ef280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b852ca55-f33e-469f-b7b9-e00d21124927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Implementation Class\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input shape: img_channels x 64 x 64\n",
    "            nn.Conv2d(\n",
    "              in_channels=img_channels, out_channels=features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # Output shape: features_d x 32 x 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1), # Output shape: features_d*2 x 16 x 16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), # Output shape: features_d*4 x 8 x 8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), # Output shape: features_d*8 x 4 x 4\n",
    "           \n",
    "            nn.Conv2d(in_channels=features_d*8, out_channels=1, kernel_size=4, stride=2, padding=0), # Output shape: 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.disc(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afe02e5-61d3-4f25-8933-8484e519cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels, features_g):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0), # z_dim: (batch_size, 100, 1, 1) -> (batch_size, 1024, 4, 4)\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), # z_dim: (batch_size, 1024, 4, 4) -> (batch_size, 512, 8, 8)\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), # z_dim: (batch_size, 512, 8, 8) -> (batch_size, 256, 16, 16)\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), # z_dim: (batch_size, 128, 16, 16) -> (batch_size, 64, 32, 32)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2, out_channels=img_channels, kernel_size=4, stride=2, padding=1 # z_dim: (batch_size, 64, 32, 32) -> (batch_size, 1, 64, 64)\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.gen(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c1be35-d58a-4d0c-990d-9250fbdbf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "z_dim = 100\n",
    "img_channels = 1\n",
    "features_disc = 64\n",
    "features_gen = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dc1f40-5c2e-4903-b2e7-7abe5cae9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instances\n",
    "gen = Generator(z_dim, img_channels, features_gen).to(device)\n",
    "disc = Discriminator(img_channels, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7a5deb-f8d8-41d4-aed3-d012d39a84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained GAN\n",
    "gen.load_state_dict(torch.load('saved models/Generator2.pth', weights_only=True))\n",
    "disc.load_state_dict(torch.load('saved models/Discriminator2.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a504e932-e168-4bb8-88cc-eea0810aafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random fake image\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "fake_img = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b5d73f-b130-4bc1-a868-c225bd2abe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d19338c-18e4-4b6e-842f-a225718fa8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "fake_img = fake_img.squeeze()\n",
    "print(fake_img.shape)\n",
    "#plt.axis(False)\n",
    "#plt.title('Fake Generated Image:')\n",
    "#plt.imshow(fake_img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf2aefac-a759-4335-a3de-2065d6a6a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Model\n",
    "class TFCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Reshape(),\n",
    "        )\n",
    "\n",
    "        self.h_size = 64 * 4 * 4\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, xs):\n",
    "        code = self.encoder(xs)\n",
    "        logits = self.classifier(code)\n",
    "        return code, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28876e2-91ee-4ebf-bc52-b71dc82edfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return xs.reshape((xs.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd57210c-2c18-4651-8f29-be94a7a7a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "    def __init__(self, net, init_way, n_classes, input_size=None):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.init_way = init_way\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        model = TFCNN(n_classes)\n",
    "\n",
    "        self.h_size = model.h_size\n",
    "\n",
    "        # Convo and pool layers\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(\n",
    "            self.h_size, self.n_classes, bias=False\n",
    "        )\n",
    "\n",
    "        if self.init_way == \"orth\":\n",
    "            ws = get_orth_weights(self.h_size, self.n_classes)\n",
    "            self.classifier.load_state_dict({\"weight\": ws})\n",
    "\n",
    "    def forward(self, xs):\n",
    "        hs = self.encoder(xs)\n",
    "        logits = self.classifier(hs)\n",
    "        return hs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ea3c08-0470-4c9a-ba15-8328a316b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target_model(base_net, n_classes, path):\n",
    "    # Create the base model\n",
    "    model = ClassifyNet(net=base_net, init_way='none', n_classes=n_classes)\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0880f1aa-9d08-445a-85ac-f1ff3cebde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifyNet(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Reshape()\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target Models Loading\n",
    "BASE_NET = 'TFCNN'\n",
    "DATASET = 'tumor4'\n",
    "N_CLASSES = 4\n",
    "\n",
    "fedavg_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedavg_global_model.pth').to(device)\n",
    "fedavgDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedavgDP_global_model1.path').to(device)\n",
    "feddyn_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/feddyn_global_model.path').to(device)\n",
    "feddynDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/feddynDP_global_model1.path').to(device)\n",
    "fedopt_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedopt_global_model.path').to(device)\n",
    "fedoptDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedoptDP_global_model1.path').to(device)\n",
    "moon_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/moon_global_model.path').to(device)\n",
    "moonDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/moonDP_global_model1.path').to(device)\n",
    "print('Models loaded successfully!')\n",
    "\n",
    "# Put all the models in evaluation mode\n",
    "fedavg_model.eval()\n",
    "feddyn_model.eval()\n",
    "fedopt_model.eval()\n",
    "moon_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7b801e-3654-4505-90a1-43805913ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Generate 80 fake images from the DCGAN\n",
    "torch.manual_seed(42)\n",
    "image_count = 80\n",
    "latent_vectors = torch.randn(image_count, z_dim, 1, 1).to(device)\n",
    "print(latent_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f38b7d-14f6-4d63-9efa-cb96ec2e1d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Display one of the random image\n",
    "img = gen(latent_vectors[56].to(device)).squeeze(dim=0).squeeze(dim=0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cad57ff-7961-4078-9dd4-1e0b4879ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2090f3bdcd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis(False)\n",
    "plt.title('Recovered Image:')\n",
    "plt.imshow(img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c7b314d-63c0-46e8-86b1-cce3bdb46bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.unsqueeze(dim=0).unsqueeze(dim=0) # Add the two extra dimensions to make it (batch_size, channels, h, w)\n",
    "#print(img.shape)\n",
    "img = img.repeat(1, 3, 1, 1) # Increase the color channels from 1 to 3 by repeating the color channel\n",
    "img = functional.interpolate(img, size=(32, 32), mode='nearest') # Change the image shape from 64x64(DCGAN output) to 32x32(Model input)\n",
    "        \n",
    "hs, logits = fedavg_model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fb89fe-aecc-45a2-9436-47dafdd0e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddae3c86-eaad-486b-b473-baaf69ca9238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Send all the 80 images in batch\n",
    "random_images = gen(latent_vectors.to(device)).squeeze(dim=0).squeeze(dim=0)\n",
    "print(random_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c820f4ce-ce4d-42a7-9fbe-0c0072b001d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "random_images = random_images.repeat(1, 3, 1, 1)\n",
    "print(random_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79adf9a-d2e6-4ec6-b3d3-74abc29d1ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3, 32, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_images_resized = functional.interpolate(random_images, size=(32, 32), mode='nearest')\n",
    "random_images_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad6c3c2c-0f08-4bfe-b7cd-5a64f24573cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_images(model, images, target_class, prob_threshold):\n",
    "    hs, logits = model(images) # Send the images down the model and get the prediction logits\n",
    "    probs = functional.softmax(logits, dim=1) # Convert the logits into probabilities\n",
    "    mask = probs[:, target_class] > prob_threshold\n",
    "    filtered_images = images[mask]\n",
    "\n",
    "    return filtered_images, probs[mask][:, target_class] # Return the image and the corresponding probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b76bbcdf-7285-420a-96b7-d863b74f11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "result, prob = filter_images(fedavg_model, random_images_resized, 0, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "feb7e71b-145f-4427-a1a5-cafcf3d93153",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "666deed3-a111-4a29-b485-158f96010a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedavg\n",
    "class0_filtered_fedavg, fedavg_class0_probs = filter_images(fedavg_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedavg, fedavg_class1_probs = filter_images(fedavg_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedavg, fedavg_class2_probs = filter_images(fedavg_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedavg, fedavg_class3_probs = filter_images(fedavg_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62e3c169-8b7c-485c-bb9c-6381800c9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FedavgDP\n",
    "class0_filtered_fedavgDP, fedavgDP_class0_probs = filter_images(fedavgDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedavgDP, fedavgDP_class1_probs = filter_images(fedavgDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedavgDP, fedavgDP_class2_probs = filter_images(fedavgDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedavgDP, fedavgDP_class3_probs = filter_images(fedavgDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01a2520d-4cbf-4704-9bb0-a063f68f9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feddyn\n",
    "class0_filtered_feddyn, feddyn_class0_probs = filter_images(feddyn_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_feddyn, feddyn_class1_probs = filter_images(feddyn_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_feddyn, feddyn_class2_probs = filter_images(feddyn_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_feddyn, feddyn_class3_probs = filter_images(feddyn_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "582ce4d2-186b-461a-987e-049043c4a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeddynDP\n",
    "class0_filtered_feddynDP, feddynDP_class0_probs = filter_images(feddynDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_feddynDP, feddynDP_class1_probs = filter_images(feddynDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_feddynDP, feddynDP_class2_probs = filter_images(feddynDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_feddynDP, feddynDP_class3_probs = filter_images(feddynDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b53865e2-d63d-4053-a43d-5ac5d9699b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedopt\n",
    "class0_filtered_fedopt, fedopt_class0_probs = filter_images(fedopt_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedopt, fedopt_class1_probs = filter_images(fedopt_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedopt, fedopt_class2_probs = filter_images(fedopt_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedopt, fedopt_class3_probs = filter_images(fedopt_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fd39f69-b58c-409b-9d1b-4c8c0d89b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedopt\n",
    "class0_filtered_fedoptDP, fedoptDP_class0_probs = filter_images(fedoptDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedoptDP, fedoptDP_class1_probs = filter_images(fedoptDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedoptDP, fedoptDP_class2_probs = filter_images(fedoptDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedoptDP, fedoptDP_class3_probs = filter_images(fedoptDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9babd679-a366-44d1-bd4e-34b7f10d56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon\n",
    "class0_filtered_moon, moon_class0_probs = filter_images(moon_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_moon, moon_class1_probs = filter_images(moon_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_moon, moon_class2_probs = filter_images(moon_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_moon, moon_class3_probs = filter_images(moon_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab9f3fbd-4614-4d5d-846e-bd11d9df7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon\n",
    "class0_filtered_moonDP, moonDP_class0_probs = filter_images(moonDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_moonDP, moonDP_class1_probs = filter_images(moonDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_moonDP, moonDP_class2_probs = filter_images(moonDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_moonDP, moonDP_class3_probs = filter_images(moonDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e9a9351-59ad-41aa-9da1-9310318a607c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FedAvg\n",
      "Class: 0\n",
      "tensor([1.0000, 1.0000, 1.0000, 0.8061, 1.0000, 1.0000, 0.8195],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([0.8174, 0.9284, 0.8129], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([0.9537, 0.8672, 0.8436, 0.9891, 0.8435, 0.9580, 0.9902],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the probabilities\n",
    "print('Model: FedAvg')\n",
    "print('Class: 0')\n",
    "print(fedavg_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedavg_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedavg_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedavg_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6d47323-85f6-4dfa-a08e-d2f57edc1dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FedAvgDP\n",
      "Class: 0\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.9921, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the probabilities\n",
    "print('Model: FedAvgDP')\n",
    "print('Class: 0')\n",
    "print(fedavgDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedavgDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedavgDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedavgDP_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "056d883c-7d54-49cd-9eda-2b492fb7db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FedDyn\n",
      "Class: 0\n",
      "tensor([1.0000, 1.0000, 1.0000, 0.8763, 1.0000, 1.0000], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([0.9018, 0.8640, 0.8165, 0.9483, 0.9317, 0.9264, 0.8754, 0.9852, 0.9874,\n",
      "        0.9625, 0.9428, 0.9850, 0.9519, 0.9332, 0.9714, 0.9658, 0.9611, 0.8317,\n",
      "        0.9720, 0.9142, 0.9663, 0.9589], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([0.8492, 0.9699, 0.8140, 0.8003, 0.9171], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Model: FedDyn')\n",
    "print('Class: 0')\n",
    "print(feddyn_class0_probs)\n",
    "print('Class: 1')\n",
    "print(feddyn_class1_probs)\n",
    "print('Class: 2')\n",
    "print(feddyn_class2_probs)\n",
    "print('Class: 3')\n",
    "print(feddyn_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "697c8c41-f1e6-488f-8b81-f6dbfcb3614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FedDynDP\n",
      "Class: 0\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.9921, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Model: FedDynDP')\n",
    "print('Class: 0')\n",
    "print(feddynDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(feddynDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(feddynDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(feddynDP_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f2eff4-8fe2-437a-a987-255d658ce788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FedOpt\n",
      "Class: 0\n",
      "tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([0.9047, 0.9595, 0.9944, 0.8246, 0.9999, 0.9775], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([0.9496, 0.9959, 0.9998, 0.9242, 0.8706, 0.9965, 0.9262, 0.9287, 0.8623,\n",
      "        0.8880, 0.9665, 0.9841, 0.9523, 0.9003, 0.9312, 0.9948, 0.8340, 0.9589,\n",
      "        0.8245, 0.9996, 0.9722, 0.9914, 0.9644, 0.8528, 0.9368, 0.9477, 0.9869,\n",
      "        0.9820, 0.9190, 0.9913, 0.9946, 0.9667, 0.9800, 0.9992, 0.8352, 0.9994,\n",
      "        0.9997, 0.9824, 0.9332, 0.9732, 0.9986, 0.9997, 0.9921, 0.9963, 0.8791,\n",
      "        0.9446, 0.9956, 0.9956, 0.9797, 0.9910, 0.8542, 0.9980],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Model: FedOpt')\n",
    "print('Class: 0')\n",
    "print(fedopt_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedopt_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedopt_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedopt_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4460fb67-e8be-4b34-a25b-5b2d0265c488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FedOptDP\n",
      "Class: 0\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9993, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9974, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([1.0000, 0.9575, 1.0000, 0.9998], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Model: FedOptDP')\n",
    "print('Class: 0')\n",
    "print(fedoptDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedoptDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedoptDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedoptDP_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "523dc2c6-1ee6-4fc6-9e4c-9f8f91bde8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Moon\n",
      "Class: 0\n",
      "tensor([1.0000, 0.8256, 1.0000, 1.0000, 0.9717, 0.9280, 0.9410, 0.8053, 0.9712,\n",
      "        0.9824, 0.8952, 0.8030, 1.0000, 0.9906, 1.0000, 0.8659, 0.8381, 0.8702,\n",
      "        0.9717, 0.8571], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([0.9636, 0.8582, 0.9998, 0.9409, 0.8565, 0.8905, 0.8665, 0.9838, 0.9025,\n",
      "        0.9381, 0.8998], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([0.9512, 0.9323, 0.9984, 0.9821, 0.9845, 0.9999, 0.9971],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Model: Moon')\n",
    "print('Class: 0')\n",
    "print(moon_class0_probs)\n",
    "print('Class: 1')\n",
    "print(moon_class1_probs)\n",
    "print('Class: 2')\n",
    "print(moon_class2_probs)\n",
    "print('Class: 3')\n",
    "print(moon_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b42297d8-c92f-4b0c-97bb-ec2f205908d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MoonDP\n",
      "Class: 0\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 1\n",
      "tensor([], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Class: 2\n",
      "tensor([1.0000, 1.0000, 1.0000, 0.9994, 1.0000, 0.9997, 0.9998, 1.0000, 0.9873,\n",
      "        0.9988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997,\n",
      "        1.0000, 0.9971, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9977, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 0.9971,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 0.9995, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Class: 3\n",
      "tensor([0.9741, 0.8150, 1.0000], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Model: MoonDP')\n",
    "print('Class: 0')\n",
    "print(moonDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(moonDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(moonDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(moonDP_class3_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
