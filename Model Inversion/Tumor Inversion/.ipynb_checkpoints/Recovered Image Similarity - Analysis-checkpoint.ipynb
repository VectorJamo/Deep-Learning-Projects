{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646b38c2-386a-4f83-a983-ea764ad8267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to recover training data using model's parameters and a DCGAN that was trained on similar data\n",
    "# Author: Suraj Neupane\n",
    "# Written from scratch as a part of a Research Project 2025, Concordia University of Edmonton.\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b80ddb2-ed75-4870-85b4-9300fce0df7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6950585-eb31-4f61-bd60-5bf8716fcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c47e07-6b7d-4123-a59d-5d465f9ef280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b852ca55-f33e-469f-b7b9-e00d21124927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Implementation Class\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input shape: img_channels x 64 x 64\n",
    "            nn.Conv2d(\n",
    "              in_channels=img_channels, out_channels=features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # Output shape: features_d x 32 x 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1), # Output shape: features_d*2 x 16 x 16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), # Output shape: features_d*4 x 8 x 8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), # Output shape: features_d*8 x 4 x 4\n",
    "           \n",
    "            nn.Conv2d(in_channels=features_d*8, out_channels=1, kernel_size=4, stride=2, padding=0), # Output shape: 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.disc(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afe02e5-61d3-4f25-8933-8484e519cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels, features_g):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0), # z_dim: (batch_size, 100, 1, 1) -> (batch_size, 1024, 4, 4)\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), # z_dim: (batch_size, 1024, 4, 4) -> (batch_size, 512, 8, 8)\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), # z_dim: (batch_size, 512, 8, 8) -> (batch_size, 256, 16, 16)\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), # z_dim: (batch_size, 128, 16, 16) -> (batch_size, 64, 32, 32)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2, out_channels=img_channels, kernel_size=4, stride=2, padding=1 # z_dim: (batch_size, 64, 32, 32) -> (batch_size, 1, 64, 64)\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.gen(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c1be35-d58a-4d0c-990d-9250fbdbf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "z_dim = 100\n",
    "img_channels = 1\n",
    "features_disc = 64\n",
    "features_gen = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dc1f40-5c2e-4903-b2e7-7abe5cae9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instances\n",
    "gen = Generator(z_dim, img_channels, features_gen).to(device)\n",
    "disc = Discriminator(img_channels, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7a5deb-f8d8-41d4-aed3-d012d39a84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained GAN\n",
    "gen.load_state_dict(torch.load('saved models/Generator2.pth', weights_only=True))\n",
    "disc.load_state_dict(torch.load('saved models/Discriminator2.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a504e932-e168-4bb8-88cc-eea0810aafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random fake image\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "fake_img = gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b5d73f-b130-4bc1-a868-c225bd2abe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d19338c-18e4-4b6e-842f-a225718fa8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "fake_img = fake_img.squeeze()\n",
    "print(fake_img.shape)\n",
    "#plt.axis(False)\n",
    "#plt.title('Fake Generated Image:')\n",
    "#plt.imshow(fake_img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf2aefac-a759-4335-a3de-2065d6a6a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Model\n",
    "class TFCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Reshape(),\n",
    "        )\n",
    "\n",
    "        self.h_size = 64 * 4 * 4\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, xs):\n",
    "        code = self.encoder(xs)\n",
    "        logits = self.classifier(code)\n",
    "        return code, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28876e2-91ee-4ebf-bc52-b71dc82edfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return xs.reshape((xs.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd57210c-2c18-4651-8f29-be94a7a7a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "    def __init__(self, net, init_way, n_classes, input_size=None):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.init_way = init_way\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        model = TFCNN(n_classes)\n",
    "\n",
    "        self.h_size = model.h_size\n",
    "\n",
    "        # Convo and pool layers\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(\n",
    "            self.h_size, self.n_classes, bias=False\n",
    "        )\n",
    "\n",
    "        if self.init_way == \"orth\":\n",
    "            ws = get_orth_weights(self.h_size, self.n_classes)\n",
    "            self.classifier.load_state_dict({\"weight\": ws})\n",
    "\n",
    "    def forward(self, xs):\n",
    "        hs = self.encoder(xs)\n",
    "        logits = self.classifier(hs)\n",
    "        return hs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ea3c08-0470-4c9a-ba15-8328a316b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target_model(base_net, n_classes, path):\n",
    "    # Create the base model\n",
    "    model = ClassifyNet(net=base_net, init_way='none', n_classes=n_classes)\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0880f1aa-9d08-445a-85ac-f1ff3cebde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifyNet(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Reshape()\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target Models Loading\n",
    "BASE_NET = 'TFCNN'\n",
    "DATASET = 'tumor4'\n",
    "N_CLASSES = 4\n",
    "\n",
    "fedavg_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedavg_global_model.pth').to(device)\n",
    "fedavgDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedavgDP_global_model1.path').to(device)\n",
    "feddyn_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/feddyn_global_model.path').to(device)\n",
    "feddynDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/feddynDP_global_model1.path').to(device)\n",
    "fedopt_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedopt_global_model.path').to(device)\n",
    "fedoptDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/fedoptDP_global_model1.path').to(device)\n",
    "moon_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/moon_global_model.path').to(device)\n",
    "moonDP_model = load_target_model(base_net=BASE_NET, n_classes=N_CLASSES, path='saved models/moonDP_global_model1.path').to(device)\n",
    "print('Models loaded successfully!')\n",
    "\n",
    "# Put all the models in evaluation mode\n",
    "fedavg_model.eval()\n",
    "feddyn_model.eval()\n",
    "fedopt_model.eval()\n",
    "moon_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7b801e-3654-4505-90a1-43805913ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Generate 80 fake images from the DCGAN\n",
    "torch.manual_seed(42)\n",
    "image_count = 80\n",
    "latent_vectors = torch.randn(image_count, z_dim, 1, 1).to(device)\n",
    "print(latent_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f38b7d-14f6-4d63-9efa-cb96ec2e1d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Display one of the random image\n",
    "img = gen(latent_vectors[56].to(device)).squeeze(dim=0).squeeze(dim=0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad57ff-7961-4078-9dd4-1e0b4879ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b924af9160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.axis(False)\n",
    "plt.title('Recovered Image:')\n",
    "plt.imshow(img.cpu().detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b314d-63c0-46e8-86b1-cce3bdb46bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.unsqueeze(dim=0).unsqueeze(dim=0) # Add the two extra dimensions to make it (batch_size, channels, h, w)\n",
    "#print(img.shape)\n",
    "img = img.repeat(1, 3, 1, 1) # Increase the color channels from 1 to 3 by repeating the color channel\n",
    "img = functional.interpolate(img, size=(32, 32), mode='nearest') # Change the image shape from 64x64(DCGAN output) to 32x32(Model input)\n",
    "        \n",
    "hs, logits = fedavg_model(img).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb89fe-aecc-45a2-9436-47dafdd0e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae3c86-eaad-486b-b473-baaf69ca9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send all the 80 images in batch\n",
    "random_images = gen(latent_vectors.to(device)).squeeze(dim=0).squeeze(dim=0)\n",
    "print(random_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820f4ce-ce4d-42a7-9fbe-0c0072b001d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = random_images.repeat(1, 3, 1, 1)\n",
    "print(random_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79adf9a-d2e6-4ec6-b3d3-74abc29d1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images_resized = functional.interpolate(random_images, size=(32, 32), mode='nearest')\n",
    "random_images_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c3c2c-0f08-4bfe-b7cd-5a64f24573cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_images(model, images, target_class, prob_threshold):\n",
    "    hs, logits = model(images) # Send the images down the model and get the prediction logits\n",
    "    probs = functional.softmax(logits, dim=1) # Convert the logits into probabilities\n",
    "    mask = probs[:, target_class] > prob_threshold\n",
    "    filtered_images = images[mask]\n",
    "\n",
    "    return filtered_images, probs[mask][:, target_class] # Return the image and the corresponding probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76bbcdf-7285-420a-96b7-d863b74f11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "result, prob = filter_images(fedavg_model, random_images_resized, 0, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7e71b-145f-4427-a1a5-cafcf3d93153",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666deed3-a111-4a29-b485-158f96010a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedavg\n",
    "class0_filtered_fedavg, fedavg_class0_probs = filter_images(fedavg_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedavg, fedavg_class1_probs = filter_images(fedavg_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedavg, fedavg_class2_probs = filter_images(fedavg_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedavg, fedavg_class3_probs = filter_images(fedavg_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3c169-8b7c-485c-bb9c-6381800c9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FedavgDP\n",
    "class0_filtered_fedavgDP, fedavgDP_class0_probs = filter_images(fedavgDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedavgDP, fedavgDP_class1_probs = filter_images(fedavgDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedavgDP, fedavgDP_class2_probs = filter_images(fedavgDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedavgDP, fedavgDP_class3_probs = filter_images(fedavgDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2520d-4cbf-4704-9bb0-a063f68f9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feddyn\n",
    "class0_filtered_feddyn, feddyn_class0_probs = filter_images(feddyn_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_feddyn, feddyn_class1_probs = filter_images(feddyn_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_feddyn, feddyn_class2_probs = filter_images(feddyn_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_feddyn, feddyn_class3_probs = filter_images(feddyn_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ce4d2-186b-461a-987e-049043c4a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeddynDP\n",
    "class0_filtered_feddynDP, feddynDP_class0_probs = filter_images(feddynDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_feddynDP, feddynDP_class1_probs = filter_images(feddynDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_feddynDP, feddynDP_class2_probs = filter_images(feddynDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_feddynDP, feddynDP_class3_probs = filter_images(feddynDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53865e2-d63d-4053-a43d-5ac5d9699b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedopt\n",
    "class0_filtered_fedopt, fedopt_class0_probs = filter_images(fedopt_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedopt, fedopt_class1_probs = filter_images(fedopt_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedopt, fedopt_class2_probs = filter_images(fedopt_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedopt, fedopt_class3_probs = filter_images(fedopt_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd39f69-b58c-409b-9d1b-4c8c0d89b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedopt\n",
    "class0_filtered_fedoptDP, fedoptDP_class0_probs = filter_images(fedoptDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_fedoptDP, fedoptDP_class1_probs = filter_images(fedoptDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_fedoptDP, fedoptDP_class2_probs = filter_images(fedoptDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_fedoptDP, fedoptDP_class3_probs = filter_images(fedoptDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babd679-a366-44d1-bd4e-34b7f10d56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon\n",
    "class0_filtered_moon, moon_class0_probs = filter_images(moon_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_moon, moon_class1_probs = filter_images(moon_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_moon, moon_class2_probs = filter_images(moon_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_moon, moon_class3_probs = filter_images(moon_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f3fbd-4614-4d5d-846e-bd11d9df7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon\n",
    "class0_filtered_moonDP, moonDP_class0_probs = filter_images(moonDP_model, random_images_resized, 0, prob_threshold)\n",
    "class1_filtered_moonDP, moonDP_class1_probs = filter_images(moonDP_model, random_images_resized, 1, prob_threshold)\n",
    "class2_filtered_moonDP, moonDP_class2_probs = filter_images(moonDP_model, random_images_resized, 2, prob_threshold)\n",
    "class3_filtered_moonDP, moonDP_class3_probs = filter_images(moonDP_model, random_images_resized, 3, prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a9351-59ad-41aa-9da1-9310318a607c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the probabilities\n",
    "print('Model: FedAvg')\n",
    "print('Class: 0')\n",
    "print(fedavg_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedavg_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedavg_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedavg_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d47323-85f6-4dfa-a08e-d2f57edc1dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the probabilities\n",
    "print('Model: FedAvgDP')\n",
    "print('Class: 0')\n",
    "print(fedavgDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedavgDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedavgDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedavgDP_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d883c-7d54-49cd-9eda-2b492fb7db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model: FedDyn')\n",
    "print('Class: 0')\n",
    "print(feddyn_class0_probs)\n",
    "print('Class: 1')\n",
    "print(feddyn_class1_probs)\n",
    "print('Class: 2')\n",
    "print(feddyn_class2_probs)\n",
    "print('Class: 3')\n",
    "print(feddyn_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c8c41-f1e6-488f-8b81-f6dbfcb3614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model: FedDynDP')\n",
    "print('Class: 0')\n",
    "print(feddynDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(feddynDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(feddynDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(feddynDP_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2eff4-8fe2-437a-a987-255d658ce788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model: FedOpt')\n",
    "print('Class: 0')\n",
    "print(fedopt_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedopt_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedopt_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedopt_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460fb67-e8be-4b34-a25b-5b2d0265c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model: FedOptDP')\n",
    "print('Class: 0')\n",
    "print(fedoptDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(fedoptDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(fedoptDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(fedoptDP_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dc2c6-1ee6-4fc6-9e4c-9f8f91bde8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model: Moon')\n",
    "print('Class: 0')\n",
    "print(moon_class0_probs)\n",
    "print('Class: 1')\n",
    "print(moon_class1_probs)\n",
    "print('Class: 2')\n",
    "print(moon_class2_probs)\n",
    "print('Class: 3')\n",
    "print(moon_class3_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42297d8-c92f-4b0c-97bb-ec2f205908d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model: MoonDP')\n",
    "print('Class: 0')\n",
    "print(moonDP_class0_probs)\n",
    "print('Class: 1')\n",
    "print(moonDP_class1_probs)\n",
    "print('Class: 2')\n",
    "print(moonDP_class2_probs)\n",
    "print('Class: 3')\n",
    "print(moonDP_class3_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
