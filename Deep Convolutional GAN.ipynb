{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3a6720-f4d4-487b-b9b4-b4c5b0dff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Deep Convolutional Generative Adverserial Network that creates realistic MNIST digit images\n",
    "# Author: Suraj Neupane\n",
    "# Written from scratch as a part of a Research Project 2025, Concordia University of Edmonton.\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fb5668-79ac-461c-93fb-601c6828aae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba3a436-a29d-4c49-a3da-43cc447768dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9beae6ba-c5e2-40c3-b2bc-bf8fef7508c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "img_channels = 1\n",
    "# Transform the images to 64x64\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(img_channels)], [0.5 for _ in range(img_channels)]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdb890e-234a-463e-811b-74baf1324f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "MNIST_TRAIN = datasets.MNIST(root='/datasets/', train=True, transform=transformations, download=True)\n",
    "MNIST_TEST = datasets.MNIST(root='/datasets/', train=False, transform=transformations, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aee7a6e-d30c-48c8-9f17-7d429ea874a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset sizes\n",
    "len(MNIST_TRAIN), len(MNIST_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b68ebca-11db-45a8-8bef-1ee9fa8b2575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single training example\n",
    "data_sample, label = MNIST_TRAIN[0]\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae00b4e-f4e9-46e1-9f4c-aebaae18661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dataloader\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = DataLoader(dataset=MNIST_TRAIN, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(dataset=MNIST_TEST, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65717445-5cfc-4527-be35-41454d878dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 64, 64]), torch.Size([128]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_dataloader)\n",
    "first_batch_samples, first_batch_labels = next(train_iter)\n",
    "first_batch_samples.shape, first_batch_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb11c2ef-b3b0-430a-9660-d6b30b010caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bcf9fef-a6f2-4a0c-a927-870911eadab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 64]), 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize a data sample\n",
    "data_sample, data_label = first_batch_samples[0], first_batch_labels[0]\n",
    "data_sample.shape, data_label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe61bcb-0017-407f-8908-141493ea9845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b8d5150bb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(f'Digit: {data_label.item()}')\n",
    "plt.axis(False)\n",
    "plt.imshow(data_sample.squeeze(dim=0), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979c5c00-402c-4cc2-a54b-09807366602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DC-GAN\n",
    "# Creating the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input shape: img_channels x 64 x 64\n",
    "            nn.Conv2d(\n",
    "              in_channels=img_channels, out_channels=features_d, kernel_size=4, stride=2, padding=1\n",
    "            ), # Output shape: features_d x 32 x 32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1), # Output shape: features_d*2 x 16 x 16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), # Output shape: features_d*4 x 8 x 8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), # Output shape: features_d*8 x 4 x 4\n",
    "           \n",
    "            nn.Conv2d(in_channels=features_d*8, out_channels=1, kernel_size=4, stride=2, padding=0), # Output shape: 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.disc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d74e527-2960-456a-a09a-bda128387337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels, features_g):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0),\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1),\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1),\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=features_g*2, out_channels=img_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.gen(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e70627-f849-49a5-a720-3b528ea4ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8e5466-2f2d-4cdd-9a1d-b2035bf0c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the output shapes of the Discriminator and the Generator\n",
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    z_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    initialize_weights(disc)\n",
    "\n",
    "    print(disc(x).shape)\n",
    "    assert disc(x).shape == (N, 1, 1, 1)\n",
    "\n",
    "    gen = Generator(z_dim, in_channels, 8)\n",
    "    initialize_weights(gen)\n",
    "    z = torch.randn((N, z_dim, 1, 1))\n",
    "\n",
    "    print(gen(z).shape)\n",
    "    assert(gen(z).shape) == (N, in_channels, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796beb13-12fb-49e8-a4b4-b172798dbb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1, 1])\n",
      "torch.Size([8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a403cf18-b222-4b8d-8e63-63da7286b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the hyperparameters\n",
    "learning_rate = 2e-4\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "img_channels = 1\n",
    "z_dim = 100\n",
    "epochs = 5\n",
    "features_disc = 64\n",
    "features_gen = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e29c242b-0645-426f-9b73-f94df06f510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instances\n",
    "gen = Generator(z_dim, img_channels, features_gen).to(device)\n",
    "disc = Discriminator(img_channels, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89e4cba-fa56-4686-adf0-f648351ff389",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_weights(gen)\n",
    "initialize_weights(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61b63f9d-e4af-4604-b3bc-cc338ae38fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and the loss function\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "opt_disc = torch.optim.Adam(disc.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc085220-ae42-4936-a3b2-ac52c41f5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fbc5de3-c51c-4378-b922-19596f757098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the models to training mode\n",
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b1f68-953b-4e64-a681-2a95f6b71a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Batches seen: 0.\n",
      "Epoch: 0. Batches seen: 50.\n",
      "Epoch: 0. Batches seen: 100.\n",
      "Epoch: 0. Batches seen: 150.\n",
      "Epoch: 0. Batches seen: 200.\n",
      "Epoch: 0. Batches seen: 250.\n",
      "Epoch: 0. Batches seen: 300.\n",
      "Epoch: 0. Batches seen: 350.\n",
      "Epoch: 0. Batches seen: 400.\n",
      "Epoch: 0. Batches seen: 450.\n",
      "Epoch: 1. Batches seen: 0.\n"
     ]
    }
   ],
   "source": [
    "fake_images = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (real_images, labels) in enumerate(train_dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        fake = gen(noise)\n",
    "        \n",
    "        # Train the discriminator max log(D(X)) + log(1 - (D(G(X))))\n",
    "        disc_real = disc(real_images).reshape(-1) # Flatten the output\n",
    "        loss_disc_real = loss_function(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).reshape(-1)\n",
    "        loss_disc_fake = loss_function(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train the Generator max log(D(G(z)))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = loss_function(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_index % 50 == 0:\n",
    "            print(f'Epoch: {epoch}. Batches seen: {batch_index}.')\n",
    "            fake_images.append(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a787d-eeb3-4085-b669-a744ecff7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795b4d7-408c-40e5-9347-f2e36538bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image = fake_images[-1]\n",
    "fake_image[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754a1be-efa1-4673-8fed-5d9b4768b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fake Generated Images:')\n",
    "\n",
    "for i in range(1, 17):\n",
    "    grid = plt.subplot(4, 4, i)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(fake_image[i][0].squeeze(dim=0).cpu().detach().numpy(), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b27ba-8a4f-4ff3-a140-ff2685aacb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
