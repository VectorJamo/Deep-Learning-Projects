{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2c6d3b-2600-4c76-9bca-7a2226200f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset # Base dataset class\n",
    "from torch.utils.data import DataLoader # For batching and shuffling data\n",
    "\n",
    "from torchvision import datasets # For built-in datasets\n",
    "from torchvision import transforms # For transforming data from one format to another\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773adaa4-f3e6-4bc3-8432-39c7b74dc09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d5cdcb-a31b-4b8a-8275-081256dbb7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc21c91-fcab-4db8-a7f4-f6f914a109bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the MNIST handwritten digits dataset\n",
    "# Get the training dataset\n",
    "MNIST_DIGITS_TRAIN = datasets.MNIST(root='./datasets/', \n",
    "                              train=True, download=True, \n",
    "                              transform=transforms.ToTensor())\n",
    "# Get the testing dataset\n",
    "MNIST_DIGITS_TEST = datasets.MNIST(root='./datasets/', \n",
    "                              train=False, download=True, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "len(MNIST_DIGITS_TRAIN), len(MNIST_DIGITS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aaaa03-37c2-4364-b43e-5397a7990d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = MNIST_DIGITS_TRAIN.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e348ae-c7e4-479a-aa23-6b68d887074c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize our data\n",
    "sample_X, sample_label = MNIST_DIGITS_TRAIN[0]\n",
    "sample_X, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1efe50f8-b6b9-411c-906f-cafbf8a2822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X.shape # (C, H, W) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322d6f2e-8553-4074-aaf1-ab4fb8cd437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X = sample_X.squeeze(dim=0) # Get rid of the outer dimension\n",
    "sample_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36c9be7-d0af-4baf-bcad-e739215100e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 5 - five')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the image data using matplotlib\n",
    "plt.imshow(sample_X, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.title(f'Label: {LABELS[sample_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd3bf2e-6321-49b3-ba38-0f9979e4c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to plot random images\n",
    "def plot_sample(index):\n",
    "    # Get the image onto the correct format\n",
    "    sample_X, sample_label = MNIST_DIGITS_TRAIN[index]\n",
    "    sample_X = sample_X.squeeze(dim=0)\n",
    "\n",
    "    # Show the image\n",
    "    plt.imshow(sample_X, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(f'Label: {LABELS[sample_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb14630d-bcb7-4a12-a1f4-f476e6445cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(5) # Plot the 6th image in our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b04a1a-eef3-4f96-b7cf-abc258c7fb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "636359ba-f6ea-4619-95af-ced176cfa326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training batches of size 32: 1875\n",
      "No. of testing batches of size 32: 313\n"
     ]
    }
   ],
   "source": [
    "# Turn the image data into batches\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset=MNIST_DIGITS_TRAIN, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=MNIST_DIGITS_TEST, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'No. of training batches of size {batch_size}: {len(train_dataloader)}')\n",
    "print(f'No. of testing batches of size {batch_size}: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c8da1e-6850-4e2e-9b91-2fb4a042eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN Model\n",
    "class MNISTDigitClassifier(nn.Module):\n",
    "    def __init__(self, in_features, hidden_units, out_features):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=hidden_units, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), # (B, C, H, W) -> (B, C*H*W)\n",
    "            nn.Linear(in_features=360, out_features=len(LABELS)) # The value 360 is found by the computations\n",
    "            # done in the cells below.\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        conv1_output = self.conv_layer_1(X)\n",
    "        conv2_output = self.conv_layer_2(conv1_output)\n",
    "        classifier_output = self.classifier(conv2_output)\n",
    "        \n",
    "        return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c701e1ed-fe0a-4074-ba54-45a585c034e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original image: torch.Size([1, 1, 28, 28])\n",
      "Shape after first conv. layer: torch.Size([1, 10, 13, 13])\n",
      "Shape after second conv. layer: torch.Size([1, 10, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# First, we will send a single image through our convolutional layers to see the transformed output shape of our image\n",
    "# after going through the two convolutional layers\n",
    "\n",
    "single_image, single_image_label =  MNIST_DIGITS_TRAIN[0]# (C, H, W)\n",
    "single_image = single_image.unsqueeze(dim=0) # (B, C, H, W)\n",
    "\n",
    "# Just the convolution layers\n",
    "conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "conv_layer_2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=10, out_channels=len(LABELS), kernel_size=(2, 2), stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2))\n",
    ")\n",
    "\n",
    "\n",
    "# Pass it through the model\n",
    "output1 = conv_layer_1(single_image)\n",
    "output2 = conv_layer_2(output1)\n",
    "print(f'Shape of the original image: {single_image.shape}')\n",
    "print(f'Shape after first conv. layer: {output1.shape}')\n",
    "print(f'Shape after second conv. layer: {output2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c113ccc-faf9-4e81-8213-8ef35863faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we can see that the original image (1, 1, 28, 28) gets transformed into a shape of (1, 10, 6, 6). \n",
    "# After flattening the image, we will end up with a shape of (1, 360).\n",
    "# Therefore, the in_features for the linear layer should be 360.\n",
    "# We will update this in the model above.\n",
    "cnn_model = MNISTDigitClassifier(in_features=1, hidden_units=10, out_features=len(LABELS))\n",
    "logits = cnn_model(single_image)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2853150f-9943-4cc3-85d7-5b0a71b7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and the optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a74694af-3473-4918-9547-2b40d9ccbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing loops\n",
    "def train_step(model, X, y): # X -> batch of images, y -> vector of labels\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(X) \n",
    "\n",
    "    # Calculate the loss\n",
    "    # The loss function expects the shape of the logits to be in [batch_size, no. of classes] and the shape of the truth labels to be in \n",
    "    # [batch_size, ]\n",
    "    loss = loss_function(logits, y)\n",
    "\n",
    "    # Clear the gradients stored in the model parameters from previous backpropagation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backpropagate (Calculate gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return {'loss': loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55cf65d8-012e-4d0b-8137-9cd989bb4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, X, y):\n",
    "    # Put the model in testing mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(logits, y)\n",
    "        \n",
    "        predicted_labels = logits.argmax(dim=1)\n",
    "\n",
    "        # Calculate the current batch's accuracy\n",
    "        total_accurate = (predicted_labels == y).sum().item()\n",
    "        accuracy_percentage = (float(total_accurate) / len(y)) * 100\n",
    "\n",
    "    return {'loss': loss.item(), 'accuracy': accuracy_percentage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a17adbba-3a35-47ef-b639-8acd3d2d380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "cnn_model = cnn_model.to(device)\n",
    "next(cnn_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccae2ae4-c5ef-4399-b84f-ac7990b371d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1777780055999756\n"
     ]
    }
   ],
   "source": [
    "# Run through a single train and test step\n",
    "train_dict = train_step(cnn_model, single_image.to(device), torch.tensor(single_image_label).unsqueeze(dim=0).to(device))\n",
    "print(train_dict['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76eb3604-0e0e-4475-a2e1-748fd899f502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1. Total batches: 1875\n",
      "Batch: 200. Training Loss: 1.7405498027801514\n",
      "Average test loss: 1.7134228110694276. Average test accuracy: 67.85143769968052%\n",
      "Batch: 400. Training Loss: 0.4928736686706543\n",
      "Average test loss: 0.6269076328022412. Average test accuracy: 80.80071884984025%\n",
      "Batch: 600. Training Loss: 0.6698384284973145\n",
      "Average test loss: 0.5087826070122825. Average test accuracy: 84.30511182108626%\n",
      "Batch: 800. Training Loss: 0.45101645588874817\n",
      "Average test loss: 0.43573859169745977. Average test accuracy: 86.86102236421725%\n",
      "Batch: 1000. Training Loss: 0.3677247166633606\n",
      "Average test loss: 0.4113871058740745. Average test accuracy: 87.83945686900958%\n",
      "Batch: 1200. Training Loss: 0.16147038340568542\n",
      "Average test loss: 0.4008178448857972. Average test accuracy: 88.00918530351437%\n",
      "Batch: 1400. Training Loss: 0.31346744298934937\n",
      "Average test loss: 0.3959602352124624. Average test accuracy: 88.07907348242811%\n",
      "Batch: 1600. Training Loss: 0.21217885613441467\n",
      "Average test loss: 0.3885050705089546. Average test accuracy: 88.02915335463258%\n",
      "Batch: 1800. Training Loss: 0.26448386907577515\n",
      "Average test loss: 0.35075520715131736. Average test accuracy: 89.57667731629392%\n",
      "EPOCH: 2. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.3500226140022278\n",
      "Average test loss: 0.36073494775179094. Average test accuracy: 89.27715654952077%\n",
      "Batch: 400. Training Loss: 0.2618873715400696\n",
      "Average test loss: 0.3339220418181187. Average test accuracy: 89.67651757188499%\n",
      "Batch: 600. Training Loss: 0.2991390526294708\n",
      "Average test loss: 0.31353291034246217. Average test accuracy: 90.75479233226837%\n",
      "Batch: 800. Training Loss: 0.09863471984863281\n",
      "Average test loss: 0.2993337244104844. Average test accuracy: 91.13418530351437%\n",
      "Batch: 1000. Training Loss: 0.3836115300655365\n",
      "Average test loss: 0.29886450851187346. Average test accuracy: 91.29392971246007%\n",
      "Batch: 1200. Training Loss: 0.20232434570789337\n",
      "Average test loss: 0.29414040889269627. Average test accuracy: 91.35383386581469%\n",
      "Batch: 1400. Training Loss: 0.2255052924156189\n",
      "Average test loss: 0.30219575510428737. Average test accuracy: 91.02436102236422%\n",
      "Batch: 1600. Training Loss: 0.2858186662197113\n",
      "Average test loss: 0.26970072460179323. Average test accuracy: 92.2623801916933%\n",
      "Batch: 1800. Training Loss: 0.4512728452682495\n",
      "Average test loss: 0.26685120218608993. Average test accuracy: 92.00279552715655%\n",
      "EPOCH: 3. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.23923389613628387\n",
      "Average test loss: 0.24472119067959225. Average test accuracy: 93.02116613418531%\n",
      "Batch: 400. Training Loss: 0.13799771666526794\n",
      "Average test loss: 0.23371354477128947. Average test accuracy: 93.47044728434504%\n",
      "Batch: 600. Training Loss: 0.14083831012248993\n",
      "Average test loss: 0.2314267169636564. Average test accuracy: 93.17092651757189%\n",
      "Batch: 800. Training Loss: 0.2864437699317932\n",
      "Average test loss: 0.2306818300441574. Average test accuracy: 93.59025559105432%\n",
      "Batch: 1000. Training Loss: 0.2417679876089096\n",
      "Average test loss: 0.21653597015804185. Average test accuracy: 93.97963258785943%\n",
      "Batch: 1200. Training Loss: 0.09298538416624069\n",
      "Average test loss: 0.20801598332024895. Average test accuracy: 93.70007987220447%\n",
      "Batch: 1400. Training Loss: 0.46419042348861694\n",
      "Average test loss: 0.20461724073694537. Average test accuracy: 94.05950479233226%\n",
      "Batch: 1600. Training Loss: 0.23424753546714783\n",
      "Average test loss: 0.19721303645152444. Average test accuracy: 94.129392971246%\n",
      "Batch: 1800. Training Loss: 0.2787794768810272\n",
      "Average test loss: 0.1967902585206488. Average test accuracy: 94.09944089456869%\n",
      "EPOCH: 4. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.09172872453927994\n",
      "Average test loss: 0.18331133971621577. Average test accuracy: 94.62859424920129%\n",
      "Batch: 400. Training Loss: 0.16332252323627472\n",
      "Average test loss: 0.17956269144143064. Average test accuracy: 94.74840255591054%\n",
      "Batch: 600. Training Loss: 0.11886091530323029\n",
      "Average test loss: 0.186179280505715. Average test accuracy: 94.54872204472844%\n",
      "Batch: 800. Training Loss: 0.24019403755664825\n",
      "Average test loss: 0.17794729085020222. Average test accuracy: 94.59864217252397%\n",
      "Batch: 1000. Training Loss: 0.07748737186193466\n",
      "Average test loss: 0.16808648193671633. Average test accuracy: 95.12779552715655%\n",
      "Batch: 1200. Training Loss: 0.11332385241985321\n",
      "Average test loss: 0.17069466690125223. Average test accuracy: 95.06789137380191%\n",
      "Batch: 1400. Training Loss: 0.2417713701725006\n",
      "Average test loss: 0.1588076724977301. Average test accuracy: 95.5870607028754%\n",
      "Batch: 1600. Training Loss: 0.09535174071788788\n",
      "Average test loss: 0.15894440088284234. Average test accuracy: 95.35742811501598%\n",
      "Batch: 1800. Training Loss: 0.11514803767204285\n",
      "Average test loss: 0.1607297103727064. Average test accuracy: 95.2376198083067%\n",
      "EPOCH: 5. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.5048041343688965\n",
      "Average test loss: 0.16647669972000886. Average test accuracy: 95.0179712460064%\n",
      "Batch: 400. Training Loss: 0.33772408962249756\n",
      "Average test loss: 0.1517353864868704. Average test accuracy: 95.64696485623003%\n",
      "Batch: 600. Training Loss: 0.07756635546684265\n",
      "Average test loss: 0.14978056338083107. Average test accuracy: 95.56709265175719%\n",
      "Batch: 800. Training Loss: 0.1647692173719406\n",
      "Average test loss: 0.14807605317171912. Average test accuracy: 95.87659744408946%\n",
      "Batch: 1000. Training Loss: 0.06063329055905342\n",
      "Average test loss: 0.16118319106017487. Average test accuracy: 95.37739616613419%\n",
      "Batch: 1200. Training Loss: 0.046120572835206985\n",
      "Average test loss: 0.13736661903250713. Average test accuracy: 96.09624600638978%\n",
      "Batch: 1400. Training Loss: 0.06414051353931427\n",
      "Average test loss: 0.14290785213884788. Average test accuracy: 95.81669329073482%\n",
      "Batch: 1600. Training Loss: 0.09445130079984665\n",
      "Average test loss: 0.13582653074284284. Average test accuracy: 95.91653354632588%\n",
      "Batch: 1800. Training Loss: 0.23907960951328278\n",
      "Average test loss: 0.13045844267577694. Average test accuracy: 96.37579872204473%\n",
      "EPOCH: 6. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.056844767183065414\n",
      "Average test loss: 0.13162710865179952. Average test accuracy: 96.1261980830671%\n",
      "Batch: 400. Training Loss: 0.11155623197555542\n",
      "Average test loss: 0.12994689413587363. Average test accuracy: 96.05630990415335%\n",
      "Batch: 600. Training Loss: 0.2559232711791992\n",
      "Average test loss: 0.13125882615757803. Average test accuracy: 96.14616613418531%\n",
      "Batch: 800. Training Loss: 0.06376839429140091\n",
      "Average test loss: 0.13410325957828176. Average test accuracy: 96.17611821086263%\n",
      "Batch: 1000. Training Loss: 0.176239475607872\n",
      "Average test loss: 0.12322788381095107. Average test accuracy: 96.47563897763578%\n",
      "Batch: 1200. Training Loss: 0.29327312111854553\n",
      "Average test loss: 0.1220697569207083. Average test accuracy: 96.37579872204473%\n",
      "Batch: 1400. Training Loss: 0.07024629414081573\n",
      "Average test loss: 0.12221213367086249. Average test accuracy: 96.26597444089457%\n",
      "Batch: 1600. Training Loss: 0.11978395283222198\n",
      "Average test loss: 0.12150984934974497. Average test accuracy: 96.08626198083067%\n",
      "Batch: 1800. Training Loss: 0.03799790143966675\n",
      "Average test loss: 0.12438845999253254. Average test accuracy: 96.33586261980831%\n",
      "EPOCH: 7. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.06354126334190369\n",
      "Average test loss: 0.12160836002557351. Average test accuracy: 96.28594249201278%\n",
      "Batch: 400. Training Loss: 0.06671535968780518\n",
      "Average test loss: 0.11984832796413963. Average test accuracy: 96.26597444089457%\n",
      "Batch: 600. Training Loss: 0.21526002883911133\n",
      "Average test loss: 0.11988323853952221. Average test accuracy: 96.47563897763578%\n",
      "Batch: 800. Training Loss: 0.09292282164096832\n",
      "Average test loss: 0.11600135907102317. Average test accuracy: 96.54552715654953%\n",
      "Batch: 1000. Training Loss: 0.390774130821228\n",
      "Average test loss: 0.11054705885185387. Average test accuracy: 96.52555910543131%\n",
      "Batch: 1200. Training Loss: 0.10115113109350204\n",
      "Average test loss: 0.11648870996993999. Average test accuracy: 96.41573482428115%\n",
      "Batch: 1400. Training Loss: 0.07206149399280548\n",
      "Average test loss: 0.109042934345501. Average test accuracy: 96.80511182108626%\n",
      "Batch: 1600. Training Loss: 0.11576928943395615\n",
      "Average test loss: 0.1109856655687308. Average test accuracy: 96.44568690095846%\n",
      "Batch: 1800. Training Loss: 0.026706794276833534\n",
      "Average test loss: 0.11257047764770985. Average test accuracy: 96.5055910543131%\n",
      "EPOCH: 8. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.07925142347812653\n",
      "Average test loss: 0.10776393792960948. Average test accuracy: 96.64536741214057%\n",
      "Batch: 400. Training Loss: 0.15133516490459442\n",
      "Average test loss: 0.10896780885854207. Average test accuracy: 96.71525559105432%\n",
      "Batch: 600. Training Loss: 0.12343847751617432\n",
      "Average test loss: 0.10492916952278585. Average test accuracy: 96.6952875399361%\n",
      "Batch: 800. Training Loss: 0.2758727967739105\n",
      "Average test loss: 0.10891411528318597. Average test accuracy: 96.76517571884985%\n",
      "Batch: 1000. Training Loss: 0.17654816806316376\n",
      "Average test loss: 0.10210893797753647. Average test accuracy: 96.875%\n",
      "Batch: 1200. Training Loss: 0.0083534624427557\n",
      "Average test loss: 0.11074655060619175. Average test accuracy: 96.5155750798722%\n",
      "Batch: 1400. Training Loss: 0.1126275509595871\n",
      "Average test loss: 0.1036860566525259. Average test accuracy: 96.80511182108626%\n",
      "Batch: 1600. Training Loss: 0.12193851172924042\n",
      "Average test loss: 0.10591767266795885. Average test accuracy: 96.91493610223642%\n",
      "Batch: 1800. Training Loss: 0.08467257022857666\n",
      "Average test loss: 0.10780869842099305. Average test accuracy: 96.85503194888179%\n",
      "EPOCH: 9. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.049430184066295624\n",
      "Average test loss: 0.12198404121932725. Average test accuracy: 96.20607028753993%\n",
      "Batch: 400. Training Loss: 0.09551754593849182\n",
      "Average test loss: 0.10276319802301331. Average test accuracy: 96.65535143769968%\n",
      "Batch: 600. Training Loss: 0.07763087004423141\n",
      "Average test loss: 0.0967355750719853. Average test accuracy: 97.07468051118211%\n",
      "Batch: 800. Training Loss: 0.37019217014312744\n",
      "Average test loss: 0.09932794701308012. Average test accuracy: 96.8849840255591%\n",
      "Batch: 1000. Training Loss: 0.10861555486917496\n",
      "Average test loss: 0.0974717255400571. Average test accuracy: 96.8650159744409%\n",
      "Batch: 1200. Training Loss: 0.09604982286691666\n",
      "Average test loss: 0.10164904002414142. Average test accuracy: 96.875%\n",
      "Batch: 1400. Training Loss: 0.15608814358711243\n",
      "Average test loss: 0.10031135074054888. Average test accuracy: 96.58546325878594%\n",
      "Batch: 1600. Training Loss: 0.1599656492471695\n",
      "Average test loss: 0.09803531329465894. Average test accuracy: 96.85503194888179%\n",
      "Batch: 1800. Training Loss: 0.014286813326179981\n",
      "Average test loss: 0.1037179012972558. Average test accuracy: 96.79512779552715%\n",
      "EPOCH: 10. Total batches: 1875\n",
      "Batch: 200. Training Loss: 0.18442000448703766\n",
      "Average test loss: 0.09243897489307763. Average test accuracy: 97.0547124600639%\n",
      "Batch: 400. Training Loss: 0.10287808626890182\n",
      "Average test loss: 0.09132982729975068. Average test accuracy: 97.03474440894568%\n",
      "Batch: 600. Training Loss: 0.23170432448387146\n",
      "Average test loss: 0.0894705602886548. Average test accuracy: 97.254392971246%\n",
      "Batch: 800. Training Loss: 0.06920024007558823\n",
      "Average test loss: 0.10480027953266931. Average test accuracy: 96.71525559105432%\n",
      "Batch: 1000. Training Loss: 0.0599210299551487\n",
      "Average test loss: 0.09117572322322692. Average test accuracy: 97.02476038338658%\n",
      "Batch: 1200. Training Loss: 0.029523320496082306\n",
      "Average test loss: 0.09011421938495229. Average test accuracy: 97.26437699680511%\n",
      "Batch: 1400. Training Loss: 0.0314510203897953\n",
      "Average test loss: 0.0902281956979261. Average test accuracy: 97.13458466453675%\n",
      "Batch: 1600. Training Loss: 0.05738314241170883\n",
      "Average test loss: 0.09287828239039503. Average test accuracy: 97.064696485623%\n",
      "Batch: 1800. Training Loss: 0.06261193007230759\n",
      "Average test loss: 0.09309771865589199. Average test accuracy: 97.17452076677316%\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing loop\n",
    "epochs = 10\n",
    "for i in range(0, epochs):\n",
    "    batch = 1\n",
    "    print(f'EPOCH: {i + 1}. Total batches: {len(train_dataloader)}')\n",
    "    for [X, y] in train_dataloader:\n",
    "        train_dict = train_step(cnn_model, X.to(device), y.to(device))\n",
    "        loss = train_dict['loss']\n",
    "        batch += 1\n",
    "\n",
    "        if batch%200 == 0:\n",
    "            print(f'Batch: {batch}. Training Loss: {loss}')\n",
    "\n",
    "            # Test the model\n",
    "            average_test_loss = 0\n",
    "            average_test_accuracy = 0\n",
    "            for [test_X, test_y] in test_dataloader:\n",
    "                test_dict = test_step(cnn_model, test_X.to(device), test_y.to(device))\n",
    "                \n",
    "                test_loss = test_dict['loss']\n",
    "                accuracy = test_dict['accuracy']\n",
    "\n",
    "                average_test_loss += test_loss\n",
    "                average_test_accuracy += accuracy\n",
    "                \n",
    "            average_test_loss = average_test_loss / len(test_dataloader)\n",
    "            average_test_accuracy = average_test_accuracy / len(test_dataloader)\n",
    "            \n",
    "            print(f'Average test loss: {average_test_loss}. Average test accuracy: {average_test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3dff10-5cd1-4e1e-8313-81ad28c2e88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
