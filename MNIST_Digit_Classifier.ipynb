{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2c6d3b-2600-4c76-9bca-7a2226200f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset # Base dataset class\n",
    "from torch.utils.data import DataLoader # For batching and shuffling data\n",
    "\n",
    "from torchvision import datasets # For built-in datasets\n",
    "from torchvision import transforms # For transforming data from one format to another\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773adaa4-f3e6-4bc3-8432-39c7b74dc09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d5cdcb-a31b-4b8a-8275-081256dbb7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc21c91-fcab-4db8-a7f4-f6f914a109bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the MNIST handwritten digits dataset\n",
    "# Get the training dataset\n",
    "MNIST_DIGITS_TRAIN = datasets.MNIST(root='./datasets/', \n",
    "                              train=True, download=True, \n",
    "                              transform=transforms.ToTensor())\n",
    "# Get the testing dataset\n",
    "MNIST_DIGITS_TEST = datasets.MNIST(root='./datasets/', \n",
    "                              train=False, download=True, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "len(MNIST_DIGITS_TRAIN), len(MNIST_DIGITS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aaaa03-37c2-4364-b43e-5397a7990d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = MNIST_DIGITS_TRAIN.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e348ae-c7e4-479a-aa23-6b68d887074c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize our data\n",
    "sample_X, sample_label = MNIST_DIGITS_TRAIN[0]\n",
    "sample_X, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1efe50f8-b6b9-411c-906f-cafbf8a2822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X.shape # (C, H, W) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322d6f2e-8553-4074-aaf1-ab4fb8cd437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X = sample_X.squeeze(dim=0) # Get rid of the outer dimension\n",
    "sample_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36c9be7-d0af-4baf-bcad-e739215100e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 5 - five')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the image data using matplotlib\n",
    "plt.imshow(sample_X, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.title(f'Label: {LABELS[sample_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd3bf2e-6321-49b3-ba38-0f9979e4c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to plot random images\n",
    "def plot_sample(index):\n",
    "    # Get the image onto the correct format\n",
    "    sample_X, sample_label = MNIST_DIGITS_TRAIN[index]\n",
    "    sample_X = sample_X.squeeze(dim=0)\n",
    "\n",
    "    # Show the image\n",
    "    plt.imshow(sample_X, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(f'Label: {LABELS[sample_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb14630d-bcb7-4a12-a1f4-f476e6445cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASiUlEQVR4nO3cf6xXBf3H8fflgnq5IIT3QuQMYphA4PilhHDlVtIFQYVWucT8Ecrm2vyBCrZWkJs5MouUVm2lEcVqI2ItqRgDx1UY6AyKEgUSBwEiAv4WAs73j76+F+IPzsfLj+Tx2PiDD+f1Oefejfvkc++HU1UURREAEBGtjvcFAHDiEAUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgWOi02bNkVVVVV897vfbbHnfPjhh6OqqioefvjhFntOONmIAkfs5z//eVRVVcXjjz9+vC/lqJg/f35cfvnl0aNHj2jbtm2cc845ceutt8aePXta/Fxz586NmTNntvjzwvvV+nhfAJwoJk2aFB/5yEfiyiuvjI9+9KPxt7/9LWbNmhULFy6MJ554ImpqalrsXHPnzo21a9fGzTff3GLPCS1BFOD/zZs3LxobGw95bNCgQXH11VfHr371q7juuuuOz4XBMeTbR7Soffv2xTe/+c0YNGhQdOjQIWpra6OhoSGWLl36jpvvf//70a1bt6ipqYkRI0bE2rVrDztm3bp18fnPfz46deoUp512WgwePDh+//vfv+f1vPbaa7Fu3brYuXPnex771iBERIwfPz4iIp588sn33B+pxsbGeOihh+LZZ5+NqqqqqKqqiu7du0dRFFFXVxeTJ0/OYw8ePBgdO3aM6urqQ76NNWPGjGjdunW88sor+diSJUuioaEhamtro2PHjnHZZZe16HVzchAFWtRLL70UP/3pT6OxsTFmzJgR06dPj+effz6amppi9erVhx3/i1/8Iu6777746le/Gl/72tdi7dq18elPfzqee+65PObvf/97fPKTn4wnn3wy7rjjjrj33nujtrY2xo0bF7/73e/e9XpWrVoVvXv3jlmzZlX08Wzfvj0iIurq6irav52vf/3r0b9//6irq4s5c+bEnDlzYubMmVFVVRXDhg2LZcuW5bF//etf48UXX4yIiEcffTQfb25ujgEDBkS7du0iImLx4sXR1NQUO3bsiOnTp8fkyZNj+fLlMWzYsNi0aVOLXTsngQKO0IMPPlhERPHYY4+94zH79+8v9u7de8hju3fvLrp06VJ85StfyceeeeaZIiKKmpqaYsuWLfn4ypUri4gobrnllnzsM5/5TNGvX7/ijTfeyMcOHjxYXHDBBcXZZ5+djy1durSIiGLp0qWHPTZt2rRKPuRi4sSJRXV1dfH0009XtH8nY8aMKbp163bY4/fcc09RXV1dvPTSS0VRFMV9991XdOvWrTj//POLqVOnFkVRFAcOHCg6dux4yOeof//+RefOnYsXXnghH1uzZk3RqlWr4qqrrmrRa+eDzSsFWlR1dXWccsopEfGfb33s2rUr9u/fH4MHD44nnnjisOPHjRsXZ555Zv7+/PPPjyFDhsTChQsjImLXrl2xZMmS+OIXvxgvv/xy7Ny5M3bu3BkvvPBCNDU1xfr16+Nf//rXO15PY2NjFEUR06dPL/2xzJ07N372s5/FrbfeGmeffXbpfSUaGhriwIEDsXz58oj4zyuChoaGaGhoiObm5oiIWLt2bezZsycaGhoiImLbtm2xevXquOaaa6JTp075XOeee26MHDkyP5dwJESBFjd79uw499xz47TTToszzjgj6uvr46GHHspvg/y3t/ti+/GPfzy/5bFhw4YoiiK+8Y1vRH19/SG/pk2bFhERO3bsaPGPobm5OSZOnBhNTU1x1113vefxL774Ymzfvj1/7dq1q6LzDhw4MNq2bZsBeDMKF154YTz++OPxxhtv5J8NHz48IiKeffbZiIg455xzDnu+3r17x86dO+PVV1+t6Ho4+Xj3ES3ql7/8ZVxzzTUxbty4uP3226Nz585RXV0dd999d2zcuLH08x08eDAiIm677bZoamp622N69uz5vq75rdasWROXXnpp9O3bN+bNmxetW7/3X5ObbropZs+enb8fMWJERf+Jrk2bNjFkyJBYtmxZbNiwIbZv3x4NDQ3RpUuX+Pe//x0rV66M5ubm6NWrV9TX15d+fngvokCLmjdvXvTo0SPmz58fVVVV+fib/6p/q/Xr1x/22NNPPx3du3ePiIgePXpExH++WF500UUtf8FvsXHjxhg1alR07tw5Fi5cmD/IfS9TpkyJK6+8Mn//oQ996F2P/+/PzVs1NDTEjBkzYvHixVFXVxe9evWKqqqq+MQnPhHNzc3R3NwcY8eOzeO7desWERFPPfXUYc+1bt26qKuri9ra2iP6OMC3j2hR1dXVERFRFEU+tnLlylixYsXbHr9gwYJDfiawatWqWLlyZYwePToiIjp37hyNjY3xk5/8JLZt23bY/vnnn3/X6ynzltTt27fHZz/72WjVqlX8+c9/LvUv8T59+sRFF12UvwYNGvSux9fW1r7tt9Mi/hOFvXv3xsyZM2P48OEZkIaGhpgzZ05s3bo1f54QEdG1a9fo379/zJ49+5C3ra5duzYWLVoUF1988RF/HOCVAqU98MAD8ac//emwx2+66aYYO3ZszJ8/P8aPHx9jxoyJZ555Jn784x9Hnz59DnlP/Zt69uwZw4cPjxtuuCG/EJ5xxhkxZcqUPOaHP/xhDB8+PPr16xfXX3999OjRI5577rlYsWJFbNmyJdasWfOO17pq1ar41Kc+FdOmTXvPHzaPGjUq/vnPf8aUKVPikUceiUceeST/rEuXLjFy5Mgj+OwcmUGDBsVvfvObmDx5cpx33nnRrl27uOSSSyIiYujQodG6det46qmnYtKkSbm58MIL40c/+lFExCFRiIi45557YvTo0TF06NCYOHFivP7663H//fdHhw4dKvohOyex4/zuJ/6HvPmW1Hf6tXnz5uLgwYPFt7/97aJbt27FqaeeWgwYMKD4wx/+UFx99dWHvAXzzbek3nPPPcW9995bnHXWWcWpp55aNDQ0FGvWrDns3Bs3biyuuuqq4sMf/nDRpk2b4swzzyzGjh1bzJs3L495v29JfbePbcSIEe/jM3e4V155pbjiiiuKjh07FhFx2NtTzzvvvCIiipUrV+ZjW7ZsKSKiOOuss972ORcvXlwMGzasqKmpKU4//fTikksuKf7xj3+06HXzwVdVFP/1Oh+Ak5qfKQCQRAGAJAoAJFEAIIkCAEkUAEhH/J/X3u2/5QNw4juS/4HglQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqfXxvgD4X9a+ffvSm3bt2lV0rjFjxpTe1NfXl95873vfK73Zu3dv6Q0nJq8UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQ3BCPD6Tu3buX3kydOrX0ZujQoaU3ffv2Lb05lrp27Vp6c+ONNx6FK+F48EoBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpqiiK4ogOrKo62tfCB1yvXr0q2t18882lNxMmTCi9qampKb2p5O/F5s2bS28iIl5++eXSm969e5fe7Ny5s/SmsbGx9GbdunWlN7w/R/Ll3isFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgtT7eF8Dx16FDh9KbGTNmlN5cfvnlpTcREe3bt69odyysX7++9Kapqamic7Vp06b0ppI7kdbV1R2TDScmrxQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJDcEI8YP3586c111113FK7k+Nq4cWPpzciRI0tvNm/eXHoTEdGzZ8+KdlCGVwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEhuiEd84QtfON6X8K42bdpUevPYY4+V3kydOrX0ptKb21Wid+/ex+xcnLy8UgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHJDPOL6668vvZk0aVLpzaJFi0pvIiI2bNhQerNjx46KznUi69Kly/G+BE4CXikAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJXVKJrVu3lt5Mnz695S+EdzV06NDjfQmcBLxSACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAckM8PpBuvPHG0pva2tqjcCUtp1+/fsfkPMuXLy+9WbFixVG4Eo4HrxQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJDcEI+KtG3btvSmT58+FZ1r2rRppTcXX3xxRecqq1Wr8v+uOnjw4FG4kre3devW0ptrr7229ObAgQOlN5yYvFIAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEByQ7wPmDZt2pTeDBgwoPTmt7/9belN165dS28iIl5//fXSm0puBLdixYrSm1GjRpXeVHIzwUq1bl3+r/jnPve50psf/OAHpTf79u0rveHo80oBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpqiiK4ogOrKo62tfCfznllFMq2lVyg7b58+dXdK6yvvWtb1W0W7JkSenNo48+WnrTqVOn0ptKrq1v376lNye6CRMmlN4sWLCgonPt3bu3oh0RR/Ll3isFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAguUvqMdCmTZvSmzvvvLOic91+++0V7cr64x//WHrz5S9/uaJz7dmzp/Smvr6+9GbhwoWlNwMHDiy92bdvX+lNRMR3vvOd0ptK7sh62WWXld5UYvHixRXtZsyYUXqze/fuis5V1urVq4/JeSrlLqkAlCIKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJDfFKqq6uLr256667Sm9uu+220puIiFdffbX05o477ii9+fWvf116U+lNyQYPHlx6M2vWrGNyng0bNpTe3HDDDaU3ERFLly4tvTn99NNLby644ILSmwkTJpTeXHrppaU3ERG1tbUV7cravHlz6c3HPvaxo3AlLccN8QAoRRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIb4pVUyc3M7r///tKb1157rfQmImLSpEmlN4sWLSq9GTJkSOnNtddeW3oTETF69OjSm5qamtKbO++8s/TmwQcfLL2p5EZrH0Rf+tKXKtpdccUVLXwlb++WW24pvankBonHkhviAVCKKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJDfEK2nbtm2lN/X19aU3e/fuLb2JiFi3bl3pTW1tbelNz549S2+OpenTp5fe3H333aU3Bw4cKL2B48UN8QAoRRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIb4pX0l7/8pfSmX79+R+FKjq+FCxeW3ixbtqyicy1YsKD0ZtOmTaU3+/fvL72B/yVuiAdAKaIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDkLqkltW/fvvRm3LhxpTcDBw4svYmI2LFjR+nNAw88UHqze/fu0pt9+/aV3gAtx11SAShFFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkhviAZwk3BAPgFJEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBaH+mBRVEczesA4ATglQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIA6f8Ap1zLo/dBRH8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(5) # Plot the 6th image in our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b04a1a-eef3-4f96-b7cf-abc258c7fb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "636359ba-f6ea-4619-95af-ced176cfa326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training batches of size 32: 1875\n",
      "No. of testing batches of size 32: 313\n"
     ]
    }
   ],
   "source": [
    "# Turn the image data into batches\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset=MNIST_DIGITS_TRAIN, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=MNIST_DIGITS_TEST, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'No. of training batches of size {batch_size}: {len(train_dataloader)}')\n",
    "print(f'No. of testing batches of size {batch_size}: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c8da1e-6850-4e2e-9b91-2fb4a042eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN Model\n",
    "class MNISTDigitClassifier(nn.Module):\n",
    "    def __init__(self, in_features, hidden_units, out_features):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=hidden_units, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), # (B, C, H, W) -> (B, C*H*W)\n",
    "            nn.Linear(in_features=360, out_features=len(LABELS)) # The value 360 is found by the computations\n",
    "            # done in the cells below.\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        conv1_output = self.conv_layer_1(X)\n",
    "        conv2_output = self.conv_layer_2(conv1_output)\n",
    "        classifier_output = self.classifier(conv2_output)\n",
    "        \n",
    "        return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c701e1ed-fe0a-4074-ba54-45a585c034e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original image: torch.Size([1, 1, 28, 28])\n",
      "Shape after first conv. layer: torch.Size([1, 10, 13, 13])\n",
      "Shape after second conv. layer: torch.Size([1, 10, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# First, we will send a single image through our convolutional layers to see the transformed output shape of our image\n",
    "# after going through the two convolutional layers\n",
    "\n",
    "single_image, single_image_label =  MNIST_DIGITS_TRAIN[0]# (C, H, W)\n",
    "single_image = single_image.unsqueeze(dim=0) # (B, C, H, W)\n",
    "\n",
    "# Just the convolution layers\n",
    "conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(2, 2), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "conv_layer_2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=10, out_channels=len(LABELS), kernel_size=(2, 2), stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2))\n",
    ")\n",
    "\n",
    "\n",
    "# Pass it through the model\n",
    "output1 = conv_layer_1(single_image)\n",
    "output2 = conv_layer_2(output1)\n",
    "print(f'Shape of the original image: {single_image.shape}')\n",
    "print(f'Shape after first conv. layer: {output1.shape}')\n",
    "print(f'Shape after second conv. layer: {output2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c113ccc-faf9-4e81-8213-8ef35863faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we can see that the original image (1, 1, 28, 28) gets transformed into a shape of (1, 10, 6, 6). \n",
    "# After flattening the image, we will end up with a shape of (1, 360).\n",
    "# Therefore, the in_features for the linear layer should be 360.\n",
    "# We will update this in the model above.\n",
    "cnn_model = MNISTDigitClassifier(in_features=1, hidden_units=10, out_features=len(LABELS))\n",
    "logits = cnn_model(single_image)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2853150f-9943-4cc3-85d7-5b0a71b7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and the optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a74694af-3473-4918-9547-2b40d9ccbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing loops\n",
    "def train_step(model, X, y): # X -> batch of images, y -> vector of labels\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(X) \n",
    "\n",
    "    # Calculate the loss\n",
    "    # The loss function expects the shape of the logits to be in [batch_size, no. of classes] and the shape of the truth labels to be in \n",
    "    # [batch_size, ]\n",
    "    loss = loss_function(logits, y)\n",
    "\n",
    "    # Clear the gradients stored in the model parameters from previous backpropagation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backpropagate (Calculate gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return {'loss': loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55cf65d8-012e-4d0b-8137-9cd989bb4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, X, y):\n",
    "    # Put the model in testing mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(logits, y)\n",
    "        \n",
    "        predicted_labels = logits.argmax(dim=1)\n",
    "\n",
    "        # Calculate the current batch's accuracy\n",
    "        total_accurate = (predicted_labels == y).sum().item()\n",
    "        accuracy_percentage = (float(total_accurate) / len(y)) * 100\n",
    "\n",
    "    return {'loss': loss.item(), 'accuracy': accuracy_percentage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a17adbba-3a35-47ef-b639-8acd3d2d380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "cnn_model = cnn_model.to(device)\n",
    "next(cnn_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccae2ae4-c5ef-4399-b84f-ac7990b371d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.270822763442993\n"
     ]
    }
   ],
   "source": [
    "# Run through a single train and test step\n",
    "train_dict = train_step(cnn_model, single_image.to(device), torch.tensor(single_image_label).unsqueeze(dim=0).to(device))\n",
    "print(train_dict['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76eb3604-0e0e-4475-a2e1-748fd899f502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH: 1. ----------\n",
      "Batch: 200. Training Loss: 0.23241141438484192\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.3923683166503906\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.049823977053165436\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.2684915363788605\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.19791476428508759\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.2367895245552063\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.05905202031135559\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.02823125198483467\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.08453433215618134\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 2. ----------\n",
      "Batch: 200. Training Loss: 0.1048223078250885\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.0380023717880249\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.01167055033147335\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.3463962972164154\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.05373343452811241\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.1438724249601364\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.2143106907606125\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.014402676373720169\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.008383078500628471\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 3. ----------\n",
      "Batch: 200. Training Loss: 0.05278996750712395\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.03919908031821251\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.033858273178339005\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.11071018129587173\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.1200418472290039\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.01804264821112156\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.06939727813005447\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.056570880115032196\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.0629463717341423\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 4. ----------\n",
      "Batch: 200. Training Loss: 0.03695813938975334\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.018133148550987244\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.055433232337236404\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.026695428416132927\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.09675989300012589\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.019420886412262917\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.017735378816723824\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.01415748056024313\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.04035651311278343\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 5. ----------\n",
      "Batch: 200. Training Loss: 0.008387546986341476\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.004645808134227991\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.004070768598467112\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.015324953943490982\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.008699248544871807\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.02596203237771988\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.030740462243556976\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.006638189312070608\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.162949800491333\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 6. ----------\n",
      "Batch: 200. Training Loss: 0.023934371769428253\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.03128455579280853\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.14554619789123535\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.10769974440336227\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.118166483938694\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.09868801385164261\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.07687356323003769\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.1704510599374771\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.02002713829278946\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 7. ----------\n",
      "Batch: 200. Training Loss: 0.03704610839486122\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.02134162373840809\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.03386564552783966\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.006572504062205553\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.015216232277452946\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.04844595864415169\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.22489069402217865\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.0937703475356102\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.017794497311115265\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 8. ----------\n",
      "Batch: 200. Training Loss: 0.06939041614532471\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.007463549729436636\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.04036591202020645\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.027766739949584007\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.17911647260189056\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.00030971679370850325\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.03660006448626518\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.35124456882476807\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.02969132550060749\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 9. ----------\n",
      "Batch: 200. Training Loss: 0.17578011751174927\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.06794866174459457\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.06374482810497284\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.07607291638851166\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.02494574710726738\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.07899706810712814\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.2422465980052948\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.010374645702540874\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.009555209428071976\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 10. ----------\n",
      "Batch: 200. Training Loss: 0.041725676506757736\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.1292324811220169\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.00826176069676876\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.04687979444861412\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.047578975558280945\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.11276271939277649\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.005721956957131624\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.0057335845194756985\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.027184057980775833\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 11. ----------\n",
      "Batch: 200. Training Loss: 0.00465931324288249\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.024641025811433792\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.13722637295722961\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.03620995581150055\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.13563457131385803\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.009685130789875984\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.00815238431096077\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.023131145164370537\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.008694898337125778\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 12. ----------\n",
      "Batch: 200. Training Loss: 0.020157773047685623\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.00962876994162798\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.012883109971880913\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.030569344758987427\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.022530337795615196\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.0953640416264534\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.01776859350502491\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.012082132510840893\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.0065842573530972\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 13. ----------\n",
      "Batch: 200. Training Loss: 0.020750783383846283\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.024556759744882584\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.029492750763893127\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.011037840507924557\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.08192747086286545\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.07851488143205643\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.11896728724241257\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.03333601728081703\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.04496077075600624\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 14. ----------\n",
      "Batch: 200. Training Loss: 0.07102565467357635\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.07815228402614594\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.019019268453121185\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.002276065992191434\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.016080819070339203\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.07964049279689789\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.29050105810165405\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.00458105094730854\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.01249495055526495\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 15. ----------\n",
      "Batch: 200. Training Loss: 0.03987525776028633\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.026333695277571678\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.13104906678199768\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.002970539266243577\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.00641059223562479\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.3207494914531708\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.020091047510504723\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.045437272638082504\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.5344027280807495\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 16. ----------\n",
      "Batch: 200. Training Loss: 0.03820003569126129\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.007020543795078993\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.008425625041127205\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.013334267772734165\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.0935307964682579\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.015826845541596413\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.02687966078519821\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.001602854230441153\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.012612164951860905\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 17. ----------\n",
      "Batch: 200. Training Loss: 0.003020066535100341\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.056607674807310104\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.05675608664751053\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.002879779553040862\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.3604629635810852\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.009749840945005417\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.02193428762257099\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.0038453375454992056\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.08653543889522552\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 18. ----------\n",
      "Batch: 200. Training Loss: 0.019613102078437805\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.08722156286239624\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.0030948480125516653\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.005593277048319578\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.11426126956939697\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.003473792690783739\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.12440622597932816\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.03240233287215233\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.0067863925360143185\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 19. ----------\n",
      "Batch: 200. Training Loss: 0.0015993255656212568\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.12583184242248535\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.025336051359772682\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.10962935537099838\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.11396423727273941\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.03968224301934242\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.025971312075853348\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.049366746097803116\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.18101122975349426\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 20. ----------\n",
      "Batch: 200. Training Loss: 0.007470638025552034\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.0007223884458653629\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.13304755091667175\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.1308586597442627\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.025281893089413643\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.040456149727106094\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.1512293815612793\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.0022983786184340715\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.03653218224644661\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 21. ----------\n",
      "Batch: 200. Training Loss: 0.016485534608364105\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.00499376654624939\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.0041138953529298306\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.0013098622439429164\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.05007578060030937\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.06043762341141701\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.04835519194602966\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.08476570248603821\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.0024010175839066505\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 22. ----------\n",
      "Batch: 200. Training Loss: 0.008403328247368336\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.0019759335555136204\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.08479137718677521\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.053850412368774414\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.003934665583074093\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.004626212641596794\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.01549581903964281\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.07368631660938263\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.00027905969182029366\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 23. ----------\n",
      "Batch: 200. Training Loss: 0.055328767746686935\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.0018904024036601186\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.15133705735206604\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.0008056795340962708\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.021098904311656952\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.012797282077372074\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.043204374611377716\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.22490160167217255\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.06869186460971832\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 24. ----------\n",
      "Batch: 200. Training Loss: 0.1381801813840866\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.003755318932235241\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.06997828930616379\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.1501622051000595\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.07418959587812424\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.01007232628762722\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.001706407405436039\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.027401654049754143\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.008487722836434841\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 25. ----------\n",
      "Batch: 200. Training Loss: 0.010739694349467754\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.0035250838845968246\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.07944636791944504\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.005551885813474655\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.0031575418543070555\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.00042945859604515135\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.0010026177624240518\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.01892862655222416\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.0012378712417557836\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 26. ----------\n",
      "Batch: 200. Training Loss: 0.0007047296967357397\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.008891809731721878\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.05280553549528122\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.054130952805280685\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.11778219044208527\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.29678356647491455\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.03207943215966225\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.01089392602443695\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.023692816495895386\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 27. ----------\n",
      "Batch: 200. Training Loss: 0.09823497384786606\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.01672257110476494\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.002650220412760973\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.00228164647705853\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.0035037279594689608\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.02185049280524254\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.04879486933350563\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.07744517922401428\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.08430138230323792\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 28. ----------\n",
      "Batch: 200. Training Loss: 0.0841672345995903\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.033545929938554764\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.02422971837222576\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.12333431839942932\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.0024852738715708256\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.0022195035126060247\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.002129414351657033\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.006497367285192013\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.027698960155248642\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 29. ----------\n",
      "Batch: 200. Training Loss: 0.006329250987619162\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.07517712563276291\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.010232985951006413\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.03364945575594902\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.0038475822657346725\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.004714277572929859\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.015030725859105587\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.006316167768090963\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.004173660650849342\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 30. ----------\n",
      "Batch: 200. Training Loss: 0.028130756691098213\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.006572259124368429\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.02607443369925022\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.0031436823774129152\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.009001117199659348\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.038521599024534225\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.0034525624942034483\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.004998104181140661\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.019395876675844193\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 31. ----------\n",
      "Batch: 200. Training Loss: 0.006294433027505875\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 400. Training Loss: 0.0036970535293221474\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 600. Training Loss: 0.036589816212654114\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 800. Training Loss: 0.18134891986846924\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1000. Training Loss: 0.043834660202264786\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1200. Training Loss: 0.003899976843968034\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1400. Training Loss: 0.0015210275305435061\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1600. Training Loss: 0.00497349863871932\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "Batch: 1800. Training Loss: 0.010194173082709312\n",
      "Average test loss: 0. Average test accuracy: 0%\n",
      "---------- EPOCH: 32. ----------\n",
      "Batch: 200. Training Loss: 0.21464519202709198\n",
      "Average test loss: 0.05519133572041016. Average test accuracy: 98.26277955271566%\n",
      "Batch: 400. Training Loss: 0.0004806819779332727\n",
      "Average test loss: 0.06113788124072342. Average test accuracy: 98.10303514376997%\n",
      "Batch: 600. Training Loss: 0.011217603459954262\n",
      "Average test loss: 0.06030554238531042. Average test accuracy: 98.1629392971246%\n",
      "Batch: 800. Training Loss: 0.022716401144862175\n",
      "Average test loss: 0.05806301539190007. Average test accuracy: 98.15295527156549%\n",
      "Batch: 1000. Training Loss: 0.00781431794166565\n",
      "Average test loss: 0.06991721374016341. Average test accuracy: 98.00319488817891%\n",
      "Batch: 1200. Training Loss: 0.0001849720865720883\n",
      "Average test loss: 0.05685603436725792. Average test accuracy: 98.27276357827476%\n",
      "Batch: 1400. Training Loss: 0.007281698752194643\n",
      "Average test loss: 0.05807936764106965. Average test accuracy: 98.15295527156549%\n",
      "Batch: 1600. Training Loss: 0.036605238914489746\n",
      "Average test loss: 0.06040510659700019. Average test accuracy: 98.11301916932908%\n",
      "Batch: 1800. Training Loss: 0.0003849984786938876\n",
      "Average test loss: 0.06579766674088296. Average test accuracy: 98.08306709265176%\n",
      "---------- EPOCH: 33. ----------\n",
      "Batch: 200. Training Loss: 0.021787703037261963\n",
      "Average test loss: 0.05562424577896855. Average test accuracy: 98.29273162939297%\n",
      "Batch: 400. Training Loss: 0.001869923435151577\n",
      "Average test loss: 0.057874062853248635. Average test accuracy: 98.28274760383387%\n",
      "Batch: 600. Training Loss: 0.0003054203698411584\n",
      "Average test loss: 0.05857950642798831. Average test accuracy: 98.1429712460064%\n",
      "Batch: 800. Training Loss: 0.007495408412069082\n",
      "Average test loss: 0.061403204032516145. Average test accuracy: 98.13298722044729%\n",
      "Batch: 1000. Training Loss: 0.008112683892250061\n",
      "Average test loss: 0.05728020829134945. Average test accuracy: 98.37260383386581%\n",
      "Batch: 1200. Training Loss: 0.00290362979285419\n",
      "Average test loss: 0.056854321518629077. Average test accuracy: 98.37260383386581%\n",
      "Batch: 1400. Training Loss: 0.20375558733940125\n",
      "Average test loss: 0.05781283856252233. Average test accuracy: 98.19289137380191%\n",
      "Batch: 1600. Training Loss: 0.0013419355964288116\n",
      "Average test loss: 0.07316106948600443. Average test accuracy: 97.90335463258786%\n",
      "Batch: 1800. Training Loss: 0.029051663354039192\n",
      "Average test loss: 0.06199639418144553. Average test accuracy: 98.18290734824281%\n",
      "---------- EPOCH: 34. ----------\n",
      "Batch: 200. Training Loss: 0.00248632556758821\n",
      "Average test loss: 0.0639693463005523. Average test accuracy: 98.06309904153355%\n",
      "Batch: 400. Training Loss: 0.001995042199268937\n",
      "Average test loss: 0.060967996197144436. Average test accuracy: 98.1729233226837%\n",
      "Batch: 600. Training Loss: 0.001148655661381781\n",
      "Average test loss: 0.061441417190512576. Average test accuracy: 98.19289137380191%\n",
      "Batch: 800. Training Loss: 0.0037812096998095512\n",
      "Average test loss: 0.06278789809124873. Average test accuracy: 98.01317891373802%\n",
      "Batch: 1000. Training Loss: 0.0332496352493763\n",
      "Average test loss: 0.05767536110092385. Average test accuracy: 98.21285942492013%\n",
      "Batch: 1200. Training Loss: 0.004305689129978418\n",
      "Average test loss: 0.06461061288494858. Average test accuracy: 98.04313099041534%\n",
      "Batch: 1400. Training Loss: 0.07114620506763458\n",
      "Average test loss: 0.06311275783138609. Average test accuracy: 98.11301916932908%\n",
      "Batch: 1600. Training Loss: 0.000517366745043546\n",
      "Average test loss: 0.061913123329425244. Average test accuracy: 98.1629392971246%\n",
      "Batch: 1800. Training Loss: 0.0007184353889897466\n",
      "Average test loss: 0.07018364229768344. Average test accuracy: 98.02316293929712%\n",
      "---------- EPOCH: 35. ----------\n",
      "Batch: 200. Training Loss: 0.0033681171480566263\n",
      "Average test loss: 0.05489699090647107. Average test accuracy: 98.3626198083067%\n",
      "Batch: 400. Training Loss: 0.01811729557812214\n",
      "Average test loss: 0.0660320872722584. Average test accuracy: 98.01317891373802%\n",
      "Batch: 600. Training Loss: 0.015709957107901573\n",
      "Average test loss: 0.05836376072695708. Average test accuracy: 98.09305111821087%\n",
      "Batch: 800. Training Loss: 0.0014116032980382442\n",
      "Average test loss: 0.06809710569222445. Average test accuracy: 97.82348242811501%\n",
      "Batch: 1000. Training Loss: 0.014092954806983471\n",
      "Average test loss: 0.05982389212946122. Average test accuracy: 98.08306709265176%\n",
      "Batch: 1200. Training Loss: 0.0006391784991137683\n",
      "Average test loss: 0.060059281886037826. Average test accuracy: 98.09305111821087%\n",
      "Batch: 1400. Training Loss: 0.009669779799878597\n",
      "Average test loss: 0.05974428782325287. Average test accuracy: 98.15295527156549%\n",
      "Batch: 1600. Training Loss: 0.0006847709883004427\n",
      "Average test loss: 0.06019479464543366. Average test accuracy: 98.28274760383387%\n",
      "Batch: 1800. Training Loss: 0.0030849443282932043\n",
      "Average test loss: 0.05905120360466463. Average test accuracy: 98.1629392971246%\n",
      "---------- EPOCH: 36. ----------\n",
      "Batch: 200. Training Loss: 0.11855918169021606\n",
      "Average test loss: 0.06873257082339077. Average test accuracy: 97.9932108626198%\n",
      "Batch: 400. Training Loss: 0.0007129575242288411\n",
      "Average test loss: 0.06046629066808376. Average test accuracy: 98.13298722044729%\n",
      "Batch: 600. Training Loss: 0.0030157563742250204\n",
      "Average test loss: 0.06836748133061807. Average test accuracy: 98.01317891373802%\n",
      "Batch: 800. Training Loss: 0.004099853802472353\n",
      "Average test loss: 0.053297340903294806. Average test accuracy: 98.38258785942492%\n",
      "Batch: 1000. Training Loss: 0.003769875969737768\n",
      "Average test loss: 0.06361408644224924. Average test accuracy: 97.9832268370607%\n",
      "Batch: 1200. Training Loss: 0.0037302167620509863\n",
      "Average test loss: 0.06172378528278674. Average test accuracy: 98.13298722044729%\n",
      "Batch: 1400. Training Loss: 0.01771472953259945\n",
      "Average test loss: 0.05517110935768276. Average test accuracy: 98.34265175718849%\n",
      "Batch: 1600. Training Loss: 0.018707312643527985\n",
      "Average test loss: 0.057647322441380745. Average test accuracy: 98.30271565495208%\n",
      "Batch: 1800. Training Loss: 0.0057013025507330894\n",
      "Average test loss: 0.05712114973238394. Average test accuracy: 98.20287539936102%\n",
      "---------- EPOCH: 37. ----------\n",
      "Batch: 200. Training Loss: 0.02588178776204586\n",
      "Average test loss: 0.056727483733819375. Average test accuracy: 98.25279552715655%\n",
      "Batch: 400. Training Loss: 0.0808493122458458\n",
      "Average test loss: 0.05365411061575142. Average test accuracy: 98.3326677316294%\n",
      "Batch: 600. Training Loss: 0.030013257637619972\n",
      "Average test loss: 0.06817331402873007. Average test accuracy: 97.94329073482429%\n",
      "Batch: 800. Training Loss: 0.0014563441509380937\n",
      "Average test loss: 0.05691270068828903. Average test accuracy: 98.29273162939297%\n",
      "Batch: 1000. Training Loss: 0.01849439926445484\n",
      "Average test loss: 0.06292360006842244. Average test accuracy: 98.13298722044729%\n",
      "Batch: 1200. Training Loss: 0.014558331109583378\n",
      "Average test loss: 0.05651466241419513. Average test accuracy: 98.29273162939297%\n",
      "Batch: 1400. Training Loss: 0.035603117197752\n",
      "Average test loss: 0.05870165315061491. Average test accuracy: 98.22284345047923%\n",
      "Batch: 1600. Training Loss: 0.05769043043255806\n",
      "Average test loss: 0.06009409829230121. Average test accuracy: 98.26277955271566%\n",
      "Batch: 1800. Training Loss: 0.0005747152608819306\n",
      "Average test loss: 0.059200314789993574. Average test accuracy: 98.3326677316294%\n",
      "---------- EPOCH: 38. ----------\n",
      "Batch: 200. Training Loss: 0.03269733861088753\n",
      "Average test loss: 0.06291266581323138. Average test accuracy: 98.15295527156549%\n",
      "Batch: 400. Training Loss: 0.016054613515734673\n",
      "Average test loss: 0.05807969576292522. Average test accuracy: 98.19289137380191%\n",
      "Batch: 600. Training Loss: 0.165976881980896\n",
      "Average test loss: 0.06258646301201794. Average test accuracy: 98.1429712460064%\n",
      "Batch: 800. Training Loss: 0.04359513148665428\n",
      "Average test loss: 0.058677663836981094. Average test accuracy: 98.27276357827476%\n",
      "Batch: 1000. Training Loss: 0.14552243053913116\n",
      "Average test loss: 0.056464969597799355. Average test accuracy: 98.39257188498402%\n",
      "Batch: 1200. Training Loss: 0.002949522575363517\n",
      "Average test loss: 0.06553512243847824. Average test accuracy: 98.08306709265176%\n",
      "Batch: 1400. Training Loss: 0.025653522461652756\n",
      "Average test loss: 0.06068882978520857. Average test accuracy: 98.34265175718849%\n",
      "Batch: 1600. Training Loss: 0.027619007974863052\n",
      "Average test loss: 0.05802446931662393. Average test accuracy: 98.22284345047923%\n",
      "Batch: 1800. Training Loss: 0.04096803814172745\n",
      "Average test loss: 0.06133971568071368. Average test accuracy: 98.1729233226837%\n",
      "---------- EPOCH: 39. ----------\n",
      "Batch: 200. Training Loss: 0.017609307542443275\n",
      "Average test loss: 0.058083973656786005. Average test accuracy: 98.29273162939297%\n",
      "Batch: 400. Training Loss: 0.00359363853931427\n",
      "Average test loss: 0.0563441947840144. Average test accuracy: 98.3326677316294%\n",
      "Batch: 600. Training Loss: 0.23080767691135406\n",
      "Average test loss: 0.05941630871747418. Average test accuracy: 98.25279552715655%\n",
      "Batch: 800. Training Loss: 0.10582674294710159\n",
      "Average test loss: 0.058537845926907035. Average test accuracy: 98.27276357827476%\n",
      "Batch: 1000. Training Loss: 0.004407004453241825\n",
      "Average test loss: 0.06226394403386159. Average test accuracy: 98.07308306709265%\n",
      "Batch: 1200. Training Loss: 0.10831990092992783\n",
      "Average test loss: 0.07562208273246508. Average test accuracy: 97.95327476038338%\n",
      "Batch: 1400. Training Loss: 8.745645027374849e-05\n",
      "Average test loss: 0.054518240541499445. Average test accuracy: 98.37260383386581%\n",
      "Batch: 1600. Training Loss: 0.002531933831050992\n",
      "Average test loss: 0.08861621566358137. Average test accuracy: 97.67372204472844%\n",
      "Batch: 1800. Training Loss: 0.01023405883461237\n",
      "Average test loss: 0.05827399760180826. Average test accuracy: 98.22284345047923%\n",
      "---------- EPOCH: 40. ----------\n",
      "Batch: 200. Training Loss: 0.004216707777231932\n",
      "Average test loss: 0.06354537474731034. Average test accuracy: 98.10303514376997%\n",
      "Batch: 400. Training Loss: 0.0011629300424829125\n",
      "Average test loss: 0.055687526617219964. Average test accuracy: 98.27276357827476%\n",
      "Batch: 600. Training Loss: 0.0570327490568161\n",
      "Average test loss: 0.059800639631749725. Average test accuracy: 98.21285942492013%\n",
      "Batch: 800. Training Loss: 0.07176550477743149\n",
      "Average test loss: 0.055433876332154755. Average test accuracy: 98.34265175718849%\n",
      "Batch: 1000. Training Loss: 0.014630469493567944\n",
      "Average test loss: 0.05866690545137038. Average test accuracy: 98.39257188498402%\n",
      "Batch: 1200. Training Loss: 0.02337893843650818\n",
      "Average test loss: 0.0568437487885612. Average test accuracy: 98.20287539936102%\n",
      "Batch: 1400. Training Loss: 0.03978016600012779\n",
      "Average test loss: 0.06372429989284273. Average test accuracy: 98.25279552715655%\n",
      "Batch: 1600. Training Loss: 0.058111026883125305\n",
      "Average test loss: 0.06679995858081375. Average test accuracy: 98.20287539936102%\n",
      "Batch: 1800. Training Loss: 0.0012123666238039732\n",
      "Average test loss: 0.059649343732217266. Average test accuracy: 98.1729233226837%\n",
      "---------- EPOCH: 41. ----------\n",
      "Batch: 200. Training Loss: 0.001224686624482274\n",
      "Average test loss: 0.0674306879489954. Average test accuracy: 98.13298722044729%\n",
      "Batch: 400. Training Loss: 0.11813700944185257\n",
      "Average test loss: 0.05584837797799739. Average test accuracy: 98.3626198083067%\n",
      "Batch: 600. Training Loss: 0.0005764737725257874\n",
      "Average test loss: 0.056423624838069765. Average test accuracy: 98.49241214057508%\n",
      "Batch: 800. Training Loss: 0.05161966010928154\n",
      "Average test loss: 0.05966152665944257. Average test accuracy: 98.20287539936102%\n",
      "Batch: 1000. Training Loss: 0.22029390931129456\n",
      "Average test loss: 0.05948197935764494. Average test accuracy: 98.3326677316294%\n",
      "Batch: 1200. Training Loss: 0.0003125511866528541\n",
      "Average test loss: 0.05803242207057396. Average test accuracy: 98.31269968051119%\n",
      "Batch: 1400. Training Loss: 0.01092378981411457\n",
      "Average test loss: 0.0771314045430746. Average test accuracy: 97.88338658146965%\n",
      "Batch: 1600. Training Loss: 0.00019391687237657607\n",
      "Average test loss: 0.056826566209430385. Average test accuracy: 98.38258785942492%\n",
      "Batch: 1800. Training Loss: 0.06787223368883133\n",
      "Average test loss: 0.061394350389142906. Average test accuracy: 98.1629392971246%\n",
      "---------- EPOCH: 42. ----------\n",
      "Batch: 200. Training Loss: 0.13893766701221466\n",
      "Average test loss: 0.05689623478131292. Average test accuracy: 98.30271565495208%\n",
      "Batch: 400. Training Loss: 0.1561504751443863\n",
      "Average test loss: 0.06137892958552587. Average test accuracy: 98.26277955271566%\n",
      "Batch: 600. Training Loss: 0.057746462523937225\n",
      "Average test loss: 0.06698126283836914. Average test accuracy: 98.05311501597444%\n",
      "Batch: 800. Training Loss: 0.00032316858414560556\n",
      "Average test loss: 0.05590564996698983. Average test accuracy: 98.25279552715655%\n",
      "Batch: 1000. Training Loss: 0.0035462300293147564\n",
      "Average test loss: 0.06103769888688284. Average test accuracy: 98.29273162939297%\n",
      "Batch: 1200. Training Loss: 0.0004801657923962921\n",
      "Average test loss: 0.06547166282465618. Average test accuracy: 98.1629392971246%\n",
      "Batch: 1400. Training Loss: 0.013808542862534523\n",
      "Average test loss: 0.05936471655255858. Average test accuracy: 98.1629392971246%\n",
      "Batch: 1600. Training Loss: 0.06436824798583984\n",
      "Average test loss: 0.056792438331969966. Average test accuracy: 98.3526357827476%\n",
      "Batch: 1800. Training Loss: 0.0034541829954832792\n",
      "Average test loss: 0.05902538290996245. Average test accuracy: 98.25279552715655%\n",
      "---------- EPOCH: 43. ----------\n",
      "Batch: 200. Training Loss: 0.013718774542212486\n",
      "Average test loss: 0.0599094467828249. Average test accuracy: 98.19289137380191%\n",
      "Batch: 400. Training Loss: 0.07840392738580704\n",
      "Average test loss: 0.06241411083107804. Average test accuracy: 98.29273162939297%\n",
      "Batch: 600. Training Loss: 0.0024079475551843643\n",
      "Average test loss: 0.060328850498352696. Average test accuracy: 98.29273162939297%\n",
      "Batch: 800. Training Loss: 0.1684168130159378\n",
      "Average test loss: 0.06734352177867689. Average test accuracy: 98.05311501597444%\n",
      "Batch: 1000. Training Loss: 0.003245179308578372\n",
      "Average test loss: 0.0603790226235951. Average test accuracy: 98.28274760383387%\n",
      "Batch: 1200. Training Loss: 0.014652099460363388\n",
      "Average test loss: 0.05956058645754205. Average test accuracy: 98.3326677316294%\n",
      "Batch: 1400. Training Loss: 4.6242021198850125e-05\n",
      "Average test loss: 0.05853167780729512. Average test accuracy: 98.19289137380191%\n",
      "Batch: 1600. Training Loss: 0.007169320248067379\n",
      "Average test loss: 0.06317664843376429. Average test accuracy: 98.1429712460064%\n",
      "Batch: 1800. Training Loss: 0.0020387666299939156\n",
      "Average test loss: 0.060100962972803054. Average test accuracy: 98.22284345047923%\n",
      "---------- EPOCH: 44. ----------\n",
      "Batch: 200. Training Loss: 0.007977304048836231\n",
      "Average test loss: 0.06729540286411798. Average test accuracy: 98.08306709265176%\n",
      "Batch: 400. Training Loss: 0.011417858302593231\n",
      "Average test loss: 0.06427170850053972. Average test accuracy: 97.9832268370607%\n",
      "Batch: 600. Training Loss: 0.0007879220647737384\n",
      "Average test loss: 0.05934936742704713. Average test accuracy: 98.13298722044729%\n",
      "Batch: 800. Training Loss: 0.008896169252693653\n",
      "Average test loss: 0.06135159268926302. Average test accuracy: 98.23282747603834%\n",
      "Batch: 1000. Training Loss: 0.027201706543564796\n",
      "Average test loss: 0.06192870791736309. Average test accuracy: 98.11301916932908%\n",
      "Batch: 1200. Training Loss: 0.02008793130517006\n",
      "Average test loss: 0.06447086574906515. Average test accuracy: 98.1729233226837%\n",
      "Batch: 1400. Training Loss: 0.01638868637382984\n",
      "Average test loss: 0.06044760640702189. Average test accuracy: 98.26277955271566%\n",
      "Batch: 1600. Training Loss: 0.0014281615149229765\n",
      "Average test loss: 0.057790770930982026. Average test accuracy: 98.31269968051119%\n",
      "Batch: 1800. Training Loss: 0.002028703922405839\n",
      "Average test loss: 0.059513764341282936. Average test accuracy: 98.24281150159744%\n",
      "---------- EPOCH: 45. ----------\n",
      "Batch: 200. Training Loss: 0.5636826753616333\n",
      "Average test loss: 0.06612323582991061. Average test accuracy: 98.20287539936102%\n",
      "Batch: 400. Training Loss: 0.005880668759346008\n",
      "Average test loss: 0.05905774899543066. Average test accuracy: 98.29273162939297%\n",
      "Batch: 600. Training Loss: 0.001666191266849637\n",
      "Average test loss: 0.058975047993019. Average test accuracy: 98.19289137380191%\n",
      "Batch: 800. Training Loss: 0.048749420791864395\n",
      "Average test loss: 0.06415180326752969. Average test accuracy: 98.19289137380191%\n",
      "Batch: 1000. Training Loss: 0.16626018285751343\n",
      "Average test loss: 0.061748025313095485. Average test accuracy: 98.18290734824281%\n",
      "Batch: 1200. Training Loss: 0.008290459401905537\n",
      "Average test loss: 0.05887021612993478. Average test accuracy: 98.25279552715655%\n",
      "Batch: 1400. Training Loss: 0.00045205868082121015\n",
      "Average test loss: 0.07129870795992851. Average test accuracy: 98.00319488817891%\n",
      "Batch: 1600. Training Loss: 0.014688784256577492\n",
      "Average test loss: 0.06357969114378938. Average test accuracy: 98.21285942492013%\n",
      "Batch: 1800. Training Loss: 0.0024348734878003597\n",
      "Average test loss: 0.06884690514091805. Average test accuracy: 98.05311501597444%\n",
      "---------- EPOCH: 46. ----------\n",
      "Batch: 200. Training Loss: 0.0061100502498447895\n",
      "Average test loss: 0.05905121434291893. Average test accuracy: 98.21285942492013%\n",
      "Batch: 400. Training Loss: 0.00165712705347687\n",
      "Average test loss: 0.062317590648907996. Average test accuracy: 98.3226837060703%\n",
      "Batch: 600. Training Loss: 0.013081084936857224\n",
      "Average test loss: 0.0619123742432324. Average test accuracy: 98.19289137380191%\n",
      "Batch: 800. Training Loss: 0.04186706990003586\n",
      "Average test loss: 0.06228477516357814. Average test accuracy: 98.19289137380191%\n",
      "Batch: 1000. Training Loss: 0.010976264253258705\n",
      "Average test loss: 0.05842412779800713. Average test accuracy: 98.27276357827476%\n",
      "Batch: 1200. Training Loss: 0.0013029900146648288\n",
      "Average test loss: 0.05956128430545415. Average test accuracy: 98.31269968051119%\n",
      "Batch: 1400. Training Loss: 0.029128720983862877\n",
      "Average test loss: 0.06051275955862835. Average test accuracy: 98.24281150159744%\n",
      "Batch: 1600. Training Loss: 0.0009938504081219435\n",
      "Average test loss: 0.06269791674319644. Average test accuracy: 98.15295527156549%\n",
      "Batch: 1800. Training Loss: 0.0007558614015579224\n",
      "Average test loss: 0.06192337940683356. Average test accuracy: 98.25279552715655%\n",
      "---------- EPOCH: 47. ----------\n",
      "Batch: 200. Training Loss: 0.004870313685387373\n",
      "Average test loss: 0.06419913513780168. Average test accuracy: 98.12300319488818%\n",
      "Batch: 400. Training Loss: 0.035164203494787216\n",
      "Average test loss: 0.061740385685342104. Average test accuracy: 98.3326677316294%\n",
      "Batch: 600. Training Loss: 0.00709069287404418\n",
      "Average test loss: 0.06336744112667883. Average test accuracy: 98.25279552715655%\n",
      "Batch: 800. Training Loss: 0.0006978752207942307\n",
      "Average test loss: 0.06561446748199845. Average test accuracy: 98.12300319488818%\n",
      "Batch: 1000. Training Loss: 0.03533932939171791\n",
      "Average test loss: 0.06274246037248693. Average test accuracy: 98.1629392971246%\n",
      "Batch: 1200. Training Loss: 0.009549707174301147\n",
      "Average test loss: 0.06879575265900395. Average test accuracy: 98.12300319488818%\n",
      "Batch: 1400. Training Loss: 0.09744437783956528\n",
      "Average test loss: 0.06556577936060613. Average test accuracy: 98.11301916932908%\n",
      "Batch: 1600. Training Loss: 0.016395000740885735\n",
      "Average test loss: 0.05921442775061528. Average test accuracy: 98.3526357827476%\n",
      "Batch: 1800. Training Loss: 0.00011839235958177596\n",
      "Average test loss: 0.06187251807956743. Average test accuracy: 98.21285942492013%\n",
      "---------- EPOCH: 48. ----------\n",
      "Batch: 200. Training Loss: 0.00035547104198485613\n",
      "Average test loss: 0.07357967333465724. Average test accuracy: 98.12300319488818%\n",
      "Batch: 400. Training Loss: 0.023125914856791496\n",
      "Average test loss: 0.06432947157624473. Average test accuracy: 98.11301916932908%\n",
      "Batch: 600. Training Loss: 0.008389393799006939\n",
      "Average test loss: 0.06069510465964288. Average test accuracy: 98.24281150159744%\n",
      "Batch: 800. Training Loss: 0.000636547221802175\n",
      "Average test loss: 0.05954953831886845. Average test accuracy: 98.29273162939297%\n",
      "Batch: 1000. Training Loss: 0.03976612538099289\n",
      "Average test loss: 0.06127842660419682. Average test accuracy: 98.1629392971246%\n",
      "Batch: 1200. Training Loss: 0.0031451950781047344\n",
      "Average test loss: 0.06862918737071606. Average test accuracy: 98.1729233226837%\n",
      "Batch: 1400. Training Loss: 0.01840815134346485\n",
      "Average test loss: 0.0644633497495742. Average test accuracy: 98.1729233226837%\n",
      "Batch: 1600. Training Loss: 0.002695456612855196\n",
      "Average test loss: 0.07100723598159239. Average test accuracy: 98.11301916932908%\n",
      "Batch: 1800. Training Loss: 0.009789061732590199\n",
      "Average test loss: 0.06361129422082933. Average test accuracy: 98.26277955271566%\n",
      "---------- EPOCH: 49. ----------\n",
      "Batch: 200. Training Loss: 0.00030158969457261264\n",
      "Average test loss: 0.06068770680142737. Average test accuracy: 98.27276357827476%\n",
      "Batch: 400. Training Loss: 0.07481464743614197\n",
      "Average test loss: 0.06189961803283296. Average test accuracy: 98.21285942492013%\n",
      "Batch: 600. Training Loss: 0.0008031180477701128\n",
      "Average test loss: 0.06239015919146868. Average test accuracy: 98.34265175718849%\n",
      "Batch: 800. Training Loss: 0.00043102476047351956\n",
      "Average test loss: 0.06246374295073366. Average test accuracy: 98.29273162939297%\n",
      "Batch: 1000. Training Loss: 0.009326595813035965\n",
      "Average test loss: 0.06732972084046486. Average test accuracy: 98.1729233226837%\n",
      "Batch: 1200. Training Loss: 0.00018257625924888998\n",
      "Average test loss: 0.06598214909151172. Average test accuracy: 98.25279552715655%\n",
      "Batch: 1400. Training Loss: 0.02209356613457203\n",
      "Average test loss: 0.06562068655176799. Average test accuracy: 98.18290734824281%\n",
      "Batch: 1600. Training Loss: 0.0819060429930687\n",
      "Average test loss: 0.06128946277789555. Average test accuracy: 98.23282747603834%\n",
      "Batch: 1800. Training Loss: 0.00565023859962821\n",
      "Average test loss: 0.07300223505371077. Average test accuracy: 97.9832268370607%\n",
      "---------- EPOCH: 50. ----------\n",
      "Batch: 200. Training Loss: 0.06499901413917542\n",
      "Average test loss: 0.07257101231660987. Average test accuracy: 98.07308306709265%\n",
      "Batch: 400. Training Loss: 0.00010822756303241476\n",
      "Average test loss: 0.0654344509279887. Average test accuracy: 98.24281150159744%\n",
      "Batch: 600. Training Loss: 0.03527895733714104\n",
      "Average test loss: 0.0709677285098232. Average test accuracy: 98.1429712460064%\n",
      "Batch: 800. Training Loss: 0.0007008385146036744\n",
      "Average test loss: 0.06629620237446635. Average test accuracy: 98.23282747603834%\n",
      "Batch: 1000. Training Loss: 0.05544320493936539\n",
      "Average test loss: 0.0597919807849215. Average test accuracy: 98.38258785942492%\n",
      "Batch: 1200. Training Loss: 0.003302900120615959\n",
      "Average test loss: 0.062421653883206314. Average test accuracy: 98.19289137380191%\n",
      "Batch: 1400. Training Loss: 0.01262995321303606\n",
      "Average test loss: 0.06711568855929852. Average test accuracy: 98.1429712460064%\n",
      "Batch: 1600. Training Loss: 0.16891597211360931\n",
      "Average test loss: 0.06327812720443665. Average test accuracy: 98.30271565495208%\n",
      "Batch: 1800. Training Loss: 0.020730938762426376\n",
      "Average test loss: 0.0591207498609384. Average test accuracy: 98.24281150159744%\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing loop\n",
    "epochs = 50\n",
    "for i in range(0, epochs):\n",
    "    batch = 1\n",
    "    print(f'---------- EPOCH: {i + 1}. ----------')\n",
    "    for [X, y] in train_dataloader:\n",
    "        train_dict = train_step(cnn_model, X.to(device), y.to(device))\n",
    "        loss = train_dict['loss']\n",
    "        batch += 1\n",
    "\n",
    "        if batch%200 == 0:\n",
    "            print(f'Batch: {batch}. Training Loss: {loss}')\n",
    "            average_test_loss = 0\n",
    "            average_test_accuracy = 0\n",
    "                \n",
    "            if i > 30:\n",
    "                # Test the model\n",
    "                for [test_X, test_y] in test_dataloader:\n",
    "                    test_dict = test_step(cnn_model, test_X.to(device), test_y.to(device))\n",
    "                    \n",
    "                    test_loss = test_dict['loss']\n",
    "                    accuracy = test_dict['accuracy']\n",
    "    \n",
    "                    average_test_loss += test_loss\n",
    "                    average_test_accuracy += accuracy\n",
    "                    \n",
    "                average_test_loss = average_test_loss / len(test_dataloader)\n",
    "                average_test_accuracy = average_test_accuracy / len(test_dataloader)\n",
    "                \n",
    "            print(f'Average test loss: {average_test_loss}. Average test accuracy: {average_test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d3dff10-5cd1-4e1e-8313-81ad28c2e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions using the model\n",
    "def predict(model, X, y):\n",
    "    logits = model(X)\n",
    "\n",
    "    prediction_class = logits.argmax(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "    print(f'Actual Label: {y}. Prediceted Label: {prediction_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7d5c28c-bb6f-4217-93c1-1fd901ed7663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: 7. Prediceted Label: tensor([[7]], device='cuda:0')\n",
      "Actual Label: 2. Prediceted Label: tensor([[2]], device='cuda:0')\n",
      "Actual Label: 1. Prediceted Label: tensor([[1]], device='cuda:0')\n",
      "Actual Label: 0. Prediceted Label: tensor([[0]], device='cuda:0')\n",
      "Actual Label: 4. Prediceted Label: tensor([[4]], device='cuda:0')\n",
      "Actual Label: 1. Prediceted Label: tensor([[1]], device='cuda:0')\n",
      "Actual Label: 4. Prediceted Label: tensor([[4]], device='cuda:0')\n",
      "Actual Label: 9. Prediceted Label: tensor([[9]], device='cuda:0')\n",
      "Actual Label: 5. Prediceted Label: tensor([[5]], device='cuda:0')\n",
      "Actual Label: 9. Prediceted Label: tensor([[9]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    X, y = MNIST_DIGITS_TEST[i]\n",
    "\n",
    "    X = X.unsqueeze(dim=0)\n",
    "    predict(cnn_model, X.to(device), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0c59ddc-c000-4d91-9b90-c0c35be5aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on our own data\n",
    "# I have created a 28x28 image on paint.net of a handwritten 5\n",
    "# First we load it \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74d74e3e-4be6-45ca-9dee-c1311cff6d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAMAAABF0y+mAAABYlBMVEUAAAAFBQULCwsMDAweHh4jIyMZGRkRERENDQ0EBAQTExNvb2+urq6+vr7Kysrt7e3y8vLe3t7Pz8/IyMjS0tKqqqodHR0JCQmUlJT6+vr+/v78/PxkZGQSEhLQ0ND////7+/vs7Ozl5eXu7u7Z2dnb29vz8/Ph4eE6OjoHBwfDw8NBQUEfHx8aGhouLi4CAgKfn581NTV0dHT9/f1VVVVRUVH5+fl5eXk5OTn09PSampoBAQEqKiq5ubkoKCgkJCQiIiIVFRXm5ubp6enc3NylpaVISEgKCgq7u7s7Ozv39/e3t7fd3d3Y2NggICDj4+MvLy8XFxfV1dWbm5sICAh3d3ctLS0sLCzg4ODv7++pqanw8PArKysDAwPR0dHq6uomJiZYWFjk5OR9fX0GBgbGxsZzc3OxsbFJSUk3NzddXV2dnZ2RkZEODg4cHBxubm6wsLCvr6+hoaF+fn5OTk4bGxv2N9/fAAABQ0lEQVR4AWNggANGJmYWVjZ2DiZOuBCMwcXNw8vHLyAoJCwiKgYThNLiEpJSMCAtgyYpKyevoKikrKiiqqSmroEmqaklpaito6PLxaajp6OPJslgIKVmiC4G5xsZS5rAOegMUzNjc3QxON/CUsrKGs5DY9goStna2Ts4YoYAUKG9kxS/M7+ii6ubO5o2IJeVX15eHhQMah6eGF7R85KXd/b28DGTkvK1Qdfr5y+lHmCtHygTZGwcHIImq68RGgZ2bbiTfAR6yMPV6ttKRWKYC5O1jpKKDoBxGBhCYvxiETzZOKn4BAQ3McknORzmhhRuL6lUDoRkmoK8NH96hj0ra0JmFo+lvJcMUjjqukYbS0l58Ssq8qtJSsmbZSNpZGBw98yJUIAkEmMvF/NchKFglnVevmtBenphUXFJKUbgoakdQC4A82cv0dwIX6UAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=28x28>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('./datasets/images/five.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b5a9dbb-47bc-4a9f-a2eb-9730f9fcdf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_tensor = transforms.ToTensor() # Convert the raw image to tensor of format (C, H, W)\n",
    "img_tensor = convert_tensor(img)\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a4327e9-e4df-4ba7-894e-30204e159188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor = img_tensor.unsqueeze(dim=0) # Add the batch dimension\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "555abd8d-4347-4e8e-b279-bed409d5f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: 5. Prediceted Label: tensor([[5]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predict(cnn_model, img_tensor.to(device), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fce6a681-4413-4c8c-aeff-bea2706e0773",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:642] . invalid file name: ./saved models/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./saved models/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:642] . invalid file name: ./saved models/"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(cnn_model.state_dict(), './saved models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec768178-4631-4b7e-80d9-d42537e21518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
